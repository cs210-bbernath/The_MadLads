{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e82e2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the data...\n",
      "Data cleaning...\n",
      "------------------------------------------\n",
      "Gradient descent computing...\n",
      "Accuracy:  71.822\n",
      "Gradient descent submission file created !\n",
      "------------------------------------------\n",
      "Stochastic Gradient descent computing...\n",
      "Accuracy:  70.8344\n",
      "Stochastic gradient descent submission file created !\n",
      "------------------------------------------\n",
      "Least Squares computing...\n",
      "Accuracy:  77.5728\n",
      "Least Square submission file created !\n",
      "------------------------------------------\n",
      "Ridge Regression computing...\n",
      "Accuracy:  79.5512\n",
      "Ridge regression submission file created !\n",
      "------------------------------------------\n",
      "Changing to logistic adaptation labels...\n",
      "Changed done !\n",
      "------------------------------------------\n",
      "Logistic Regression...\n",
      "Accuracy:  72.232\n",
      "Logistic regression submission file created !\n",
      "------------------------------------------\n",
      "Regularized Logistic Regression...\n",
      "Accuracy:  72.2456\n",
      "Regularized logistic regression submission file created !\n",
      "------------------------------------------\n",
      "ALL DONE !\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from hilp import *\n",
    "from helper2 import *\n",
    "from costs import *\n",
    "from implementations import *\n",
    "\n",
    "\n",
    "# TODO Return type: Note that all functions should return: (w, loss), which is the last weight vector of the\n",
    "# method, and the corresponding loss value (cost function). Note that while in previous labs you might\n",
    "# have kept track of all encountered w for iterative methods, here we only want the last one. Moreover, the\n",
    "# loss returned by the regularized methods (ridge regression and reg logistic regression) should\n",
    "# not include the penalty term TODO TODO TODO\n",
    "\n",
    "# TODO check tests in https://github.com/epfml/ML_course/tree/master/projects/project1/grading_tests \n",
    "\n",
    "# DONE change entete des fonctions + modifier output pour que elles return toutes (w, loss) DONE\n",
    "\n",
    "\n",
    "# run script that execute our pipeline\n",
    "\n",
    "print(\"Load the data...\")\n",
    "y_tr, x_tr, ids_tr = load_csv_data('data/train.csv')\n",
    "y_te, x_te, ids_te = load_csv_data('data/test.csv')\n",
    "\n",
    "print(\"Data cleaning...\")\n",
    "\n",
    "# this is for the correlation between two features : we remove 1 of the two feature if they have more than 0.5 correlation.\n",
    "factor = 0.5\n",
    "tx_tr, y_tr = clean_standardize(x_tr, y_tr, factor)\n",
    "tx_te, y_te = clean_standardize(x_te, y_te, factor)\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "######################################################################\n",
    "\n",
    "# best gamma for GRADIENT descent mean squared error using cross validation\n",
    "print(\"Gradient descent computing...\")\n",
    "\n",
    "# from cross validation have that the best combo is :\n",
    "degree_gd = 1\n",
    "max_iters_gd = 1000\n",
    "gamma_gd = 0.08\n",
    "\n",
    "# polynomial expansion only of degree 1\n",
    "tx_tr_1 = build_poly(tx_tr, degree_gd)\n",
    "tx_te_1 = build_poly(tx_te, degree_gd)\n",
    "\n",
    "# creation of inital weights\n",
    "initial_w_gd = np.random.uniform(-1,1,tx_tr_1.shape[1])\n",
    "\n",
    "# run the gradient descent function with optimal parameters\n",
    "w_gradient_descent, loss = mean_squared_error_gd(y_tr, tx_tr_1, initial_w_gd, max_iters_gd, gamma_gd)\n",
    "\n",
    "# predict on the train test JUST FOR US !!!\n",
    "y_pred_tr_gd = predict_y(tx_tr_1, w_gradient_descent)\n",
    "print(\"Accuracy: \", compute_accuracy(y_pred_tr_gd, y_tr))\n",
    "\n",
    "# predict on the test set\n",
    "y_pred_te_gd = predict_y(tx_te_1, w_gradient_descent)\n",
    "\n",
    "# creation of the gradient descent file\n",
    "name = 'gradient_descent_submission'\n",
    "create_csv_submission(ids_te, y_pred_te_gd, name)\n",
    "print(\"Gradient descent submission file created !\")\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "######################################################################\n",
    "\n",
    "# For SGD, you must use the standard mini-batch-size 1\n",
    "# best gamma for STOCHASTIC GRADIENT descent mean squared error using cross validation\n",
    "print(\"Stochastic Gradient descent computing...\")\n",
    "\n",
    "# from cross validation have that the best combo is :\n",
    "degree_sgd = 1\n",
    "max_iters_sgd = 1000\n",
    "gamma_sgd = 0.009\n",
    "\n",
    "# creation of inital weights\n",
    "initial_w_sgd = np.random.uniform(-1,1,tx_tr_1.shape[1])\n",
    "\n",
    "# run the stochastic gradient descent function with optimal parameters (note that we use tx_tr_1 as the degree is still 1)\n",
    "w_stoch_gradient_descent, loss = mean_squared_error_sgd(y_tr, tx_tr_1, initial_w_sgd, max_iters_sgd, gamma_sgd)\n",
    "\n",
    "# predict on the train set JUST FOR US !!!\n",
    "y_pred_tr_sgd = predict_y(tx_tr_1, w_stoch_gradient_descent)\n",
    "print(\"Accuracy: \", compute_accuracy(y_pred_tr_sgd, y_tr))\n",
    "\n",
    "# predict on the test set\n",
    "y_pred_te_sgd = predict_y(tx_te_1, w_stoch_gradient_descent)\n",
    "\n",
    "# creation of the stochastic gradient descent file\n",
    "name = 'stoch_gradient_descent_submission'\n",
    "create_csv_submission(ids_te, y_pred_te_sgd, name)\n",
    "print(\"Stochastic gradient descent submission file created !\")\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "######################################################################\n",
    "\n",
    "# LEAST SQUARES using cross validation\n",
    "print(\"Least Squares computing...\")\n",
    "\n",
    "# from cross validation have that the best degree is :\n",
    "degree_ls = 6\n",
    "lambda_ls = 0 # this is for the call to ridge regression\n",
    "\n",
    "# polynomial expansion of degree 6\n",
    "tx_tr_6 = build_poly(tx_tr, degree_ls)\n",
    "tx_te_6 = build_poly(tx_te, degree_ls)\n",
    "\n",
    "# run the least square function with optimal degree\n",
    "w_least_squares, loss = ridge_regression(y_tr, tx_tr_6, lambda_ls)\n",
    "\n",
    "# predict on the train set JUST FOR US !!!\n",
    "y_pred_tr_ls = predict_y(tx_tr_6, w_least_squares)\n",
    "print(\"Accuracy: \", compute_accuracy(y_pred_tr_ls, y_tr))\n",
    "\n",
    "# predict on the test set\n",
    "y_pred_te_ls = predict_y(tx_te_6, w_least_squares)\n",
    "\n",
    "# creation of the least square file\n",
    "name = 'least_squares_submission'\n",
    "create_csv_submission(ids_te, y_pred_te_ls, name)\n",
    "print(\"Least Square submission file created !\")\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "######################################################################\n",
    "\n",
    "# best lambda for RIDGE REGRESSION (least squared with lambda) using cross validation\n",
    "print(\"Ridge Regression computing...\")\n",
    "\n",
    "# from cross validation have that the best combo is :\n",
    "degree_ridge = 15\n",
    "lambda_ridge = 0.00005\n",
    "\n",
    "# polynomial expansion of degree 6\n",
    "tx_tr_15 = build_poly(tx_tr, degree_ridge)\n",
    "tx_te_15 = build_poly(tx_te, degree_ridge)\n",
    "\n",
    "# run the ridge regression function with optimal parameters\n",
    "w_ridge_regression, loss = ridge_regression(y_tr, tx_tr_15, lambda_ridge)\n",
    "\n",
    "# predict on the train set JUST FOR US !!!\n",
    "y_pred_tr_ridge = predict_y(tx_tr_15, w_ridge_regression)\n",
    "print(\"Accuracy: \", compute_accuracy(y_pred_tr_ridge, y_tr))\n",
    "\n",
    "# predict on the test set\n",
    "y_pred_te_ridge = predict_y(tx_te_15, w_ridge_regression)\n",
    "\n",
    "# creation of the ridge regression file\n",
    "name = 'ridge_regression_submission'\n",
    "create_csv_submission(ids_te, y_pred_te_ridge, name)\n",
    "print(\"Ridge regression submission file created !\")\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "# change logisitic\n",
    "print(\"Changing to logistic adaptation labels...\")\n",
    "y_tr[np.where(y_tr == -1)] = 0\n",
    "y_te[np.where(y_te == -1)] = 0 \n",
    "print(\"Changed done !\")\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "######################################################################\n",
    "\n",
    "# best gamma for LOGISTIC REGRESSION (stochastic) gradient descent using cross validation\n",
    "print(\"Logistic Regression...\")\n",
    "\n",
    "# from cross validation have that the best combo is :\n",
    "degree_log = 1\n",
    "gamma_log = 0.08\n",
    "max_iters_log = 1000\n",
    "lambda_log = 0 # this is for the call to regularized logistic regression\n",
    "\n",
    "# creation of inital weights\n",
    "initial_w_log = np.random.uniform(-1,1,tx_tr_1.shape[1])\n",
    "\n",
    "# run the logistic regression function with optimal parameters (note that we use tx_tr_1 as the degree is still 1)\n",
    "w_log_regression, loss = reg_logistic_regression(y_tr, tx_tr_1, lambda_log, initial_w_log, max_iters_log, gamma_log)\n",
    "\n",
    "# predict on the train set JUST FOR US !!!\n",
    "y_pred_tr_log = predict_logistic(tx_tr_1, w_log_regression)\n",
    "print(\"Accuracy: \", compute_accuracy(y_pred_tr_log, y_tr))\n",
    "\n",
    "# predict on the test set\n",
    "y_pred_te_log = predict_logistic(tx_te_1, w_log_regression)\n",
    "\n",
    "# changing back to the correct label\n",
    "y_pred_te_log[np.where(y_pred_te_log == 0)] = -1 \n",
    "\n",
    "# creation of the logistic regression file\n",
    "name = 'logistic_regression_submission'\n",
    "create_csv_submission(ids_te, y_pred_te_log, name)\n",
    "print(\"Logistic regression submission file created !\")\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "######################################################################\n",
    "\n",
    "# best gamma AND lambda for REGULARIZED LOGISTIC REGRESSION (stochastic) gradient descent using cross validation\n",
    "print(\"Regularized Logistic Regression...\")\n",
    "\n",
    "# from cross validation have that the best combo is :\n",
    "degree_reg = 1\n",
    "gamma_reg = 0.085\n",
    "lambda_reg = 0.001\n",
    "max_iters_reg = 1000\n",
    "\n",
    "# creation of inital weights\n",
    "initial_w_reg = np.random.uniform(-1,1,tx_tr_1.shape[1])\n",
    "\n",
    "# run the regularized logistic regression function with optimal parameters (note that we use tx_tr_1 as the degree is still 1)\n",
    "w_reg_log_regression, loss = reg_logistic_regression(y_tr, tx_tr_1, lambda_reg, initial_w_reg, max_iters_reg, gamma_reg)\n",
    "\n",
    "# predict on the train set JUST FOR US !!!\n",
    "y_pred_tr_reg = predict_logistic(tx_tr_1, w_reg_log_regression)\n",
    "print(\"Accuracy: \", compute_accuracy(y_pred_tr_reg, y_tr))\n",
    "\n",
    "# predict on the test set\n",
    "y_pred_te_reg = predict_logistic(tx_te_1, w_reg_log_regression)\n",
    "\n",
    "# changing back to the correct label\n",
    "y_pred_te_reg[np.where(y_pred_te_reg == 0)] = -1 \n",
    "\n",
    "# creation of the regularized logistic regression file\n",
    "name = 'reg_logistic_regression_submission'\n",
    "create_csv_submission(ids_te, y_pred_te_reg, name)\n",
    "print(\"Regularized logistic regression submission file created !\")\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "######################################################################\n",
    "\n",
    "# convert the results into the file that we can put on the website challenge\n",
    "print(\"ALL DONE !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bc97a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
