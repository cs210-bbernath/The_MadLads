{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fd0d97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from hilp import load_csv_data\n",
    "from helper import *\n",
    "from costs import *\n",
    "from least_squares import *\n",
    "from ridge_regression import *\n",
    "from logistic_regression import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3351eeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "yb, input_data, ids = load_csv_data('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0380fb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f58f925f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 250000)\n"
     ]
    }
   ],
   "source": [
    "cleaned_columns = [c for c in input_data.T if (c==-999).sum()/len(c) < 0.2]\n",
    "print(np.shape(cleaned_columns))\n",
    "for c in cleaned_columns:\n",
    "    numb_of_nan = (c==-999).sum()\n",
    "    median = np.median(list(filter(lambda x : x!= -999, c)))\n",
    "    c[c == -999] = median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2eae98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([138.47 , 160.937, 112.406, ..., 105.457,  94.951, 112.406]),\n",
       " array([ 51.655,  68.768, 162.172, ...,  60.526,  19.362,  72.756]),\n",
       " array([ 97.827, 103.235, 125.953, ...,  75.839,  68.812,  70.831]),\n",
       " array([27.98 , 48.146, 35.635, ..., 39.757, 13.504,  7.479]),\n",
       " array([3.064, 3.473, 3.148, ..., 2.39 , 3.365, 2.025]),\n",
       " array([41.928,  2.078,  9.336, ..., 22.183, 13.504,  7.479]),\n",
       " array([197.76 , 125.157, 197.814, ..., 120.462,  55.859,  83.24 ]),\n",
       " array([1.582, 0.879, 3.776, ..., 1.202, 0.999, 0.936]),\n",
       " array([ 1.396,  1.414,  1.414, ...,  0.529,  1.414, -1.411]),\n",
       " array([32.638, 42.014, 32.154, ..., 35.636, 27.944, 43.003]),\n",
       " array([ 1.017,  2.039, -0.705, ..., -0.266, -2.211,  1.685]),\n",
       " array([ 0.381, -3.011, -2.093, ..., -3.132,  2.792,  2.653]),\n",
       " array([ 51.626,  36.918, 121.409, ...,  42.834,  27.915,  40.236]),\n",
       " array([ 2.273,  0.501, -0.953, ...,  0.381, -0.874,  1.49 ]),\n",
       " array([-2.414,  0.103,  1.052, ...,  0.851, -0.296,  0.637]),\n",
       " array([16.824, 44.704, 54.283, ..., 23.419, 12.15 , 40.729]),\n",
       " array([-0.277, -1.916, -2.186, ..., -2.89 ,  0.811, -1.596]),\n",
       " array([258.733, 164.546, 260.414, ..., 198.907, 112.718,  99.405]),\n",
       " array([2., 1., 1., ..., 1., 0., 0.]),\n",
       " array([113.497,  46.226,  44.251, ...,  41.992,   0.   ,   0.   ])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d151ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.random.randint(0,1,len(cleaned_columns))\n",
    "batch_size = 1\n",
    "max_iters = 20\n",
    "#gamma = 0.01\n",
    "lambda_ = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0420b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = [0.001, 0.002, 0.005, 0.01, 0.02, 0.03]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36439cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the data\n",
    "std_data, mean, std = standardize(np.transpose(cleaned_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a430b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model (add 1's for the w0)\n",
    "tx, y = build_model_data(std_data, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55a855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "W = []\n",
    "for gamma in gammas:\n",
    "    loss, ws = gradient_descent(y, tx, initial_w, max_iters, gamma)\n",
    "    l.append(loss[max_iters-1])\n",
    "    W.append(ws)\n",
    "best_loss = min(l)\n",
    "ind = l.index(best_loss)\n",
    "print(\"best loss :\" + str(best_loss)+ \" \" + str(gammas[ind]) +\" \\n \"+ str(W[ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74eead0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/19: loss=0.5, w0=0.03, w1=0.03017918643569754\n",
      "SGD iter. 1/19: loss=0.9229100967786545, w0=-0.009705064918780183, w1=-0.004663240736245754\n",
      "SGD iter. 2/19: loss=0.6399740929166161, w0=-0.002661443954838913, w1=0.0036358678329791016\n",
      "SGD iter. 3/19: loss=0.7645522196061143, w0=0.006098844224092026, w1=0.014351063696840338\n",
      "SGD iter. 4/19: loss=0.9041656377049222, w0=0.022515681858204153, w1=0.03440532353838178\n",
      "SGD iter. 5/19: loss=1.1171880488022574, w0=-0.01027217158835007, w1=0.005245185109580022\n",
      "SGD iter. 6/19: loss=0.9223599870931383, w0=0.015437015269327371, w1=0.02780579475902592\n",
      "SGD iter. 7/19: loss=1.1910645413632905, w0=-0.013368068346780511, w1=0.0025284395871102427\n",
      "SGD iter. 8/19: loss=1.0458314872351304, w0=-0.05908932583930209, w1=-0.0497475569083414\n",
      "SGD iter. 9/19: loss=0.5407527197873756, w0=-0.11157884554518674, w1=-0.06331782154975882\n",
      "SGD iter. 10/19: loss=0.7819871023513536, w0=-0.04876698401679484, w1=0.04302768727108648\n",
      "SGD iter. 11/19: loss=5.172503282661993, w0=-0.08264173194200544, w1=0.0316786290726971\n",
      "SGD iter. 12/19: loss=4.640593133892575, w0=-0.11777316632589477, w1=-0.015342300091401737\n",
      "SGD iter. 13/19: loss=3.2478541097268745, w0=-0.08241028110751401, w1=0.020244700515074272\n",
      "SGD iter. 14/19: loss=3.8442651672131416, w0=-0.10901026279754376, w1=0.0052467981087477075\n",
      "SGD iter. 15/19: loss=3.5254021363072137, w0=-0.18590449638291034, w1=-0.05743979184373161\n",
      "SGD iter. 16/19: loss=1.5993450971762317, w0=-0.26160765403286695, w1=-0.12387166300425836\n",
      "SGD iter. 17/19: loss=0.7640233116621199, w0=-0.2628613338183894, w1=-0.12436616212028218\n",
      "SGD iter. 18/19: loss=0.7682807018754992, w0=-0.2801853258828213, w1=-0.150278709717616\n",
      "SGD iter. 19/19: loss=0.9304503932592871, w0=-0.21564265583042064, w1=-0.08405924427132523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.5,\n",
       "  0.9229100967786545,\n",
       "  0.6399740929166161,\n",
       "  0.7645522196061143,\n",
       "  0.9041656377049222,\n",
       "  1.1171880488022574,\n",
       "  0.9223599870931383,\n",
       "  1.1910645413632905,\n",
       "  1.0458314872351304,\n",
       "  0.5407527197873756,\n",
       "  0.7819871023513536,\n",
       "  5.172503282661993,\n",
       "  4.640593133892575,\n",
       "  3.2478541097268745,\n",
       "  3.8442651672131416,\n",
       "  3.5254021363072137,\n",
       "  1.5993450971762317,\n",
       "  0.7640233116621199,\n",
       "  0.7682807018754992,\n",
       "  0.9304503932592871],\n",
       " [array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  array([ 0.03      ,  0.03017919, -0.01049198,  0.00167854,  0.06166404,\n",
       "         -0.0171028 , -0.00703534,  0.08554633, -0.01734664, -0.01739783,\n",
       "          0.00023626, -0.01780812, -0.01745996, -0.00615184, -0.01829813,\n",
       "         -0.01752521,  0.03283282, -0.01740916,  0.14134448, -0.01681599,\n",
       "          0.05626691]),\n",
       "  array([-0.00970506, -0.00466324, -0.02956234, -0.04475465,  0.06534441,\n",
       "          0.00422022,  0.01594262,  0.06132705,  0.00520777,  0.00638847,\n",
       "          0.01188256,  0.00624969,  0.00667336,  0.00057051,  0.00411198,\n",
       "          0.00443341,  0.02903804,  0.00619199,  0.10476178,  0.00595718,\n",
       "          0.06025964]),\n",
       "  array([-2.66144395e-03,  3.63586783e-03, -3.01393574e-02, -4.03210257e-02,\n",
       "          7.62381524e-02,  2.32590480e-04,  1.20927003e-02,  8.33561338e-02,\n",
       "          1.11508675e-03,  2.33083772e-03,  1.56944276e-02,  2.02324547e-03,\n",
       "          2.37370411e-03, -1.85046329e-04,  1.02208419e-05,  6.16457929e-05,\n",
       "          2.95523261e-02,  1.90554814e-03,  1.35089556e-01,  2.00899483e-03,\n",
       "          7.09689786e-02]),\n",
       "  array([ 0.00609884,  0.01435106, -0.03342653, -0.03812318,  0.08359467,\n",
       "         -0.00466038,  0.00909455,  0.0996328 , -0.00392759, -0.00266483,\n",
       "          0.01507297, -0.00324399, -0.00258579, -0.00152468, -0.00522929,\n",
       "         -0.0053707 ,  0.03403181, -0.00356183,  0.1622452 , -0.00290144,\n",
       "          0.07892934]),\n",
       "  array([ 0.02251568,  0.03440532, -0.04262508, -0.03013959,  0.08232485,\n",
       "         -0.01372143,  0.00402748,  0.11618233, -0.01339621, -0.01205946,\n",
       "          0.01583969, -0.01319822, -0.01181812, -0.00331369, -0.01522586,\n",
       "         -0.01517033,  0.03220773, -0.0133497 ,  0.19552012, -0.01231745,\n",
       "          0.07724169]),\n",
       "  array([-0.01027217,  0.00524519, -0.05377057, -0.04489596,  0.09989593,\n",
       "          0.00421394,  0.02159856,  0.10210458,  0.00495898,  0.00774008,\n",
       "          0.02416942,  0.00652017,  0.00820477, -0.00648836,  0.00485682,\n",
       "          0.00360765,  0.04105507,  0.00684957,  0.18685035,  0.00691536,\n",
       "          0.0964745 ]),\n",
       "  array([ 0.01543702,  0.02780579, -0.04165584, -0.02815904,  0.08569371,\n",
       "         -0.01008841,  0.00739634,  0.11746954, -0.00978439, -0.00781401,\n",
       "          0.02425937, -0.00877473, -0.00628645, -0.00629393, -0.01082753,\n",
       "         -0.01231378,  0.04148725, -0.00834687,  0.22770113, -0.00816522,\n",
       "          0.08139392]),\n",
       "  array([-0.01336807,  0.00252844, -0.05721757, -0.04314073,  0.10235918,\n",
       "          0.00575089,  0.02406181,  0.10790827,  0.00624887,  0.00957633,\n",
       "          0.03313996,  0.00801718,  0.00991978, -0.00783921,  0.00659883,\n",
       "          0.0054041 ,  0.04340238,  0.00835574,  0.22990853,  0.00873137,\n",
       "          0.0982905 ]),\n",
       "  array([-0.05908933, -0.04974756, -0.09989481, -0.08395753,  0.10894895,\n",
       "          0.03105221,  0.04812324,  0.04077022,  0.03146385,  0.03721868,\n",
       "          0.03958085,  0.03487045,  0.037181  , -0.03591048,  0.03301677,\n",
       "          0.03037788,  0.04544766,  0.03479393,  0.11775734,  0.03495517,\n",
       "          0.1064215 ]),\n",
       "  array([-0.11157885, -0.06331782, -0.10720386, -0.0845972 ,  0.06596641,\n",
       "          0.06098913,  0.01696714, -0.1075133 ,  0.06079571,  0.06794458,\n",
       "          0.05347229,  0.0645257 ,  0.06861588, -0.04114863,  0.06336118,\n",
       "          0.06131094,  0.03495126,  0.06502074, -0.14541999,  0.06369359,\n",
       "          0.01106366]),\n",
       "  array([-0.04876698,  0.04302769, -0.12588148, -0.01453862,  0.10669634,\n",
       "          0.02645598,  0.14604629,  0.49011826,  0.02709215,  0.03106746,\n",
       "          0.03580835,  0.02848717,  0.03314269, -0.00434987,  0.02572646,\n",
       "          0.02416629,  0.01242184,  0.0274122 ,  0.5346849 ,  0.02930361,\n",
       "          0.51587078]),\n",
       "  array([-0.08264173,  0.03167863, -0.11900965, -0.01910117,  0.11012131,\n",
       "          0.04540242,  0.14947126,  0.47985109,  0.04604123,  0.05103975,\n",
       "          0.04591834,  0.04894304,  0.05218664, -0.00485667,  0.04618189,\n",
       "          0.04413416,  0.03012093,  0.04618259,  0.50408204,  0.04917397,\n",
       "          0.53574115]),\n",
       "  array([-1.17773166e-01, -1.53423001e-02, -1.12659421e-01, -4.53189104e-02,\n",
       "          9.11743001e-02,  6.49309263e-02,  1.69568556e-01,  4.03684095e-01,\n",
       "          6.64794374e-02,  7.10020562e-02,  2.49510492e-02,  6.94302106e-02,\n",
       "          7.37830173e-02,  3.71993695e-04,  6.66104866e-02,  6.46529073e-02,\n",
       "          4.23544593e-02,  6.73227439e-02,  4.25411546e-01,  6.88662990e-02,\n",
       "          5.16527817e-01]),\n",
       "  array([-0.08241028,  0.0202447 , -0.11935687, -0.02394034,  0.08247274,\n",
       "          0.04591355,  0.160867  ,  0.40959538,  0.04612445,  0.04960747,\n",
       "          0.01867414,  0.04868785,  0.05330182, -0.00818309,  0.0449142 ,\n",
       "          0.04273323,  0.03459024,  0.04775125,  0.47796327,  0.04812302,\n",
       "          0.49578453]),\n",
       "  array([-0.10901026,  0.0052468 , -0.11083632, -0.02917608,  0.08709099,\n",
       "          0.06059747,  0.17638002,  0.39383802,  0.06134644,  0.0647224 ,\n",
       "          0.0245972 ,  0.06371582,  0.06936296, -0.00322877,  0.06019371,\n",
       "          0.05791052,  0.0451768 ,  0.06327918,  0.47036495,  0.06337965,\n",
       "          0.50035635]),\n",
       "  array([-0.1859045 , -0.05743979, -0.11472   , -0.06076106,  0.11629001,\n",
       "          0.10251537,  0.19468101,  0.26563803,  0.10529852,  0.1112435 ,\n",
       "          0.03594131,  0.11078277,  0.11300356,  0.0030284 ,  0.10657659,\n",
       "          0.10473109,  0.07084029,  0.1078963 ,  0.29216882,  0.10648141,\n",
       "          0.44476485]),\n",
       "  array([-2.61607654e-01, -1.23871663e-01, -1.57520166e-01, -3.67384932e-02,\n",
       "          6.76898918e-02,  1.46300378e-01,  1.99048615e-01,  6.82447358e-02,\n",
       "          1.47662627e-01,  1.55612247e-01,  5.75667862e-02,  1.54319287e-01,\n",
       "          1.60150014e-01,  2.50085715e-04,  1.50262988e-01,  1.51275065e-01,\n",
       "          4.91554334e-02,  1.50011919e-01,  7.13577905e-02,  1.47929470e-01,\n",
       "          3.17337817e-01]),\n",
       "  array([-0.26286133, -0.12436616, -0.15729065, -0.03682494,  0.06752211,\n",
       "          0.14699971,  0.1994082 ,  0.06730857,  0.14837669,  0.15632466,\n",
       "          0.05793367,  0.15506454,  0.16086096,  0.00050423,  0.15102385,\n",
       "          0.15205609,  0.04944477,  0.15070921,  0.06925288,  0.14864853,\n",
       "          0.31725141]),\n",
       "  array([-2.80185326e-01, -1.50278710e-01, -1.62862739e-01, -5.62516520e-02,\n",
       "          5.58338046e-02,  1.56688716e-01,  2.01979555e-01,  1.77285991e-02,\n",
       "          1.58455616e-01,  1.66659699e-01,  4.73126787e-02,  1.65054559e-01,\n",
       "          1.70562600e-01,  3.02729688e-03,  1.61438769e-01,  1.61549462e-01,\n",
       "          5.14274051e-02,  1.60867566e-01, -1.83084034e-04,  1.58133548e-01,\n",
       "          2.96093285e-01]),\n",
       "  array([-0.21564266, -0.08405924, -0.18709156, -0.03257749,  0.0946058 ,\n",
       "          0.1205617 ,  0.1847881 ,  0.1037016 ,  0.1212912 ,  0.129882  ,\n",
       "          0.04655742,  0.1260709 ,  0.13448939, -0.00415919,  0.12177921,\n",
       "          0.12388064,  0.05527657,  0.12352156,  0.12786276,  0.12111457,\n",
       "          0.31428787])])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b760d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2.95137215e+02,  1.64125594e-02, -5.66315112e-01, -5.06641813e-01,\n",
       "         4.81722734e-03,  2.78021755e+01, -1.39849015e-01, -2.39440384e+02,\n",
       "        -1.76046142e+01,  8.44767115e+00,  2.40099930e+02, -5.66366907e-02,\n",
       "        -3.61739715e-02,  2.40467040e+02, -3.75756920e-02,  9.84796412e-02,\n",
       "         2.46397753e-01,  3.50785025e-02, -4.26087024e-02,  6.56880260e+00,\n",
       "         2.39396437e+02]),\n",
       " 0.3524927958636052)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "least_squares(y, tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeef44c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08441931,  0.07796364, -0.63864758, -0.09861935,  0.07178693,\n",
       "        0.08681586, -0.07426362,  0.14507038,  0.02372936,  0.1226225 ,\n",
       "        0.37526345,  0.04789182,  0.04381124,  0.01369728,  0.04816825,\n",
       "        0.05241047,  0.05438705,  0.04991389, -0.03574803,  0.0640392 ,\n",
       "       -0.14485184])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_regression(y, tx, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3976a3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 0/19: loss=-48754.730509484805, w0=-0.048879839999999994, w1=-0.04695973632452501\n",
      "Logistic regression iter. 1/19: loss=-35707.24449313245, w0=-0.08985714268353152, w1=-0.08554133505336953\n",
      "Logistic regression iter. 2/19: loss=-27400.740608232303, w0=-0.12557639870454806, w1=-0.11870400741611611\n",
      "Logistic regression iter. 3/19: loss=-21668.50599601481, w0=-0.15768922238301664, w1=-0.14825707584077008\n",
      "Logistic regression iter. 4/19: loss=-17495.7892064634, w0=-0.1871999402570617, w1=-0.17527031963985346\n",
      "Logistic regression iter. 5/19: loss=-14342.976646977268, w0=-0.21475873152825634, w1=-0.20041883217887124\n",
      "Logistic regression iter. 6/19: loss=-11894.873501409907, w0=-0.2408104444433073, w1=-0.2241542070951057\n",
      "Logistic regression iter. 7/19: loss=-9954.147865209428, w0=-0.2656728388295577, w1=-0.2467926323454867\n",
      "Logistic regression iter. 8/19: loss=-8390.53641002046, w0=-0.2895806336480255, w1=-0.26856351577779847\n",
      "Logistic regression iter. 9/19: loss=-7114.384379808029, w0=-0.312711868130403, w1=-0.289638066194614\n",
      "Logistic regression iter. 10/19: loss=-6061.870217468984, w0=-0.33520450053276457, w1=-0.31014699508520455\n",
      "Logistic regression iter. 11/19: loss=-5186.282445945543, w0=-0.3571673086287244, w1=-0.3301919659659219\n",
      "Logistic regression iter. 12/19: loss=-4452.6291641406415, w0=-0.37868730413075213, w1=-0.34985327314037057\n",
      "Logistic regression iter. 13/19: loss=-3834.1773874662094, w0=-0.3998349281321938, w1=-0.36919515210180265\n",
      "Logistic regression iter. 14/19: loss=-3310.1580823805384, w0=-0.42066778553541384, w1=-0.3882695497605615\n",
      "Logistic regression iter. 15/19: loss=-2864.2010657928963, w0=-0.4412333890615309, w1=-0.4071188626148418\n",
      "Logistic regression iter. 16/19: loss=-2483.2412669871364, w0=-0.46157121457181144, w1=-0.4257779650563361\n",
      "Logistic regression iter. 17/19: loss=-2156.7377523566697, w0=-0.4817142666727991, w1=-0.4442757380778828\n",
      "Logistic regression iter. 18/19: loss=-1876.105290579835, w0=-0.5016902891063301, w1=-0.46263623913730334\n",
      "Logistic regression iter. 19/19: loss=-1634.2934553523055, w0=-0.5215227128712135, w1=-0.48087960955285847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-48754.730509484805,\n",
       "  -35707.24449313245,\n",
       "  -27400.740608232303,\n",
       "  -21668.50599601481,\n",
       "  -17495.7892064634,\n",
       "  -14342.976646977268,\n",
       "  -11894.873501409907,\n",
       "  -9954.147865209428,\n",
       "  -8390.53641002046,\n",
       "  -7114.384379808029,\n",
       "  -6061.870217468984,\n",
       "  -5186.282445945543,\n",
       "  -4452.6291641406415,\n",
       "  -3834.1773874662094,\n",
       "  -3310.1580823805384,\n",
       "  -2864.2010657928963,\n",
       "  -2483.2412669871364,\n",
       "  -2156.7377523566697,\n",
       "  -1876.105290579835,\n",
       "  -1634.2934553523055],\n",
       " [array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  array([-0.04887984, -0.04695974, -0.01189169, -0.02343984,  0.00090263,\n",
       "          0.02716833,  0.01637539, -0.05904454,  0.02763437,  0.02899441,\n",
       "          0.00793917,  0.02867824,  0.02867137, -0.00155803,  0.02868595,\n",
       "          0.02864994,  0.00266014,  0.0286886 , -0.09218265,  0.02814552,\n",
       "         -0.00808147]),\n",
       "  array([-0.08985714, -0.08554134, -0.02352073, -0.04277793,  0.00520394,\n",
       "          0.04993495,  0.0304316 , -0.10193141,  0.05078451,  0.05336131,\n",
       "          0.01567792,  0.05271987,  0.05270654, -0.00253167,  0.05273479,\n",
       "          0.05266833,  0.00565215,  0.05274081, -0.16239227,  0.05178642,\n",
       "         -0.00966024]),\n",
       "  array([-0.1255764 , -0.11870401, -0.03491624, -0.05946993,  0.01102353,\n",
       "          0.069777  ,  0.042809  , -0.13572889,  0.07094979,  0.07464335,\n",
       "          0.02312091,  0.07367648,  0.07365688, -0.00326036,  0.07369817,\n",
       "          0.0736052 ,  0.00860953,  0.07370807, -0.21974627,  0.07242117,\n",
       "         -0.00826737]),\n",
       "  array([-0.15768922, -0.14825708, -0.04611726, -0.0744087 ,  0.01748492,\n",
       "          0.08761534,  0.05396515, -0.16415267,  0.08906715,  0.09380573,\n",
       "          0.03027619,  0.0925171 ,  0.09249134, -0.00389904,  0.0925452 ,\n",
       "          0.09242859,  0.01141155,  0.09255936, -0.26922428,  0.09098846,\n",
       "         -0.00553405]),\n",
       "  array([-0.18719994, -0.17527032, -0.05715988, -0.08812225,  0.02420973,\n",
       "          0.10400897,  0.06420573, -0.18912096,  0.10570661,  0.11143653,\n",
       "          0.03718182,  0.10983098,  0.10979914, -0.00451246,  0.1098652 ,\n",
       "          0.10972712,  0.0140385 ,  0.10988393, -0.31349592,  0.10806091,\n",
       "         -0.00217354]),\n",
       "  array([-0.21475873, -0.20041883, -0.06807373, -0.10094291,  0.03102332,\n",
       "          0.11931941,  0.07374068, -0.21172456,  0.12123726,  0.12791711,\n",
       "          0.04387705,  0.12599962,  0.12596175, -0.00512912,  0.12603969,\n",
       "          0.12588184,  0.01650097,  0.12606323, -0.35412655,  0.12401021,\n",
       "          0.00147535]),\n",
       "  array([-0.24081044, -0.22415421, -0.0788824 , -0.11309208,  0.03784125,\n",
       "          0.13379379,  0.0827196 , -0.23263565,  0.13591173,  0.14350875,\n",
       "          0.05039597,  0.141284  ,  0.14124015, -0.00576174,  0.14132973,\n",
       "          0.14115344,  0.01881738,  0.14135828, -0.39210517,  0.13909118,\n",
       "          0.00524096]),\n",
       "  array([-0.26567284, -0.24679263, -0.08960462, -0.12472397,  0.04462225,\n",
       "          0.14760855,  0.09125258, -0.25229441,  0.14991047,  0.1583983 ,\n",
       "          0.05676689,  0.15587057,  0.15582077, -0.00641565,  0.15592177,\n",
       "          0.15572814,  0.02100675,  0.15595551, -0.42809291,  0.15348614,\n",
       "          0.00903292]),\n",
       "  array([-0.28958063, -0.26856352, -0.10025529, -0.13594984,  0.05134697,\n",
       "          0.16089395,  0.09942283, -0.27100277,  0.16336681,  0.17272394,\n",
       "          0.06301306,  0.16989705,  0.16984133, -0.00709255,  0.16995358,\n",
       "          0.16974348,  0.02308649,  0.16999264, -0.46255124,  0.16732999,\n",
       "          0.01280313]),\n",
       "  array([-0.31271187, -0.28963807, -0.11084639, -0.14685221,  0.05800764,\n",
       "          0.17374877,  0.10729466, -0.28897546,  0.17638187,  0.18659055,\n",
       "          0.06915358,  0.18346789,  0.18340628, -0.00779231,  0.18352962,\n",
       "          0.18330381,  0.02507179,  0.18357414, -0.49581325,  0.18072514,\n",
       "          0.01652648]),\n",
       "  array([-0.3352045 , -0.310147  , -0.12138763, -0.15749369,  0.06460266,\n",
       "          0.18624955,  0.11491867, -0.30636963,  0.18903404,  0.20007948,\n",
       "          0.07520421,  0.19666404,  0.19659656, -0.00851388,  0.19673086,\n",
       "          0.19648995,  0.02697576,  0.19678095, -0.5281259 ,  0.19375105,\n",
       "          0.02019085]),\n",
       "  array([-0.35716731, -0.33019197, -0.13188692, -0.1679227 ,  0.07113382,\n",
       "          0.1984566 ,  0.1223353 , -0.32330302,  0.20138511,  0.2132549 ,\n",
       "          0.08117806,  0.20954933,  0.209476  , -0.0092558 ,  0.20962114,\n",
       "          0.20936566,  0.02880956,  0.2096769 , -0.55967629,  0.20647044,\n",
       "          0.02379157]),\n",
       "  array([-0.3786873 , -0.34985327, -0.14235077, -0.17817719,  0.07760459,\n",
       "          0.21041819,  0.12957725, -0.33986549,  0.21348452,  0.22616818,\n",
       "          0.08708607,  0.22217481,  0.22209564, -0.01001641,  0.22225154,\n",
       "          0.22198191,  0.03058271,  0.22231303, -0.59060872,  0.21893348,\n",
       "          0.02732825]),\n",
       "  array([-0.39983493, -0.36919515, -0.15278458, -0.18828738,  0.08401929,\n",
       "          0.22217337,  0.13667129, -0.35612676,  0.2253723 ,  0.23886093,\n",
       "          0.09293743,  0.2345818 ,  0.23449681, -0.01079405,  0.23466338,\n",
       "          0.23437999,  0.03230335,  0.23473069, -0.62103609,  0.23118086,\n",
       "          0.03080296]),\n",
       "  array([-0.42066779, -0.38826955, -0.16319282, -0.19827753,  0.09038255,\n",
       "          0.23375407,  0.1436395 , -0.37214162,  0.23708117,  0.2513672 ,\n",
       "          0.0987399 ,  0.24680411,  0.2467133 , -0.01158711,  0.24689048,\n",
       "          0.24659364,  0.03397844,  0.24696366, -0.6510479 ,  0.24324585,\n",
       "          0.03421911]),\n",
       "  array([-0.44123339, -0.40711886, -0.17357927, -0.20816731,  0.09669898,\n",
       "          0.24518663,  0.15050024, -0.3879536 ,  0.24863814,  0.26371507,\n",
       "          0.1045001 ,  0.25886962,  0.25877301, -0.01239407,  0.25896072,\n",
       "          0.25865071,  0.03561394,  0.25903983, -0.68071576,  0.25515591,\n",
       "          0.03758077]),\n",
       "  array([-0.46157121, -0.42577797, -0.18394706, -0.21797281,  0.10297301,\n",
       "          0.25649294,  0.15726889, -0.40359764,  0.26006564,  0.27592789,\n",
       "          0.11022365,  0.27080148,  0.27069907, -0.01321352,  0.27089727,\n",
       "          0.27057432,  0.03721499,  0.27098236, -0.71009757,  0.26693388,\n",
       "          0.0408923 ]),\n",
       "  array([-0.48171427, -0.44427574, -0.19429887, -0.22770728,  0.10920883,\n",
       "          0.26769129,  0.16395837, -0.41910202,  0.27138243,  0.28802521,\n",
       "          0.11591535,  0.28261907,  0.28251087, -0.01404416,  0.28271951,\n",
       "          0.28238382,  0.03878599,  0.28281061, -0.73924042,  0.27859885,\n",
       "          0.04415803]),\n",
       "  array([-0.50169029, -0.46263624, -0.20463696, -0.23738167,  0.1154103 ,\n",
       "          0.27879706,  0.17057966, -0.43448975,  0.28260428,  0.30002344,\n",
       "          0.12157934,  0.29433865,  0.29422467, -0.01488482,  0.29444371,\n",
       "          0.29409546,  0.04033077,  0.29454086, -0.76818291,  0.29016688,\n",
       "          0.04738217]),\n",
       "  array([-0.52152271, -0.48087961, -0.21496324, -0.24700513,  0.121581  ,\n",
       "          0.28982324,  0.17714203, -0.44977971,  0.29374449,  0.31193645,\n",
       "          0.1272192 ,  0.30597398,  0.30585422, -0.01573444,  0.30608363,\n",
       "          0.30572297,  0.04185263,  0.30618686, -0.79695684,  0.30165156,\n",
       "          0.05056872])])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression(y, tx, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abab0671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reg Logistic regression iter. 0/19: loss=-48754.7305094843, w0=-0.048879839999999994, w1=-0.04695973632452501\n",
      "Reg Logistic regression iter. 1/19: loss=-35707.24450984966, w0=-0.08985714262487571, w1=-0.08554133499701785\n",
      "Reg Logistic regression iter. 2/19: loss=-27400.74064238416, w0=-0.12557639854520963, w1=-0.11870400726436083\n",
      "Reg Logistic regression iter. 3/19: loss=-21668.506045273127, w0=-0.1576892220884089, w1=-0.14825707556179624\n",
      "Reg Logistic regression iter. 4/19: loss=-17495.7892680503, w0=-0.18719993979626884, w1=-0.17527031920520758\n",
      "Reg Logistic regression iter. 5/19: loss=-14342.976718272328, w0=-0.2147587308723477, w1=-0.2004188315618374\n",
      "Reg Logistic regression iter. 6/19: loss=-11894.873580093541, w0=-0.2408104435644833, w1=-0.22415420626992782\n",
      "Reg Logistic regression iter. 7/19: loss=-9954.147949277789, w0=-0.2656728377006816, w1=-0.24679263128696324\n",
      "Reg Logistic regression iter. 8/19: loss=-8390.536497763096, w0=-0.2895806322423549, w1=-0.26856351446106036\n",
      "Reg Logistic regression iter. 9/19: loss=-7114.384469776403, w0=-0.3127118664214334, w1=-0.28963806459500113\n",
      "Reg Logistic regression iter. 10/19: loss=-6061.870308444927, w0=-0.3352044984941379, w1=-0.31014699317819727\n",
      "Reg Logistic regression iter. 11/19: loss=-5186.282536912325, w0=-0.35716730623417864, w1=-0.33019196372710496\n",
      "Reg Logistic regression iter. 12/19: loss=-4452.629254257233, w0=-0.37868730135409495, w1=-0.34985327054541643\n",
      "Reg Logistic regression iter. 13/19: loss=-3834.1774760446015, w0=-0.39983492494729217, w1=-0.3691951491264658\n",
      "Reg Logistic regression iter. 14/19: loss=-3310.158168865751, w0=-0.4206677819161928, w1=-0.38826954638067945\n",
      "Reg Logistic regression iter. 15/19: loss=-2864.2011497453364, w0=-0.44123338498197817, w1=-0.4071188588063398\n",
      "Reg Logistic regression iter. 16/19: loss=-2483.241348066989, w0=-0.4615712100059848, w1=-0.42577796079523283\n",
      "Reg Logistic regression iter. 17/19: loss=-2156.7378303100504, w0=-0.48171426159483516, w1=-0.444275733340297\n",
      "Reg Logistic regression iter. 18/19: loss=-1876.1053652265157, w0=-0.5016902834904533, w1=-0.4626362338994597\n",
      "Reg Logistic regression iter. 19/19: loss=-1634.2935265748158, w0=-0.5215227066917448, w1=-0.48087960379109257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-48754.7305094843,\n",
       "  -35707.24450984966,\n",
       "  -27400.74064238416,\n",
       "  -21668.506045273127,\n",
       "  -17495.7892680503,\n",
       "  -14342.976718272328,\n",
       "  -11894.873580093541,\n",
       "  -9954.147949277789,\n",
       "  -8390.536497763096,\n",
       "  -7114.384469776403,\n",
       "  -6061.870308444927,\n",
       "  -5186.282536912325,\n",
       "  -4452.629254257233,\n",
       "  -3834.1774760446015,\n",
       "  -3310.158168865751,\n",
       "  -2864.2011497453364,\n",
       "  -2483.241348066989,\n",
       "  -2156.7378303100504,\n",
       "  -1876.1053652265157,\n",
       "  -1634.2935265748158],\n",
       " [array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  array([-0.04887984, -0.04695974, -0.01189169, -0.02343984,  0.00090263,\n",
       "          0.02716833,  0.01637539, -0.05904454,  0.02763437,  0.02899441,\n",
       "          0.00793917,  0.02867824,  0.02867137, -0.00155803,  0.02868595,\n",
       "          0.02864994,  0.00266014,  0.0286886 , -0.09218265,  0.02814552,\n",
       "         -0.00808147]),\n",
       "  array([-0.08985714, -0.08554133, -0.02352073, -0.04277793,  0.00520394,\n",
       "          0.04993495,  0.0304316 , -0.10193141,  0.05078451,  0.05336131,\n",
       "          0.01567792,  0.05271987,  0.05270654, -0.00253167,  0.05273479,\n",
       "          0.05266833,  0.00565215,  0.05274081, -0.16239227,  0.05178642,\n",
       "         -0.00966024]),\n",
       "  array([-0.1255764 , -0.11870401, -0.03491624, -0.05946993,  0.01102353,\n",
       "          0.069777  ,  0.042809  , -0.13572889,  0.07094979,  0.07464335,\n",
       "          0.02312091,  0.07367648,  0.07365688, -0.00326036,  0.07369817,\n",
       "          0.0736052 ,  0.00860953,  0.07370807, -0.21974627,  0.07242117,\n",
       "         -0.00826737]),\n",
       "  array([-0.15768922, -0.14825708, -0.04611726, -0.0744087 ,  0.01748492,\n",
       "          0.08761534,  0.05396515, -0.16415267,  0.08906715,  0.09380573,\n",
       "          0.03027619,  0.09251709,  0.09249134, -0.00389904,  0.0925452 ,\n",
       "          0.09242859,  0.01141155,  0.09255936, -0.26922428,  0.09098846,\n",
       "         -0.00553405]),\n",
       "  array([-0.18719994, -0.17527032, -0.05715988, -0.08812225,  0.02420973,\n",
       "          0.10400897,  0.06420573, -0.18912096,  0.10570661,  0.11143653,\n",
       "          0.03718182,  0.10983098,  0.10979914, -0.00451246,  0.1098652 ,\n",
       "          0.10972712,  0.0140385 ,  0.10988393, -0.31349592,  0.10806091,\n",
       "         -0.00217354]),\n",
       "  array([-0.21475873, -0.20041883, -0.06807373, -0.10094291,  0.03102332,\n",
       "          0.11931941,  0.07374068, -0.21172456,  0.12123726,  0.12791711,\n",
       "          0.04387705,  0.12599962,  0.12596175, -0.00512912,  0.12603969,\n",
       "          0.12588184,  0.01650097,  0.12606323, -0.35412655,  0.12401021,\n",
       "          0.00147535]),\n",
       "  array([-0.24081044, -0.22415421, -0.0788824 , -0.11309208,  0.03784125,\n",
       "          0.13379379,  0.0827196 , -0.23263565,  0.13591173,  0.14350875,\n",
       "          0.05039597,  0.141284  ,  0.14124015, -0.00576174,  0.14132973,\n",
       "          0.14115344,  0.01881738,  0.14135828, -0.39210516,  0.13909118,\n",
       "          0.00524096]),\n",
       "  array([-0.26567284, -0.24679263, -0.08960462, -0.12472397,  0.04462225,\n",
       "          0.14760855,  0.09125258, -0.25229441,  0.14991047,  0.1583983 ,\n",
       "          0.05676689,  0.15587057,  0.15582077, -0.00641565,  0.15592177,\n",
       "          0.15572814,  0.02100675,  0.15595551, -0.42809291,  0.15348614,\n",
       "          0.00903292]),\n",
       "  array([-0.28958063, -0.26856351, -0.10025529, -0.13594984,  0.05134697,\n",
       "          0.16089394,  0.09942283, -0.27100277,  0.16336681,  0.17272394,\n",
       "          0.06301306,  0.16989705,  0.16984133, -0.00709255,  0.16995357,\n",
       "          0.16974348,  0.02308649,  0.16999264, -0.46255123,  0.16732999,\n",
       "          0.01280313]),\n",
       "  array([-0.31271187, -0.28963806, -0.11084639, -0.14685221,  0.05800764,\n",
       "          0.17374877,  0.10729466, -0.28897546,  0.17638187,  0.18659055,\n",
       "          0.06915358,  0.18346789,  0.18340628, -0.00779231,  0.18352962,\n",
       "          0.18330381,  0.02507179,  0.18357414, -0.49581324,  0.18072514,\n",
       "          0.01652648]),\n",
       "  array([-0.3352045 , -0.31014699, -0.12138763, -0.15749369,  0.06460266,\n",
       "          0.18624954,  0.11491867, -0.30636963,  0.18903404,  0.20007948,\n",
       "          0.07520421,  0.19666404,  0.19659656, -0.00851388,  0.19673086,\n",
       "          0.19648995,  0.02697576,  0.19678095, -0.5281259 ,  0.19375105,\n",
       "          0.02019085]),\n",
       "  array([-0.35716731, -0.33019196, -0.13188692, -0.1679227 ,  0.07113381,\n",
       "          0.1984566 ,  0.1223353 , -0.32330302,  0.20138511,  0.2132549 ,\n",
       "          0.08117806,  0.20954933,  0.209476  , -0.0092558 ,  0.20962114,\n",
       "          0.20936566,  0.02880956,  0.2096769 , -0.55967629,  0.20647044,\n",
       "          0.02379157]),\n",
       "  array([-0.3786873 , -0.34985327, -0.14235077, -0.17817719,  0.07760459,\n",
       "          0.21041819,  0.12957725, -0.33986549,  0.21348452,  0.22616818,\n",
       "          0.08708607,  0.2221748 ,  0.22209564, -0.01001641,  0.22225154,\n",
       "          0.22198191,  0.03058271,  0.22231303, -0.59060871,  0.21893348,\n",
       "          0.02732825]),\n",
       "  array([-0.39983492, -0.36919515, -0.15278458, -0.18828738,  0.08401929,\n",
       "          0.22217336,  0.13667129, -0.35612676,  0.22537229,  0.23886093,\n",
       "          0.09293742,  0.2345818 ,  0.23449681, -0.01079405,  0.23466338,\n",
       "          0.23437999,  0.03230335,  0.23473069, -0.62103609,  0.23118086,\n",
       "          0.03080296]),\n",
       "  array([-0.42066778, -0.38826955, -0.16319282, -0.19827752,  0.09038255,\n",
       "          0.23375406,  0.1436395 , -0.37214161,  0.23708117,  0.2513672 ,\n",
       "          0.0987399 ,  0.24680411,  0.2467133 , -0.01158711,  0.24689048,\n",
       "          0.24659364,  0.03397844,  0.24696366, -0.65104789,  0.24324584,\n",
       "          0.03421911]),\n",
       "  array([-0.44123338, -0.40711886, -0.17357927, -0.2081673 ,  0.09669898,\n",
       "          0.24518663,  0.15050024, -0.38795359,  0.24863814,  0.26371507,\n",
       "          0.1045001 ,  0.25886962,  0.258773  , -0.01239407,  0.25896072,\n",
       "          0.2586507 ,  0.03561394,  0.25903983, -0.68071576,  0.25515591,\n",
       "          0.03758077]),\n",
       "  array([-0.46157121, -0.42577796, -0.18394706, -0.21797281,  0.10297301,\n",
       "          0.25649294,  0.15726889, -0.40359764,  0.26006564,  0.27592789,\n",
       "          0.11022364,  0.27080148,  0.27069907, -0.01321352,  0.27089727,\n",
       "          0.27057432,  0.03721498,  0.27098235, -0.71009756,  0.26693387,\n",
       "          0.0408923 ]),\n",
       "  array([-0.48171426, -0.44427573, -0.19429887, -0.22770727,  0.10920883,\n",
       "          0.26769129,  0.16395837, -0.41910201,  0.27138243,  0.28802521,\n",
       "          0.11591535,  0.28261906,  0.28251086, -0.01404416,  0.28271951,\n",
       "          0.28238382,  0.03878599,  0.2828106 , -0.73924041,  0.27859884,\n",
       "          0.04415803]),\n",
       "  array([-0.50169028, -0.46263623, -0.20463696, -0.23738167,  0.1154103 ,\n",
       "          0.27879706,  0.17057965, -0.43448974,  0.28260427,  0.30002343,\n",
       "          0.12157934,  0.29433865,  0.29422466, -0.01488482,  0.29444371,\n",
       "          0.29409546,  0.04033077,  0.29454086, -0.7681829 ,  0.29016688,\n",
       "          0.04738217]),\n",
       "  array([-0.52152271, -0.4808796 , -0.21496324, -0.24700512,  0.12158099,\n",
       "          0.28982324,  0.17714203, -0.4497797 ,  0.29374449,  0.31193644,\n",
       "          0.1272192 ,  0.30597398,  0.30585421, -0.01573444,  0.30608363,\n",
       "          0.30572296,  0.04185263,  0.30618686, -0.79695683,  0.30165155,\n",
       "          0.05056872])])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28629568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cross_validation import * \n",
    "degree = 3\n",
    "k_fold = 4\n",
    "lambdas = np.logspace(-10,0,8)\n",
    "degrees = [3,5,7]\n",
    "seed = 1\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce751586",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.random.randint(0,1,len(tx.T))\n",
    "max_iters = 30\n",
    "gammas = np.arange(0.005,0.5,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc869e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21,)\n"
     ]
    }
   ],
   "source": [
    "print(initial_w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dd2a1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "For polynomial expansion up to degree, the choice of gamma which leads to the best rmse is 0.11500 with a test rmse of 0.887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.115, 0.887128695050202)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cross_validation_gradient_descent(y, tx, k_fold, initial_w=initial_w, max_iters = max_iters, gammas = gammas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "506df381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=-0.005, w1=-0.0032266509807341503\n",
      "SGD iter. 1/29: loss=0.48738773548531755, w0=-0.009690154280385426, w1=-0.008902179417477455\n",
      "SGD iter. 2/29: loss=0.4797962314816254, w0=-0.014360776128217387, w1=-0.013000795095695364\n",
      "SGD iter. 3/29: loss=0.4742164257491586, w0=-0.008406433211810671, w1=-0.007389863628374582\n",
      "SGD iter. 4/29: loss=0.4817498862136722, w0=-0.013054215245972552, w1=-0.013181343920798196\n",
      "SGD iter. 5/29: loss=0.47396156651453936, w0=-0.017591251062536693, w1=-0.017162733709191746\n",
      "SGD iter. 6/29: loss=0.4689548822852936, w0=-0.021709924904315808, w1=-0.020476962463584304\n",
      "SGD iter. 7/29: loss=0.4703110483805417, w0=-0.020783915634442596, w1=-0.019978099081733567\n",
      "SGD iter. 8/29: loss=0.47147540753472633, w0=-0.01916656137033746, w1=-0.018440392829621014\n",
      "SGD iter. 9/29: loss=0.4649415155881739, w0=-0.023478493365413567, w1=-0.02285647766959703\n",
      "SGD iter. 10/29: loss=0.46131090799647023, w0=-0.027500578951201067, w1=-0.027355423759972557\n",
      "SGD iter. 11/29: loss=0.4627127995685077, w0=-0.02026266289990121, w1=-0.01952928193127794\n",
      "SGD iter. 12/29: loss=0.48176147252792056, w0=-0.025811764699014868, w1=-0.04982985742595686\n",
      "SGD iter. 13/29: loss=0.4539674302069992, w0=-0.024634391526738113, w1=-0.0481205143056326\n",
      "SGD iter. 14/29: loss=0.4547019429589957, w0=-0.02858168933702063, w1=-0.05089612388029514\n",
      "SGD iter. 15/29: loss=0.45303223336811327, w0=-0.027522880995208904, w1=-0.05076724184332485\n",
      "SGD iter. 16/29: loss=0.4527992503548659, w0=-0.03144524273696794, w1=-0.052446318543324794\n",
      "SGD iter. 17/29: loss=0.45166010034439985, w0=-0.030383379958935496, w1=-0.050622998297186236\n",
      "SGD iter. 18/29: loss=0.45221322314695855, w0=-0.03420263172188036, w1=-0.053476996534499825\n",
      "SGD iter. 19/29: loss=0.4519407808232742, w0=-0.02757363145350562, w1=-0.046796948167056214\n",
      "SGD iter. 20/29: loss=0.45274815188014456, w0=-0.0317032855071611, w1=-0.0486183068068084\n",
      "SGD iter. 21/29: loss=0.4526958788903873, w0=-0.03522198918706451, w1=-0.05464662561856368\n",
      "SGD iter. 22/29: loss=0.4543944870088056, w0=-0.03351932890696178, w1=-0.05553893469748177\n",
      "SGD iter. 23/29: loss=0.45487022300091096, w0=-0.0373144119365348, w1=-0.057977535688073754\n",
      "SGD iter. 24/29: loss=0.4546991661232888, w0=-0.04073129930340935, w1=-0.06162785951175603\n",
      "SGD iter. 25/29: loss=0.4559349459413564, w0=-0.04413676838943953, w1=-0.06324686161728915\n",
      "SGD iter. 26/29: loss=0.46261819098854823, w0=-0.04211007320568232, w1=-0.06291021712304946\n",
      "SGD iter. 27/29: loss=0.46656478931271766, w0=-0.03483245159772924, w1=-0.054193881096451166\n",
      "SGD iter. 28/29: loss=0.44906374251841935, w0=-0.03860535119955646, w1=-0.05673433625943615\n",
      "SGD iter. 29/29: loss=0.45713581767905437, w0=-0.0421403985766875, w1=-0.05873299247145175\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=-0.005, w1=-0.00436118144425501\n",
      "SGD iter. 1/29: loss=0.4909118140968086, w0=-0.009824391768318872, w1=-0.01179681615887385\n",
      "SGD iter. 2/29: loss=0.479145116018015, w0=-0.009415249592435707, w1=-0.011243207816781812\n",
      "SGD iter. 3/29: loss=0.4805247342457063, w0=-0.009040225718039585, w1=-0.011056259385760905\n",
      "SGD iter. 4/29: loss=0.4828679137615276, w0=-0.003721625756907267, w1=-0.007158947730257034\n",
      "SGD iter. 5/29: loss=0.4953633474471871, w0=-0.003629068844176705, w1=-0.007535304566657317\n",
      "SGD iter. 6/29: loss=0.4960820060381628, w0=-0.0085657548184154, w1=-0.013045892586357261\n",
      "SGD iter. 7/29: loss=0.4840961865727599, w0=-0.013277991132504195, w1=-0.02068948731584605\n",
      "SGD iter. 8/29: loss=0.4742906204473523, w0=-0.01271174787136428, w1=-0.022047664129372292\n",
      "SGD iter. 9/29: loss=0.47787823272081587, w0=-0.017530618820123793, w1=-0.024676513776070746\n",
      "SGD iter. 10/29: loss=0.4687533192435867, w0=-0.02188252652024906, w1=-0.02931928214793167\n",
      "SGD iter. 11/29: loss=0.4648044324060831, w0=-0.025941169017105907, w1=-0.032177798403361255\n",
      "SGD iter. 12/29: loss=0.4652689409238568, w0=-0.02975423160015542, w1=-0.03531577531658578\n",
      "SGD iter. 13/29: loss=0.4677443505039647, w0=-0.03139121052793301, w1=-0.03637328277847836\n",
      "SGD iter. 14/29: loss=0.46413258791797957, w0=-0.03469950614370878, w1=-0.03909080996624798\n",
      "SGD iter. 15/29: loss=0.4729753500419146, w0=-0.03293774892691888, w1=-0.038998616152461564\n",
      "SGD iter. 16/29: loss=0.4692574477939029, w0=-0.03148556560749661, w1=-0.03860161521560111\n",
      "SGD iter. 17/29: loss=0.4636211458060113, w0=-0.029802671539248708, w1=-0.03958323733688858\n",
      "SGD iter. 18/29: loss=0.45851887069685243, w0=-0.03323278265429769, w1=-0.043352604219509316\n",
      "SGD iter. 19/29: loss=0.46964614249282877, w0=-0.03051889479956592, w1=-0.04076516023733714\n",
      "SGD iter. 20/29: loss=0.45565301231693445, w0=-0.030093854091143204, w1=-0.04342241856175085\n",
      "SGD iter. 21/29: loss=0.45862180013959003, w0=-0.0354139160807318, w1=-0.04746513201730818\n",
      "SGD iter. 22/29: loss=0.4479101609141686, w0=-0.039489376878300814, w1=-0.050201028202768935\n",
      "SGD iter. 23/29: loss=0.44885435653747235, w0=-0.037862862644701345, w1=-0.052067309182553805\n",
      "SGD iter. 24/29: loss=0.44659899061332053, w0=-0.04193853525806093, w1=-0.05456954226762678\n",
      "SGD iter. 25/29: loss=0.45546827104241044, w0=-0.04548323565008588, w1=-0.057680126912385055\n",
      "SGD iter. 26/29: loss=0.456836928663602, w0=-0.043771072713900835, w1=-0.05523984555827367\n",
      "SGD iter. 27/29: loss=0.4560934231363369, w0=-0.046831733020987296, w1=-0.06038140444542695\n",
      "SGD iter. 28/29: loss=0.4589290760524311, w0=-0.044978407139647236, w1=-0.059298225120946554\n",
      "SGD iter. 29/29: loss=0.45174695040578133, w0=-0.048487850538439195, w1=-0.061159278707996825\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.0, w1=0.001203792146477976\n",
      "SGD iter. 1/29: loss=0.4973531429066851, w0=0.00011945429157843052, w1=0.0016865337322443013\n",
      "SGD iter. 2/29: loss=0.4947120007450941, w0=0.0003494862009054825, w1=0.003496187117804239\n",
      "SGD iter. 3/29: loss=0.49739584524877206, w0=0.00039547934666345794, w1=0.004149637890137768\n",
      "SGD iter. 4/29: loss=0.49893056169167527, w0=0.005409973584741742, w1=0.009414544199006728\n",
      "SGD iter. 5/29: loss=0.5154782118681723, w0=0.0050979935345467076, w1=0.010430501967177971\n",
      "SGD iter. 6/29: loss=0.5217557328010156, w0=-0.0002759874655073125, w1=0.009201265239194735\n",
      "SGD iter. 7/29: loss=0.5026441933173754, w0=0.004643128835402613, w1=0.014011162384218727\n",
      "SGD iter. 8/29: loss=0.5201781814466245, w0=0.004298961654106691, w1=0.013781084963718429\n",
      "SGD iter. 9/29: loss=0.5237433255108596, w0=0.0039963479003660455, w1=0.013747325466135078\n",
      "SGD iter. 10/29: loss=0.5242046162072702, w0=-0.0014424676071698355, w1=0.009823062665840505\n",
      "SGD iter. 11/29: loss=0.5017322161368474, w0=-0.006561319424612531, w1=-0.0018513787542107536\n",
      "SGD iter. 12/29: loss=0.48411512201035894, w0=-0.01139085265898365, w1=-0.007651990988377534\n",
      "SGD iter. 13/29: loss=0.47420178408343455, w0=-0.0158636122249254, w1=-0.009244303593749106\n",
      "SGD iter. 14/29: loss=0.4685320398948491, w0=-0.020300958566654356, w1=-0.013138212772959434\n",
      "SGD iter. 15/29: loss=0.4639648333003131, w0=-0.019607634340251725, w1=-0.012348636018087257\n",
      "SGD iter. 16/29: loss=0.46412354454491805, w0=-0.018947587404795917, w1=-0.011701694216242859\n",
      "SGD iter. 17/29: loss=0.4648464484492545, w0=-0.023309329946727218, w1=-0.013384344350318897\n",
      "SGD iter. 18/29: loss=0.4631928355512727, w0=-0.02189953894480516, w1=-0.010348699310718447\n",
      "SGD iter. 19/29: loss=0.4638334516661887, w0=-0.01609956741943873, w1=-0.005479793513823942\n",
      "SGD iter. 20/29: loss=0.4690843847556099, w0=-0.010463616907994227, w1=-5.6628242552092534e-05\n",
      "SGD iter. 21/29: loss=0.4782173942143965, w0=-0.010166355478369008, w1=0.0001762215208357669\n",
      "SGD iter. 22/29: loss=0.4800109121455961, w0=-0.010053708264415569, w1=0.0013286065473573619\n",
      "SGD iter. 23/29: loss=0.48142672526073826, w0=-0.005092960340338076, w1=0.006398723358944814\n",
      "SGD iter. 24/29: loss=0.5121551013737524, w0=-0.0053159373494029265, w1=-0.0011145335575767816\n",
      "SGD iter. 25/29: loss=0.49971482217307567, w0=-0.005410678872105609, w1=0.00025194900540044117\n",
      "SGD iter. 26/29: loss=0.5033914968251443, w0=-0.010185756741787455, w1=-0.004188008704475877\n",
      "SGD iter. 27/29: loss=0.4928165309257811, w0=-0.010265916168926742, w1=-0.0031033404030741995\n",
      "SGD iter. 28/29: loss=0.4935743757404562, w0=-0.015090039733236343, w1=-0.007336658603201617\n",
      "SGD iter. 29/29: loss=0.479264600311525, w0=-0.01465998262484792, w1=-0.00729733344194921\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.0, w1=0.0007119918654712432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 1/29: loss=0.5038527169428592, w0=-0.005054236537646834, w1=-0.003341857357981879\n",
      "SGD iter. 2/29: loss=0.4916382261609393, w0=-0.009916590209455359, w1=-0.006500165488317101\n",
      "SGD iter. 3/29: loss=0.481957044804486, w0=-0.004587755609610541, w1=-0.002277292876456047\n",
      "SGD iter. 4/29: loss=0.49309009416737115, w0=-0.004476726990482019, w1=-0.005896869681629641\n",
      "SGD iter. 5/29: loss=0.49400341192295827, w0=-0.004521955506288895, w1=-0.004794915152702899\n",
      "SGD iter. 6/29: loss=0.4923001798101985, w0=-0.009395003198611066, w1=-0.00825196526187854\n",
      "SGD iter. 7/29: loss=0.48242853039684075, w0=-0.01401978475937925, w1=-0.011052427669346209\n",
      "SGD iter. 8/29: loss=0.4762256432241899, w0=-0.008347442254553958, w1=-0.004477650042917027\n",
      "SGD iter. 9/29: loss=0.48530254946675816, w0=-0.007933448693892205, w1=-4.024940695301904e-05\n",
      "SGD iter. 10/29: loss=0.4863165602783808, w0=-0.007654197903867968, w1=0.0012479239030256284\n",
      "SGD iter. 11/29: loss=0.48705690861678, w0=-0.007363464929253908, w1=0.003164001396946183\n",
      "SGD iter. 12/29: loss=0.4895406394778374, w0=-0.007182326957808779, w1=0.005377458724994671\n",
      "SGD iter. 13/29: loss=0.489038855385666, w0=-0.012031713973825133, w1=0.001479723638898442\n",
      "SGD iter. 14/29: loss=0.4806367924762832, w0=-0.016551030124785215, w1=-0.004902125474735796\n",
      "SGD iter. 15/29: loss=0.47528003208386216, w0=-0.020932919373511716, w1=-0.007368985116294247\n",
      "SGD iter. 16/29: loss=0.47228742282361064, w0=-0.020107041406464993, w1=-0.009134031637784201\n",
      "SGD iter. 17/29: loss=0.4699845045095447, w0=-0.013662084474220758, w1=-0.003176091897867819\n",
      "SGD iter. 18/29: loss=0.47633242367458817, w0=-0.018276383156089256, w1=-0.006377510466918958\n",
      "SGD iter. 19/29: loss=0.4679468493340152, w0=-0.017730641505218813, w1=-0.005158673307547707\n",
      "SGD iter. 20/29: loss=0.47115647453883336, w0=-0.017677824857429258, w1=-0.0035819427510035634\n",
      "SGD iter. 21/29: loss=0.4681224519036494, w0=-0.016955368476511218, w1=-0.005280055592545989\n",
      "SGD iter. 22/29: loss=0.46882281284779953, w0=-0.02137490884824164, w1=-0.00941259781032001\n",
      "SGD iter. 23/29: loss=0.4635188831373717, w0=-0.020634150636175023, w1=-0.008121104226257038\n",
      "SGD iter. 24/29: loss=0.4644501209717083, w0=-0.019909382637162958, w1=-0.006834060219704718\n",
      "SGD iter. 25/29: loss=0.4665048129198389, w0=-0.023740518166652718, w1=-0.00896045759766508\n",
      "SGD iter. 26/29: loss=0.47061876552704446, w0=-0.02239618885790347, w1=-0.004884641416377107\n",
      "SGD iter. 27/29: loss=0.46405079577836605, w0=-0.026689220823670208, w1=-0.008842878429356615\n",
      "SGD iter. 28/29: loss=0.46123609636311097, w0=-0.020689839781394024, w1=-0.0031771807046104853\n",
      "SGD iter. 29/29: loss=0.46344837668374816, w0=-0.02505872640614981, w1=-0.006103927070338687\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.0, w1=-0.014922001388535642\n",
      "SGD iter. 1/29: loss=0.5320112134452298, w0=-0.05452504363118559, w1=-0.07468543724189451\n",
      "SGD iter. 2/29: loss=0.43797987884426437, w0=-0.09213080140758988, w1=-0.10168311173604719\n",
      "SGD iter. 3/29: loss=0.6415746419906267, w0=-0.04235318132767639, w1=-0.02852019885568395\n",
      "SGD iter. 4/29: loss=0.5037093221064264, w0=-0.03046656793379253, w1=-0.02233838776719966\n",
      "SGD iter. 5/29: loss=0.5444735514614645, w0=0.03100432990441746, w1=0.04966437636070744\n",
      "SGD iter. 6/29: loss=0.9221533231872853, w0=-0.05277116093339492, w1=-0.061404348487260806\n",
      "SGD iter. 7/29: loss=0.4416384271746529, w0=-0.025647029124904058, w1=0.0225895847571966\n",
      "SGD iter. 8/29: loss=0.5416877472325277, w0=-0.08949668927320946, w1=-0.042226546758929485\n",
      "SGD iter. 9/29: loss=0.5757845887946657, w0=-0.11251466041222555, w1=-0.062425531062257504\n",
      "SGD iter. 10/29: loss=0.6702039116295767, w0=-0.11880118404976259, w1=-0.0604423868088051\n",
      "SGD iter. 11/29: loss=0.695786422521924, w0=-0.06494310078556702, w1=0.00798949701988904\n",
      "SGD iter. 12/29: loss=0.5348344276733477, w0=-0.05152946639356469, w1=0.01746576809993161\n",
      "SGD iter. 13/29: loss=0.5584277436749414, w0=-0.09379940588415774, w1=-0.0063225527684255545\n",
      "SGD iter. 14/29: loss=0.46709247508843715, w0=-0.030154112490408863, w1=0.07383921306225254\n",
      "SGD iter. 15/29: loss=0.9455390673850349, w0=-0.1813172321242735, w1=0.011439538118860379\n",
      "SGD iter. 16/29: loss=12.842466876023083, w0=-0.15988651734239612, w1=0.018839446262539382\n",
      "SGD iter. 17/29: loss=12.202997069569165, w0=-0.00010108323424776189, w1=0.16823791026976703\n",
      "SGD iter. 18/29: loss=5.820338028762058, w0=-0.0352630248952498, w1=0.0724419892425224\n",
      "SGD iter. 19/29: loss=6.054960982685555, w0=0.02096640866977379, w1=0.04775213054156287\n",
      "SGD iter. 20/29: loss=2.3914810920467193, w0=0.044359233420283674, w1=0.052847337820654094\n",
      "SGD iter. 21/29: loss=1.510840799041889, w0=0.13919609914169329, w1=0.09181453648855212\n",
      "SGD iter. 22/29: loss=1.3260141595075323, w0=0.13571940522658155, w1=0.08855015310094295\n",
      "SGD iter. 23/29: loss=1.2915778367100732, w0=0.1324708689272371, w1=0.08434088775342581\n",
      "SGD iter. 24/29: loss=1.2422370235490099, w0=0.07364501552472039, w1=-0.008225301100827379\n",
      "SGD iter. 25/29: loss=0.8193173409601938, w0=-0.015099222197382065, w1=-0.07754173824162387\n",
      "SGD iter. 26/29: loss=0.613429086192246, w0=-0.0704148170986684, w1=-0.11561952280317775\n",
      "SGD iter. 27/29: loss=0.8953980975007445, w0=0.0347646628166043, w1=-0.010245637467429833\n",
      "SGD iter. 28/29: loss=2.1980929256828796, w0=-0.034369821617267426, w1=-0.08386400278823347\n",
      "SGD iter. 29/29: loss=0.7897141953240685, w0=-0.07631495601783839, w1=-0.10820152283021\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.0, w1=0.002341155598441369\n",
      "SGD iter. 1/29: loss=0.49207026275738597, w0=0.004900481649140986, w1=0.0264541278069252\n",
      "SGD iter. 2/29: loss=0.557288009419228, w0=0.001988317187799466, w1=0.028431344392789756\n",
      "SGD iter. 3/29: loss=0.5088732783448622, w0=-0.0005818818671144729, w1=0.03258880302925089\n",
      "SGD iter. 4/29: loss=0.5230974164864932, w0=0.0342948398627983, w1=0.06190768112822545\n",
      "SGD iter. 5/29: loss=0.6169772865272692, w0=-0.0351181841006485, w1=0.009964101237847522\n",
      "SGD iter. 6/29: loss=0.9769209791581612, w0=-0.025436600912952734, w1=0.034244461225803616\n",
      "SGD iter. 7/29: loss=0.780856723248329, w0=-0.04506923155667998, w1=0.017016218310271215\n",
      "SGD iter. 8/29: loss=0.9709008789694417, w0=-0.07372819438621618, w1=-0.004838577485166973\n",
      "SGD iter. 9/29: loss=1.188775927524712, w0=-0.0026310672870943352, w1=0.07265113854642655\n",
      "SGD iter. 10/29: loss=0.537223879885491, w0=-0.009567006652222829, w1=0.06858553400048983\n",
      "SGD iter. 11/29: loss=0.5183909619962753, w0=-0.01675233327801211, w1=0.005058884954340648\n",
      "SGD iter. 12/29: loss=0.7590360744726475, w0=-0.020281566559895714, w1=-0.0012638964349824195\n",
      "SGD iter. 13/29: loss=0.7625735402651692, w0=0.08310793119958478, w1=0.06822543761137143\n",
      "SGD iter. 14/29: loss=2.133247462287296, w0=0.08659967218044938, w1=0.07018678292042152\n",
      "SGD iter. 15/29: loss=2.0820656799953405, w0=-0.04883564076354861, w1=-0.23920721260161004\n",
      "SGD iter. 16/29: loss=0.5353599180645986, w0=0.012993001632232846, w1=-0.16151832953423823\n",
      "SGD iter. 17/29: loss=0.7460151696370579, w0=-0.01611564406656512, w1=-0.1767902432655151\n",
      "SGD iter. 18/29: loss=0.5550243735236212, w0=-0.07216981347123486, w1=-0.21686908090710355\n",
      "SGD iter. 19/29: loss=0.44606306878165336, w0=-0.10615215648296558, w1=-0.25106337855008115\n",
      "SGD iter. 20/29: loss=0.5340125428844656, w0=-0.10414367145747816, w1=-0.21319999532027645\n",
      "SGD iter. 21/29: loss=0.49337405288254754, w0=-0.10954629035735344, w1=-0.2234609298051827\n",
      "SGD iter. 22/29: loss=0.5497289688782127, w0=-0.1131010555198733, w1=-0.2190121453903535\n",
      "SGD iter. 23/29: loss=0.5400175427350085, w0=-0.017284808246559077, w1=-0.14143590822562657\n",
      "SGD iter. 24/29: loss=0.5656559532297316, w0=-0.015195094857215825, w1=-0.09875945854394327\n",
      "SGD iter. 25/29: loss=0.6550405963655879, w0=-0.02510178550238682, w1=-0.10061135613322232\n",
      "SGD iter. 26/29: loss=0.6523317794138621, w0=-0.08739508762484183, w1=-0.1579669425553354\n",
      "SGD iter. 27/29: loss=0.43990355767294176, w0=-0.13147459063892697, w1=-0.16653913651098598\n",
      "SGD iter. 28/29: loss=0.6492140117848295, w0=-0.14239401344490743, w1=-0.1723947832173801\n",
      "SGD iter. 29/29: loss=0.7522584007599876, w0=-0.0942319265852927, w1=-0.15297310204084424\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.0, w1=0.014147227894131009\n",
      "SGD iter. 1/29: loss=0.49422707336744026, w0=-0.05415306030299047, w1=-0.11792297072960822\n",
      "SGD iter. 2/29: loss=0.5596255935797605, w0=-0.02395651553094455, w1=-0.060493180719650014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 3/29: loss=0.46193008996364643, w0=-0.07279448340277048, w1=-0.13927093036071359\n",
      "SGD iter. 4/29: loss=0.5682852143232103, w0=-0.03370806528750745, w1=-0.1173269795671015\n",
      "SGD iter. 5/29: loss=0.4432602211336715, w0=-0.07154939445287167, w1=-0.14094332061612574\n",
      "SGD iter. 6/29: loss=0.48282956492009216, w0=-0.03747046518022447, w1=-0.10771270508993296\n",
      "SGD iter. 7/29: loss=0.4331125458569411, w0=-0.06915623841601902, w1=-0.14050966530858333\n",
      "SGD iter. 8/29: loss=0.545013448801056, w0=-0.08898865941095117, w1=-0.15540237037398472\n",
      "SGD iter. 9/29: loss=0.6954388214238832, w0=-0.03955564067611613, w1=-0.0803982538816301\n",
      "SGD iter. 10/29: loss=0.43251113375641914, w0=-0.07476493290677172, w1=-0.10458354870460994\n",
      "SGD iter. 11/29: loss=0.48067098155973526, w0=0.019626116750714878, w1=-0.012857353261085225\n",
      "SGD iter. 12/29: loss=0.7638361583333072, w0=-0.05101310782655678, w1=-0.0760359363796182\n",
      "SGD iter. 13/29: loss=0.4439996979275357, w0=-0.080432405110588, w1=-0.11374433218997368\n",
      "SGD iter. 14/29: loss=0.4392307336059748, w0=-0.09612048005195249, w1=-0.14940625997464962\n",
      "SGD iter. 15/29: loss=0.47866774051635175, w0=-0.0062544387986975675, w1=-0.06314442088905807\n",
      "SGD iter. 16/29: loss=0.727380605059645, w0=-0.007178482670923319, w1=-0.08400533986657012\n",
      "SGD iter. 17/29: loss=0.729825000468716, w0=-0.07183978059508674, w1=-0.15100826535128725\n",
      "SGD iter. 18/29: loss=0.44975353679842234, w0=-0.0451405975021202, w1=-0.10331570059646428\n",
      "SGD iter. 19/29: loss=0.505865051666999, w0=-0.053696771036466626, w1=-0.10148601295719502\n",
      "SGD iter. 20/29: loss=0.49511803884632166, w0=-0.035245311032347376, w1=-0.07383202291445459\n",
      "SGD iter. 21/29: loss=0.5195439688889095, w0=-0.07637372012789517, w1=-0.10550201957930658\n",
      "SGD iter. 22/29: loss=0.4691069573656101, w0=-0.1069990486251815, w1=-0.13071054196817228\n",
      "SGD iter. 23/29: loss=0.45174189746383137, w0=-0.07972863781872758, w1=-0.09704138555187733\n",
      "SGD iter. 24/29: loss=0.6072457399355201, w0=-0.10432884547582844, w1=-0.10787228387976809\n",
      "SGD iter. 25/29: loss=0.47753768755186976, w0=-0.12448581702099673, w1=-0.13151891658700313\n",
      "SGD iter. 26/29: loss=0.4642926705016983, w0=-0.1604850051976219, w1=-0.14841862082732693\n",
      "SGD iter. 27/29: loss=0.9158643542120158, w0=-0.10869599757308797, w1=-0.11458806068998754\n",
      "SGD iter. 28/29: loss=0.4869465178419163, w0=-0.07526714522145189, w1=-0.06627490145237061\n",
      "SGD iter. 29/29: loss=0.48595428266520496, w0=-0.034427140085477946, w1=-0.04364637055837366\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.0, w1=0.00827033990397992\n",
      "SGD iter. 1/29: loss=0.4876464156839858, w0=0.003247934580047757, w1=0.043048641643512596\n",
      "SGD iter. 2/29: loss=0.6792654462252851, w0=-0.004263915207151095, w1=0.016507219735959193\n",
      "SGD iter. 3/29: loss=0.5176910892798872, w0=0.045260904655709, w1=0.06485591984619263\n",
      "SGD iter. 4/29: loss=0.9568544180141815, w0=0.006968557848733917, w1=0.03532534036618097\n",
      "SGD iter. 5/29: loss=0.5826207485307217, w0=-0.005780193176439529, w1=0.026141216933933724\n",
      "SGD iter. 6/29: loss=0.47122701804282174, w0=-0.002957133939204088, w1=0.03629934365855421\n",
      "SGD iter. 7/29: loss=0.5529826218573369, w0=-0.06553280128864632, w1=-0.011870889275118084\n",
      "SGD iter. 8/29: loss=0.5293740953724698, w0=-0.09457741440191593, w1=-0.03439909122963632\n",
      "SGD iter. 9/29: loss=0.6278677827893774, w0=-0.10869099663249882, w1=-0.0536679381792676\n",
      "SGD iter. 10/29: loss=0.7233362808214739, w0=-0.06558183500168888, w1=-0.002423263116527373\n",
      "SGD iter. 11/29: loss=0.49745931176990377, w0=-0.06814712450231229, w1=0.001833730550922178\n",
      "SGD iter. 12/29: loss=0.6568804921760725, w0=-0.05978657985528708, w1=0.009645607044903009\n",
      "SGD iter. 13/29: loss=0.5947600752699337, w0=-0.1350248364448761, w1=-0.06057580921335336\n",
      "SGD iter. 14/29: loss=1.0074071397951536, w0=-0.014679297071619232, w1=0.053806298928510396\n",
      "SGD iter. 15/29: loss=0.5090483940034574, w0=-0.07027709658150627, w1=-0.012730584618980606\n",
      "SGD iter. 16/29: loss=0.4297096351930722, w0=-0.10720890641753791, w1=-0.04314848279004252\n",
      "SGD iter. 17/29: loss=0.6753192022238034, w0=0.005245504958877545, w1=0.08255158712275984\n",
      "SGD iter. 18/29: loss=2.005563618819496, w0=-0.0871673036351795, w1=-0.0117234087663253\n",
      "SGD iter. 19/29: loss=0.4186239242128896, w0=-0.060348422178608946, w1=0.02137016422160165\n",
      "SGD iter. 20/29: loss=0.457879076397896, w0=-0.04659346890596167, w1=0.04483419528477634\n",
      "SGD iter. 21/29: loss=0.446507364707104, w0=-0.0071080990861088905, w1=0.08402602492117546\n",
      "SGD iter. 22/29: loss=0.9361798519673166, w0=-0.08470830864294762, w1=0.03408927140104594\n",
      "SGD iter. 23/29: loss=0.4393252843475634, w0=-0.07038033119835582, w1=0.033561638077180156\n",
      "SGD iter. 24/29: loss=0.43595485245686766, w0=-0.11191374153850152, w1=0.005538277787843781\n",
      "SGD iter. 25/29: loss=0.4227406898175096, w0=-0.09533527971003164, w1=0.024488516704492164\n",
      "SGD iter. 26/29: loss=0.662119872301818, w0=-0.1838888975475026, w1=-0.020740861963163926\n",
      "SGD iter. 27/29: loss=1.548496384700312, w0=-0.10323680958038975, w1=0.04771738091334189\n",
      "SGD iter. 28/29: loss=0.4733538644131058, w0=-0.07761387050268036, w1=0.08043957088213886\n",
      "SGD iter. 29/29: loss=0.4407136251436209, w0=-0.11790246128660124, w1=0.05372292323922512\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=-0.10500000000000001, w1=-0.05304444480010093\n",
      "SGD iter. 1/29: loss=4.577050046809098, w0=-0.019476654082259418, w1=0.01136011901711826\n",
      "SGD iter. 2/29: loss=2.373522197919523, w0=0.11477864776270856, w1=0.14029554687550147\n",
      "SGD iter. 3/29: loss=1.0222711433421159, w0=0.2522363475759149, w1=0.27299088947611294\n",
      "SGD iter. 4/29: loss=5.709684991510021, w0=0.16838457353055888, w1=0.21766918159186727\n",
      "SGD iter. 5/29: loss=4.054246678934079, w0=0.020973138116909373, w1=0.02883405821012794\n",
      "SGD iter. 6/29: loss=0.5918056608877068, w0=-0.09727848548131633, w1=-0.022889962017428787\n",
      "SGD iter. 7/29: loss=0.5259576297920583, w0=0.014099563149056346, w1=0.09901148701841089\n",
      "SGD iter. 8/29: loss=3.2677230239554715, w0=-0.11108191077535055, w1=0.0008198072528645961\n",
      "SGD iter. 9/29: loss=0.8092770136533693, w0=-0.3308713214321505, w1=-0.06291868917409715\n",
      "SGD iter. 10/29: loss=26.156966053432694, w0=0.00891882739221972, w1=0.26092115691794826\n",
      "SGD iter. 11/29: loss=9.72152290636102, w0=0.14592888342504098, w1=0.39984157905725104\n",
      "SGD iter. 12/29: loss=2.5270378751438445, w0=0.00333530363010745, w1=0.262389169315593\n",
      "SGD iter. 13/29: loss=3.7988329796602547, w0=0.12148337451120421, w1=0.3518909040852706\n",
      "SGD iter. 14/29: loss=1.6268838875883336, w0=0.0526609944689312, w1=0.10537209391702984\n",
      "SGD iter. 15/29: loss=1.1402688614079792, w0=-0.07711696249488083, w1=-0.0020079845148244374\n",
      "SGD iter. 16/29: loss=2.3387065609947233, w0=0.000785086567913415, w1=0.1495431797512497\n",
      "SGD iter. 17/29: loss=0.7071902160534778, w0=-0.16382091430944629, w1=0.01078955517391117\n",
      "SGD iter. 18/29: loss=3.452680215306209, w0=0.15068010424766753, w1=0.2474459107663509\n",
      "SGD iter. 19/29: loss=18.290005395044727, w0=-0.242338190428033, w1=-0.08827005411837321\n",
      "SGD iter. 20/29: loss=0.9953589422902962, w0=-0.2313254310632361, w1=-0.08162434504254971\n",
      "SGD iter. 21/29: loss=1.0803898722202125, w0=-0.20183989739329786, w1=-0.029213094366401904\n",
      "SGD iter. 22/29: loss=0.6275255055014395, w0=-0.17668630854058448, w1=-0.08430065597444605\n",
      "SGD iter. 23/29: loss=1.0716682915383324, w0=0.010802694335503127, w1=0.11270985043987716\n",
      "SGD iter. 24/29: loss=1.8486167179837998, w0=-0.16638364907623207, w1=-0.17021151628734302\n",
      "SGD iter. 25/29: loss=0.6429752926789017, w0=-0.20556483739074896, w1=-0.17018323500493282\n",
      "SGD iter. 26/29: loss=0.8332987830573062, w0=-0.07631007324115913, w1=-0.031352313888833055\n",
      "SGD iter. 27/29: loss=0.7934368617540971, w0=-0.13843241249068405, w1=-0.08586659526527626\n",
      "SGD iter. 28/29: loss=0.5475308206287204, w0=-0.18547162713872414, w1=-0.12066706023561258\n",
      "SGD iter. 29/29: loss=0.6616181496098384, w0=-0.003345404474136948, w1=0.048233850076420476\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.10500000000000001, w1=0.11414419036502056\n",
      "SGD iter. 1/29: loss=1.9221814225708893, w0=0.017892739487772383, w1=0.009388314846712972\n",
      "SGD iter. 2/29: loss=1.0252339797199876, w0=-0.12269421078911165, w1=-0.09791202362749658\n",
      "SGD iter. 3/29: loss=0.5293331537118996, w0=-0.14385928137884557, w1=-0.11609734157319934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 4/29: loss=0.7678453590049177, w0=-0.0343840509744663, w1=-0.03537160743283606\n",
      "SGD iter. 5/29: loss=0.43310922057947715, w0=-0.11626086006179182, w1=-0.16134319241849765\n",
      "SGD iter. 6/29: loss=2.235022975121044, w0=0.07834071461376191, w1=0.1356471659097956\n",
      "SGD iter. 7/29: loss=1.6879334448385812, w0=-0.14658460953633856, w1=-0.15022160542775892\n",
      "SGD iter. 8/29: loss=1.6144651287450653, w0=-0.08801383931747746, w1=-0.14164857413549747\n",
      "SGD iter. 9/29: loss=0.5462007071374485, w0=0.0018755764366101546, w1=-0.04150476165883675\n",
      "SGD iter. 10/29: loss=3.001102240604686, w0=-0.15459437547225877, w1=-0.16270458575810476\n",
      "SGD iter. 11/29: loss=1.0617685102111565, w0=-0.18738816798571156, w1=-0.1995391150038737\n",
      "SGD iter. 12/29: loss=0.839843774152138, w0=-0.12384231903437909, w1=-0.14393879376501728\n",
      "SGD iter. 13/29: loss=1.253014646803564, w0=-0.0806414706832627, w1=-0.021270804383243788\n",
      "SGD iter. 14/29: loss=1.12831697413908, w0=-0.36282095627314187, w1=-0.2683456777961553\n",
      "SGD iter. 15/29: loss=33.58348419827621, w0=0.2500107401673826, w1=0.2694331621469536\n",
      "SGD iter. 16/29: loss=2.681593919425835, w0=0.16248266115636292, w1=0.24599984198856176\n",
      "SGD iter. 17/29: loss=2.7143441131915815, w0=0.004486946383588014, w1=0.11193616045170315\n",
      "SGD iter. 18/29: loss=3.9953020967643114, w0=0.2630261870577907, w1=0.4394904586567222\n",
      "SGD iter. 19/29: loss=4.980936620057587, w0=-0.12442032656347302, w1=0.09949411407230563\n",
      "SGD iter. 20/29: loss=2.6521245989365148, w0=-0.19252590144052484, w1=0.05328676785446295\n",
      "SGD iter. 21/29: loss=3.2192967142290696, w0=0.08727609755048338, w1=0.2675801730312751\n",
      "SGD iter. 22/29: loss=4.468757547419821, w0=-0.06868224043369861, w1=0.11594069499466639\n",
      "SGD iter. 23/29: loss=1.8777750108568245, w0=-0.1491663200380194, w1=0.07073507052030019\n",
      "SGD iter. 24/29: loss=0.9909724504830295, w0=-0.2685112398768258, w1=0.0439995106482773\n",
      "SGD iter. 25/29: loss=0.6771805967872587, w0=-0.16408369150061647, w1=0.2554917937553847\n",
      "SGD iter. 26/29: loss=3.5982655400627417, w0=-0.3312091009567222, w1=0.1942530684072941\n",
      "SGD iter. 27/29: loss=0.7997648209907231, w0=-0.2941263254613491, w1=0.281857876360522\n",
      "SGD iter. 28/29: loss=0.7731394568916476, w0=-0.3198648486308056, w1=0.23660536320025533\n",
      "SGD iter. 29/29: loss=0.8248342581494988, w0=-0.15311815142720314, w1=0.47140474662839915\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.10500000000000001, w1=0.09697403355036396\n",
      "SGD iter. 1/29: loss=1.5717566203250406, w0=0.0050579001636572585, w1=0.0019132715212073353\n",
      "SGD iter. 2/29: loss=0.6671460175322496, w0=-0.10351972400708369, w1=-0.08685828943128959\n",
      "SGD iter. 3/29: loss=0.47037869107944696, w0=-0.15932028551279426, w1=-0.1435231381082603\n",
      "SGD iter. 4/29: loss=1.0579348777068913, w0=0.09149006803592014, w1=0.06574512661416043\n",
      "SGD iter. 5/29: loss=7.989661999711398, w0=-0.08675719797153689, w1=-0.06539155684275905\n",
      "SGD iter. 6/29: loss=4.499917158089376, w0=-0.0615593509880051, w1=-0.03889902362932063\n",
      "SGD iter. 7/29: loss=5.354131806871789, w0=-0.21428115565718078, w1=-0.25408826904868126\n",
      "SGD iter. 8/29: loss=1.6620858492980128, w0=-0.23226424280489602, w1=-0.2591177287882302\n",
      "SGD iter. 9/29: loss=1.5843210903801233, w0=-0.32787838399471736, w1=-0.30330667605522244\n",
      "SGD iter. 10/29: loss=1.5951007519099503, w0=-0.19398092385408278, w1=-0.1858074977261729\n",
      "SGD iter. 11/29: loss=0.7851169708014015, w0=-0.12638497750955266, w1=-0.11768035012300854\n",
      "SGD iter. 12/29: loss=1.3931796867285389, w0=-0.2269426756318348, w1=-0.18501948013039113\n",
      "SGD iter. 13/29: loss=1.1964888228297967, w0=-0.19687379356284562, w1=-0.16339614687340684\n",
      "SGD iter. 14/29: loss=0.9229451345473322, w0=-0.13246323787522069, w1=-0.0705995384303306\n",
      "SGD iter. 15/29: loss=0.6365794653651292, w0=-0.17978070752840636, w1=-0.0998495592996484\n",
      "SGD iter. 16/29: loss=0.5820506837694988, w0=-0.080211189786023, w1=-0.0006649512554354614\n",
      "SGD iter. 17/29: loss=0.9275251145578963, w0=-0.12509185934163514, w1=-0.04075978672454453\n",
      "SGD iter. 18/29: loss=0.6737913658045224, w0=-0.11144997729081134, w1=-0.014158268292732059\n",
      "SGD iter. 19/29: loss=0.4960511437217658, w0=-0.0982904723187989, w1=-0.03187541583064289\n",
      "SGD iter. 20/29: loss=0.46004964263756215, w0=-0.05044055796876179, w1=0.0061304177345932395\n",
      "SGD iter. 21/29: loss=0.4359961123490081, w0=-0.12776276959265884, w1=-0.056572876755965874\n",
      "SGD iter. 22/29: loss=0.8155784302899899, w0=-0.10361160836100365, w1=6.753302381017068e-05\n",
      "SGD iter. 23/29: loss=0.5443586635156824, w0=0.08064365617914164, w1=0.1916635786404608\n",
      "SGD iter. 24/29: loss=6.250435463196106, w0=-0.5970016382830261, w1=-0.01729266384222211\n",
      "SGD iter. 25/29: loss=110.24841289760194, w0=0.8753882666744641, w1=2.1742076567826314\n",
      "SGD iter. 26/29: loss=141.3945531621718, w0=-1.130271169006673, w1=0.7702103561646412\n",
      "SGD iter. 27/29: loss=292.38708654971515, w0=4.686041527985281, w1=6.184763477906351\n",
      "SGD iter. 28/29: loss=22196.639862638433, w0=-14.625835460006158, w1=-6.5029421717147216\n",
      "SGD iter. 29/29: loss=30974.303001183744, w0=2.591019364924872, w1=0.8117017208581192\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.0, w1=0.021585613525486876\n",
      "SGD iter. 1/29: loss=0.5322888907495562, w0=-0.09390311949818725, w1=-0.05835660717189717\n",
      "SGD iter. 2/29: loss=0.9173632044801768, w0=0.14104548016478374, w1=0.20606733374782443\n",
      "SGD iter. 3/29: loss=6.364404656811003, w0=0.028848240164988312, w1=0.10656892508200182\n",
      "SGD iter. 4/29: loss=2.9329319524975537, w0=-0.19729447379728599, w1=-0.4804041312454069\n",
      "SGD iter. 5/29: loss=0.9648541891165424, w0=-0.16400475177799123, w1=-0.448272396530758\n",
      "SGD iter. 6/29: loss=0.8105870546628909, w0=-0.1557081961379591, w1=-0.4425722054790381\n",
      "SGD iter. 7/29: loss=0.7880788484293278, w0=-0.11573489548551175, w1=-0.39537113463079776\n",
      "SGD iter. 8/29: loss=1.3011728721121094, w0=-0.3195780656646445, w1=-0.4884617893033594\n",
      "SGD iter. 9/29: loss=7.714627076248714, w0=0.017338413046345535, w1=-0.20957124901640745\n",
      "SGD iter. 10/29: loss=0.6714787506722488, w0=-0.005770121673324993, w1=-0.2194679837873798\n",
      "SGD iter. 11/29: loss=0.8550065668860697, w0=-0.15397336444463944, w1=-0.424168457846055\n",
      "SGD iter. 12/29: loss=1.806262590000762, w0=0.028417405979108662, w1=-0.20222836930719001\n",
      "SGD iter. 13/29: loss=0.9120264995597128, w0=-0.11558591371239932, w1=-0.26936026715864664\n",
      "SGD iter. 14/29: loss=0.5056771379469615, w0=-0.08054083090384578, w1=-0.20256873435763184\n",
      "SGD iter. 15/29: loss=0.5842128477900457, w0=-0.0729746621574852, w1=-0.19509664188295084\n",
      "SGD iter. 16/29: loss=1.1243152545483919, w0=-0.19528938330654907, w1=-0.2682328975134659\n",
      "SGD iter. 17/29: loss=0.5820800486965965, w0=-0.28918436461630487, w1=-0.35340357052137517\n",
      "SGD iter. 18/29: loss=2.6287417578200825, w0=-0.04468526483857843, w1=-0.08112714616351546\n",
      "SGD iter. 19/29: loss=0.6294179243700962, w0=-0.15859504156000626, w1=-0.29283949174699997\n",
      "SGD iter. 20/29: loss=3.015181837357018, w0=-0.05255358389587601, w1=-0.23426974599249628\n",
      "SGD iter. 21/29: loss=0.5305728470576956, w0=-0.040014793301556786, w1=-0.22657907345063222\n",
      "SGD iter. 22/29: loss=0.6422636659719015, w0=0.05010375422910776, w1=-0.1492231934140611\n",
      "SGD iter. 23/29: loss=2.7420991286738374, w0=-0.1671503646365891, w1=-0.34502390185947\n",
      "SGD iter. 24/29: loss=9.246761682779287, w0=0.46093229723872026, w1=0.3032300755898439\n",
      "SGD iter. 25/29: loss=34.56638525618149, w0=0.052990017258602695, w1=-0.06985575844925379\n",
      "SGD iter. 26/29: loss=12.220991867485816, w0=-0.25901784807162204, w1=-0.48908182474658185\n",
      "SGD iter. 27/29: loss=2.8431624855286355, w0=0.08420153035197642, w1=-0.12690134387705304\n",
      "SGD iter. 28/29: loss=2.535205074231737, w0=-0.006080904187090189, w1=-0.21074308091585092\n",
      "SGD iter. 29/29: loss=1.3606638909529685, w0=0.09780343424205994, w1=-0.08151977024064253\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.0, w1=0.04911607420066014\n",
      "SGD iter. 1/29: loss=0.7141028757556128, w0=-0.14232616412290913, w1=-0.036580109846031675\n",
      "SGD iter. 2/29: loss=1.5159301407396255, w0=-0.12046700968248197, w1=-0.01739802317298858\n",
      "SGD iter. 3/29: loss=1.2847083266133419, w0=0.26916299523163145, w1=0.4447680012348808\n",
      "SGD iter. 4/29: loss=15.350636382708835, w0=-0.1697464317304559, w1=0.017846743042561508\n",
      "SGD iter. 5/29: loss=2.024792854700662, w0=-0.18081951021406145, w1=0.005345100562143292\n",
      "SGD iter. 6/29: loss=1.9266364840855705, w0=-0.3301635815413419, w1=-0.13800297408927753\n",
      "SGD iter. 7/29: loss=2.211021503057151, w0=0.1827669484212298, w1=0.5274314347057681\n",
      "SGD iter. 8/29: loss=21.674842745097664, w0=-0.6032762560900822, w1=-1.2883775578915886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 9/29: loss=12.16613370268561, w0=0.27214822294438246, w1=1.1314541432926277\n",
      "SGD iter. 10/29: loss=38.67414990170075, w0=-0.9819485799280782, w1=-0.14478989556750999\n",
      "SGD iter. 11/29: loss=78.06825917004052, w0=0.47311776917740056, w1=1.9399753832749143\n",
      "SGD iter. 12/29: loss=117.28431845519886, w0=-2.096407607396317, w1=-4.973370720960279\n",
      "SGD iter. 13/29: loss=579.9193515276628, w0=4.042616790759949, w1=15.470849956390904\n",
      "SGD iter. 14/29: loss=4135.173866778096, w0=-6.21665408083839, w1=4.654929807292149\n",
      "SGD iter. 15/29: loss=723.5824922653574, w0=-1.5323624942118208, w1=7.746613863722571\n",
      "SGD iter. 16/29: loss=152.66280512689232, w0=-1.1455297901622756, w1=7.65057600947578\n",
      "SGD iter. 17/29: loss=200.23926438379354, w0=-3.325495608439704, w1=5.988331823730668\n",
      "SGD iter. 18/29: loss=61.992103953819786, w0=-3.20572352486688, w1=6.179405321405953\n",
      "SGD iter. 19/29: loss=59.10256610432938, w0=-3.3047846453930423, w1=6.12393621636159\n",
      "SGD iter. 20/29: loss=54.930912874294044, w0=-3.1428616972916084, w1=6.266028630471258\n",
      "SGD iter. 21/29: loss=54.95113478164022, w0=-4.114841963488419, w1=5.185924924602603\n",
      "SGD iter. 22/29: loss=123.9949501595419, w0=-2.527210990719917, w1=6.210181578959837\n",
      "SGD iter. 23/29: loss=60.06735189554601, w0=-2.4459672555662784, w1=6.271738363904871\n",
      "SGD iter. 24/29: loss=61.01440814785799, w0=-3.5606075608065484, w1=4.427845687052739\n",
      "SGD iter. 25/29: loss=99.14411171941232, w0=-2.518488161151983, w1=5.616862966751565\n",
      "SGD iter. 26/29: loss=48.71519414282601, w0=-3.7576231197573957, w1=3.445791502371887\n",
      "SGD iter. 27/29: loss=134.0115743690545, w0=-2.3218080456520043, w1=4.050786371732072\n",
      "SGD iter. 28/29: loss=61.31137269170577, w0=-0.9791434213589345, w1=5.165012581422503\n",
      "SGD iter. 29/29: loss=97.748103786007, w0=-2.965168673504099, w1=3.2554721731474303\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.0, w1=0.069782659762772\n",
      "SGD iter. 1/29: loss=0.7385002061058226, w0=-0.24978133457503385, w1=-0.5285000257435448\n",
      "SGD iter. 2/29: loss=7.310699998884096, w0=0.3497814748778504, w1=0.016727597464745503\n",
      "SGD iter. 3/29: loss=24.30841141389483, w0=-0.957258571568199, w1=-1.1216110411768425\n",
      "SGD iter. 4/29: loss=201.04046716986159, w0=0.8930688162865817, w1=6.238304607258367\n",
      "SGD iter. 5/29: loss=77.59273744170216, w0=2.0537232303864887, w1=6.894200968034433\n",
      "SGD iter. 6/29: loss=562.0398968089422, w0=-1.1237446208579045, w1=4.298314531523259\n",
      "SGD iter. 7/29: loss=90.05183609865024, w0=-1.909462460392192, w1=3.7798277066002774\n",
      "SGD iter. 8/29: loss=22.68867478668828, w0=-1.4423354612789083, w1=4.119326448312007\n",
      "SGD iter. 9/29: loss=44.651254198381814, w0=-2.673148956960241, w1=2.4925483808399145\n",
      "SGD iter. 10/29: loss=136.951540954721, w0=-0.16964335533165764, w1=5.807563639356907\n",
      "SGD iter. 11/29: loss=629.7521848722955, w0=-3.3610149070403663, w1=1.2462963747705222\n",
      "SGD iter. 12/29: loss=97.52230936906024, w0=-1.8424882568380228, w1=2.0037342524431745\n",
      "SGD iter. 13/29: loss=25.33290525325885, w0=-1.161499452170149, w1=2.4912874864826593\n",
      "SGD iter. 14/29: loss=40.13480407860359, w0=-2.14674838222954, w1=0.8031635447227887\n",
      "SGD iter. 15/29: loss=24.264933368216653, w0=-1.2995633465113277, w1=1.6758250389074465\n",
      "SGD iter. 16/29: loss=28.12625584526634, w0=-2.223781068039388, w1=-1.6670890043794555\n",
      "SGD iter. 17/29: loss=50.11234885973663, w0=-0.5587259055470317, w1=-0.16459554885331928\n",
      "SGD iter. 18/29: loss=74.95303531634818, w0=-3.5128030856577395, w1=-3.245596220676023\n",
      "SGD iter. 19/29: loss=3052.731837957836, w0=0.15031221951783236, w1=-0.6331693127699851\n",
      "SGD iter. 20/29: loss=1663.8578322023297, w0=0.48796405468271126, w1=-0.47970366799803177\n",
      "SGD iter. 21/29: loss=1423.1863276959111, w0=1.799266574621554, w1=0.560702139151553\n",
      "SGD iter. 22/29: loss=970.6924300768677, w0=6.416691371140437, w1=4.612635753256745\n",
      "SGD iter. 23/29: loss=2963.432831535681, w0=-5.033098365142483, w1=-12.209383309511713\n",
      "SGD iter. 24/29: loss=6192.3076087220725, w0=5.431282921154891, w1=-2.032343474546158\n",
      "SGD iter. 25/29: loss=304.2247984897603, w0=6.522172364817279, w1=3.246261630081957\n",
      "SGD iter. 26/29: loss=1462.2166150951457, w0=0.542885328909879, w1=1.8390410736975105\n",
      "SGD iter. 27/29: loss=184.8329930214896, w0=1.6631505239238213, w1=2.8221085627037796\n",
      "SGD iter. 28/29: loss=112.1387178065331, w0=0.6712372481396044, w1=2.59873561990146\n",
      "SGD iter. 29/29: loss=95.93120550122107, w0=3.9691125585915774, w1=5.52000253826284\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.15500000000000003, w1=0.14996697987105156\n",
      "SGD iter. 1/29: loss=3.9506105673616445, w0=0.1142524892299295, w1=0.12060166635652064\n",
      "SGD iter. 2/29: loss=2.9681112400346326, w0=-0.14770662200787807, w1=-0.13942057940187874\n",
      "SGD iter. 3/29: loss=1.4128712217276005, w0=-0.1143719114636739, w1=-0.10871804072880328\n",
      "SGD iter. 4/29: loss=0.8588205726604082, w0=-0.0732243151751149, w1=-0.07048951742664623\n",
      "SGD iter. 5/29: loss=0.46759017196589786, w0=0.02138617354658673, w1=0.03532335122073617\n",
      "SGD iter. 6/29: loss=0.6507986993585587, w0=-0.06240406840309107, w1=-0.02671349476187812\n",
      "SGD iter. 7/29: loss=0.47416164082115214, w0=-0.135189201208499, w1=-0.10105087035700792\n",
      "SGD iter. 8/29: loss=1.150401860042486, w0=-0.07017503344912983, w1=0.005126763380320101\n",
      "SGD iter. 9/29: loss=0.5201408623488061, w0=-0.10675374365041901, w1=-0.01339052203013745\n",
      "SGD iter. 10/29: loss=0.4949064501790764, w0=-0.015806686793494965, w1=0.08162517185030767\n",
      "SGD iter. 11/29: loss=8.518549147400398, w0=-0.5438034575500569, w1=-0.5926994756478636\n",
      "SGD iter. 12/29: loss=17.787276565897983, w0=0.11820579613780346, w1=-0.16474917558350977\n",
      "SGD iter. 13/29: loss=0.5672724020647774, w0=-0.06507750538768087, w1=-0.19357186598628673\n",
      "SGD iter. 14/29: loss=4.5696160552317915, w0=0.07870899576920476, w1=-0.12219574093554754\n",
      "SGD iter. 15/29: loss=1.2443419566768938, w0=-0.01798054910071531, w1=-0.18630040419864397\n",
      "SGD iter. 16/29: loss=3.058327241941323, w0=0.12794402717801315, w1=0.04221251848940619\n",
      "SGD iter. 17/29: loss=1.0301849976799624, w0=0.11646455289311815, w1=0.051908339163996176\n",
      "SGD iter. 18/29: loss=0.7618704910608306, w0=-0.06983774390515642, w1=-0.0820811277209639\n",
      "SGD iter. 19/29: loss=2.6751224524068196, w0=0.02361089704950775, w1=0.05237339130480889\n",
      "SGD iter. 20/29: loss=1.3117233461612474, w0=-0.0419942041193062, w1=0.01899267428992891\n",
      "SGD iter. 21/29: loss=1.906030132553893, w0=-0.06997874722200645, w1=0.003229510678387334\n",
      "SGD iter. 22/29: loss=2.02451566306431, w0=0.0859998525598436, w1=0.2909908488347184\n",
      "SGD iter. 23/29: loss=4.59908206646483, w0=-0.3493300944285065, w1=-0.07283384531679299\n",
      "SGD iter. 24/29: loss=7.321705704649967, w0=0.06667151959863887, w1=0.1331291121395681\n",
      "SGD iter. 25/29: loss=4.055948650774785, w0=-0.5411501615973124, w1=-0.9196055583760718\n",
      "SGD iter. 26/29: loss=77.61767555752779, w0=1.3734029116655808, w1=2.1495867133817588\n",
      "SGD iter. 27/29: loss=208.5330070065997, w0=-1.9075381965211724, w1=-0.3127023709609493\n",
      "SGD iter. 28/29: loss=1425.007124435097, w0=1.8338499766415248, w1=4.141012039361382\n",
      "SGD iter. 29/29: loss=303.78268728370495, w0=0.34227963780608905, w1=3.161687068979147\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.0, w1=-0.06622230967838923\n",
      "SGD iter. 1/29: loss=0.6024048706338817, w0=-0.013093114421205867, w1=-0.0235851864516761\n",
      "SGD iter. 2/29: loss=0.6926622841231866, w0=-0.19901945180250824, w1=-0.1402039726344317\n",
      "SGD iter. 3/29: loss=2.1299996483699704, w0=-0.10636908596640794, w1=-0.06452011926976656\n",
      "SGD iter. 4/29: loss=1.0171194916574071, w0=-0.09724919515994408, w1=-0.0527056589797694\n",
      "SGD iter. 5/29: loss=0.9143102977303098, w0=0.041615918492015444, w1=0.12261365938779173\n",
      "SGD iter. 6/29: loss=1.2765987341895464, w0=-0.2144247474585057, w1=-0.07602443067977399\n",
      "SGD iter. 7/29: loss=2.973484073471975, w0=0.22153734148579896, w1=0.14073638787661674\n",
      "SGD iter. 8/29: loss=3.1174627238831456, w0=-0.18549006054025113, w1=-0.470109308138045\n",
      "SGD iter. 9/29: loss=4.326399902408983, w0=0.39175022320837, w1=0.03643696217695919\n",
      "SGD iter. 10/29: loss=171.88589251288658, w0=-1.000118662194506, w1=-1.1613806699252494\n",
      "SGD iter. 11/29: loss=25.25943057359964, w0=-1.0268872901377113, w1=-1.060713535026827\n",
      "SGD iter. 12/29: loss=21.418375054219595, w0=-0.8070185791873958, w1=-0.9249317819643226\n",
      "SGD iter. 13/29: loss=24.193537871195563, w0=-0.5912927925683819, w1=-0.8142809218025369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 14/29: loss=26.765924785549434, w0=0.09753230614932562, w1=-0.3797669917152183\n",
      "SGD iter. 15/29: loss=51.950391566480796, w0=-0.12297638697253815, w1=-0.34738316708380934\n",
      "SGD iter. 16/29: loss=39.33252153907322, w0=-0.800140197446243, w1=-0.9339218813559581\n",
      "SGD iter. 17/29: loss=60.89271075381015, w0=0.07512519841828691, w1=-0.47033448691058344\n",
      "SGD iter. 18/29: loss=14.149361855484768, w0=-0.0007873567787036045, w1=-0.5187936934752855\n",
      "SGD iter. 19/29: loss=16.19504639933141, w0=1.0759897025063159, w1=0.6356454896790003\n",
      "SGD iter. 20/29: loss=130.2763257008931, w0=-1.1260388806278938, w1=-1.7852418333075568\n",
      "SGD iter. 21/29: loss=195.1284651602517, w0=2.995971403933832, w1=6.384543303260148\n",
      "SGD iter. 22/29: loss=4805.980145801085, w0=-2.885402448409878, w1=1.7013277359214296\n",
      "SGD iter. 23/29: loss=1520.5753410315021, w0=-10.252313428978393, w1=-9.006557364110268\n",
      "SGD iter. 24/29: loss=22886.559702087186, w0=20.71978873149148, w1=20.77491993224961\n",
      "SGD iter. 25/29: loss=63103.10223980636, w0=-19.769487173351497, w1=-18.828055991607414\n",
      "SGD iter. 26/29: loss=10449.318380822999, w0=2.4986007145975364, w1=-3.137840486231383\n",
      "SGD iter. 27/29: loss=26558.222182754227, w0=-0.6401594304449958, w1=-10.562266133474857\n",
      "SGD iter. 28/29: loss=20999.207331409525, w0=-23.814449419908165, w1=-22.621315486118377\n",
      "SGD iter. 29/29: loss=9123.181973413315, w0=-1.1966908432749506, w1=21.512529627952553\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=-0.20500000000000002, w1=-0.06944215778769475\n",
      "SGD iter. 1/29: loss=1.4874914369058854, w0=-0.1355083724267182, w1=0.024705407743215335\n",
      "SGD iter. 2/29: loss=0.7732008078331223, w0=0.14119258923429406, w1=0.35652428807983766\n",
      "SGD iter. 3/29: loss=35.79547269958815, w0=-0.7895536061676582, w1=-1.2523214591403449\n",
      "SGD iter. 4/29: loss=24.686247095258093, w0=0.430080309876989, w1=0.6466603465993406\n",
      "SGD iter. 5/29: loss=11.341854259904732, w0=-0.41945673864687716, w1=0.5123567292864404\n",
      "SGD iter. 6/29: loss=3.5035586187496115, w0=0.1330129696300556, w1=1.0189302130879085\n",
      "SGD iter. 7/29: loss=36.47789960319155, w0=-1.1802935149065608, w1=-0.09960935787506653\n",
      "SGD iter. 8/29: loss=42.39536121859491, w0=1.0179995072567425, w1=2.5681545223285225\n",
      "SGD iter. 9/29: loss=647.6954471226531, w0=-3.1130803845977826, w1=-6.92041388773118\n",
      "SGD iter. 10/29: loss=310.52661137861475, w0=1.9573144313022044, w1=-2.9080439383852683\n",
      "SGD iter. 11/29: loss=1426.4186501606205, w0=-1.8910129813131022, w1=-7.145174512522623\n",
      "SGD iter. 12/29: loss=335.41869126353924, w0=-4.749942178411974, w1=-8.245797777209868\n",
      "SGD iter. 13/29: loss=3457.117437069488, w0=23.364102870137888, w1=47.38785074268488\n",
      "SGD iter. 14/29: loss=238407.7824586819, w0=-125.9310707813762, w1=-139.86497531452505\n",
      "SGD iter. 15/29: loss=1837570.9203722265, w0=112.15036002234513, w1=-4.086886679978676\n",
      "SGD iter. 16/29: loss=257745.27248576315, w0=-27.059369687058847, w1=-102.36016442104075\n",
      "SGD iter. 17/29: loss=800870.083440027, w0=111.53997042621643, w1=25.15282819206118\n",
      "SGD iter. 18/29: loss=262146.3303846154, w0=-18.22576159504888, w1=-75.5242065156976\n",
      "SGD iter. 19/29: loss=1358458.6930236348, w0=101.63020720019202, w1=25.670516942198134\n",
      "SGD iter. 20/29: loss=342368.3172689053, w0=8.733542272575079, w1=-43.67255051125055\n",
      "SGD iter. 21/29: loss=722183.3922381864, w0=86.01410075873352, w1=38.07676363580104\n",
      "SGD iter. 22/29: loss=237481.8945534574, w0=-80.05441161799729, w1=-57.393852585419495\n",
      "SGD iter. 23/29: loss=875218.9696797885, w0=107.4058728680618, w1=69.839594381041\n",
      "SGD iter. 24/29: loss=752166.4570599685, w0=-107.12618539415726, w1=-140.2563580745975\n",
      "SGD iter. 25/29: loss=1389970.0749447155, w0=208.656287961691, w1=568.7680423465163\n",
      "SGD iter. 26/29: loss=3948392.743273407, w0=-390.80214478549806, w1=-393.9991310142228\n",
      "SGD iter. 27/29: loss=25790657.892701626, w0=302.2551879311052, w1=90.24818245016849\n",
      "SGD iter. 28/29: loss=6228874.905629212, w0=964.0949379860946, w1=701.218210321608\n",
      "SGD iter. 29/29: loss=53788582.441235155, w0=-774.4026125767804, w1=-890.3344934183929\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=-0.20500000000000002, w1=-0.20797740922815064\n",
      "SGD iter. 1/29: loss=1.4882795918971161, w0=0.12781833431971606, w1=0.10490562629881309\n",
      "SGD iter. 2/29: loss=7.106387483066934, w0=-0.4274901290479251, w1=-0.34525547674266177\n",
      "SGD iter. 3/29: loss=17.387186909680796, w0=0.8028990933281557, w1=1.032556180571833\n",
      "SGD iter. 4/29: loss=111.72880781608848, w0=-1.0230815608656696, w1=-0.6494725568699051\n",
      "SGD iter. 5/29: loss=8.351605291268694, w0=-0.16967760335202386, w1=0.2767381243108682\n",
      "SGD iter. 6/29: loss=30.48190540507079, w0=-0.5966333231778582, w1=0.013227518075984768\n",
      "SGD iter. 7/29: loss=16.11767814714656, w0=-0.9090492107924472, w1=-0.28535041709718606\n",
      "SGD iter. 8/29: loss=5.626340263161966, w0=-0.041463313191992035, w1=0.47598314360556776\n",
      "SGD iter. 9/29: loss=1.9751141084213883, w0=-0.44180618676705125, w1=-0.024769996291792495\n",
      "SGD iter. 10/29: loss=6.9487417003140255, w0=0.3740975620677048, w1=0.8127743804384449\n",
      "SGD iter. 11/29: loss=9.505221158608602, w0=-0.1156768806031564, w1=0.4118889060118126\n",
      "SGD iter. 12/29: loss=13.496735323852384, w0=-0.1548305422271401, w1=0.34923356364607905\n",
      "SGD iter. 13/29: loss=13.201617447147171, w0=-0.3540174914130985, w1=-0.35363312447118556\n",
      "SGD iter. 14/29: loss=23.89801856959543, w0=0.281003016417621, w1=0.9011078896085186\n",
      "SGD iter. 15/29: loss=7.043031191516111, w0=0.16802443078233475, w1=0.6629609596622559\n",
      "SGD iter. 16/29: loss=4.066065405026925, w0=-0.37934548954911285, w1=0.4214529577087419\n",
      "SGD iter. 17/29: loss=6.8085678260094555, w0=0.7354610709064733, w1=1.3615231576070688\n",
      "SGD iter. 18/29: loss=461.66552567423065, w0=-6.221507092992583, w1=-5.021879521011321\n",
      "SGD iter. 19/29: loss=6067.2565246877375, w0=25.13838634752382, w1=22.497400589881487\n",
      "SGD iter. 20/29: loss=451013.3012158539, w0=-296.2540727402807, w1=-220.2411595311981\n",
      "SGD iter. 21/29: loss=28865981.210635003, w0=793.6166588092794, w1=1175.1713825472161\n",
      "SGD iter. 22/29: loss=81876238.290204, w0=-722.664468244926, w1=-998.1493839655013\n",
      "SGD iter. 23/29: loss=10143538.098988676, w0=-67.83413919861698, w1=-423.4422442566686\n",
      "SGD iter. 24/29: loss=21229377.85255271, w0=-68.70991489818194, w1=752.8274412781623\n",
      "SGD iter. 25/29: loss=7947934.891845487, w0=-998.0789563904815, w1=-203.00370488133933\n",
      "SGD iter. 26/29: loss=108848686.18378347, w0=1150.194603518954, w1=1853.1536574673064\n",
      "SGD iter. 27/29: loss=82486437.4776843, w0=-873.3939803499779, w1=86.22899419013152\n",
      "SGD iter. 28/29: loss=117695580.29317029, w0=474.19216820710994, w1=1075.347824103621\n",
      "SGD iter. 29/29: loss=38996811.37880174, w0=1341.2960499706996, w1=1703.3133042890106\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=-0.20500000000000002, w1=-0.031154546928451335\n",
      "SGD iter. 1/29: loss=3.2174476120516577, w0=0.4391395665294949, w1=0.4939813584957259\n",
      "SGD iter. 2/29: loss=18.988178484398645, w0=-0.8778801607151774, w1=-1.73184326142779\n",
      "SGD iter. 3/29: loss=132.84608451596384, w0=2.3219205656478277, w1=0.7616673601959851\n",
      "SGD iter. 4/29: loss=432.1446389965102, w0=-3.321848730220652, w1=-4.519883430389293\n",
      "SGD iter. 5/29: loss=3348.642065201155, w0=5.938683128342755, w1=3.6065207831658617\n",
      "SGD iter. 6/29: loss=313.42307396606947, w0=0.39915789226558385, w1=-2.2702072293829794\n",
      "SGD iter. 7/29: loss=883.359912273331, w0=3.7275405486456084, w1=2.1248334647775895\n",
      "SGD iter. 8/29: loss=229.04598022792865, w0=3.414008777555269, w1=-1.2461172302992733\n",
      "SGD iter. 9/29: loss=78.47708541383857, w0=0.6957078261850262, w1=-2.519110108196616\n",
      "SGD iter. 10/29: loss=201.60928651581358, w0=1.626303134717498, w1=-1.5118028880582846\n",
      "SGD iter. 11/29: loss=82.03440188322546, w0=3.8389952903398137, w1=0.9422451443069022\n",
      "SGD iter. 12/29: loss=470.513688380272, w0=-3.4115778915026147, w1=-5.399566853279817\n",
      "SGD iter. 13/29: loss=5079.825162276644, w0=7.3134342098661325, w1=2.6327700311053874\n",
      "SGD iter. 14/29: loss=722.4102128035448, w0=0.0111507553277157, w1=-4.159862233061906\n",
      "SGD iter. 15/29: loss=1939.4151121487134, w0=14.550808648824214, w1=11.731616718074859\n",
      "SGD iter. 16/29: loss=23468.41830806508, w0=-7.107225841948244, w1=-4.985749491384576\n",
      "SGD iter. 17/29: loss=4224.800568664445, w0=-10.308984991924069, w1=-8.865072587495014\n",
      "SGD iter. 18/29: loss=2599.109313347167, w0=2.2484957370733305, w1=-0.4275476617927705\n",
      "SGD iter. 19/29: loss=1896.334985669218, w0=-1.4457113204249739, w1=-3.358074900273509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 20/29: loss=902.7872953434367, w0=-5.229299869696847, w1=-7.599669327927549\n",
      "SGD iter. 21/29: loss=701.0613122082166, w0=1.4993985223232613, w1=-2.443607226944838\n",
      "SGD iter. 22/29: loss=1733.5140539606016, w0=-6.528525730897297, w1=-9.104282694166505\n",
      "SGD iter. 23/29: loss=2534.2858729266472, w0=3.615138530372139, w1=-0.20290237901293828\n",
      "SGD iter. 24/29: loss=87.23604634095544, w0=2.462812683852085, w1=-1.1665477853297954\n",
      "SGD iter. 25/29: loss=51.44819312011581, w0=4.233716559548514, w1=0.26305925556288945\n",
      "SGD iter. 26/29: loss=152.63679175929207, w0=0.49296325658225726, w1=-2.6961795403631035\n",
      "SGD iter. 27/29: loss=730.1719411157588, w0=5.940856093938447, w1=2.1858954004367694\n",
      "SGD iter. 28/29: loss=636.0235371567348, w0=0.9177117119740812, w1=-3.204201815604634\n",
      "SGD iter. 29/29: loss=566.916633082708, w0=6.491139762765038, w1=2.5911409926066407\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.20500000000000002, w1=0.18484837057393716\n",
      "SGD iter. 1/29: loss=3.192100123823885, w0=-0.43921310323020407, w1=-0.8709656988570122\n",
      "SGD iter. 2/29: loss=25.31437575025275, w0=0.07351021049398204, w1=-0.6265431453947479\n",
      "SGD iter. 3/29: loss=6.642806697980683, w0=0.06979441693116441, w1=-0.635650774473821\n",
      "SGD iter. 4/29: loss=6.464052069007746, w0=0.2074592526408403, w1=-0.5217147153907613\n",
      "SGD iter. 5/29: loss=3.1171991465586197, w0=0.36695293070046964, w1=-0.3066623772457013\n",
      "SGD iter. 6/29: loss=4.211545885951773, w0=0.05509415816919361, w1=-0.5326970979400096\n",
      "SGD iter. 7/29: loss=0.6311703669604211, w0=0.2529376226273219, w1=-0.37344457129385344\n",
      "SGD iter. 8/29: loss=1.4761174700573214, w0=-0.10841774439173318, w1=-0.7403412157435163\n",
      "SGD iter. 9/29: loss=27.042838451062867, w0=2.095637202955811, w1=1.9977003167739453\n",
      "SGD iter. 10/29: loss=1243.4015017142967, w0=-18.223013291819115, w1=-15.46771026998429\n",
      "SGD iter. 11/29: loss=387023.25310212036, w0=413.3768587325759, w1=410.87295031240524\n",
      "SGD iter. 12/29: loss=216530552.31981432, w0=-3568.083831735664, w1=-8358.153842392063\n",
      "SGD iter. 13/29: loss=3051719282.419686, w0=17844.94050973081, w1=3632.160040156705\n",
      "SGD iter. 14/29: loss=195898331449.78296, w0=-101454.91405418218, w1=-85291.97209225266\n",
      "SGD iter. 15/29: loss=1087490105952.6326, w0=55224.05535469574, w1=24355.783254684196\n",
      "SGD iter. 16/29: loss=113231645439.13289, w0=91279.84724188766, w1=48617.196644225085\n",
      "SGD iter. 17/29: loss=761020907219.712, w0=-101153.86725307682, w1=-60779.61460582816\n",
      "SGD iter. 18/29: loss=549907050358.8627, w0=52955.773022036345, w1=74294.67745626095\n",
      "SGD iter. 19/29: loss=233372920079.74103, w0=-71532.01809020233, w1=-56240.568098668984\n",
      "SGD iter. 20/29: loss=388040862251.9916, w0=49859.69148026014, w1=35074.50333398626\n",
      "SGD iter. 21/29: loss=97195726244.40964, w0=-51750.20659138443, w1=-300486.13824240415\n",
      "SGD iter. 22/29: loss=1655060908690.2017, w0=507376.3211387635, w1=148762.22707964736\n",
      "SGD iter. 23/29: loss=99843743819649.42, w0=-2825055.7438243716, w1=-5054202.325885939\n",
      "SGD iter. 24/29: loss=1585676449097912.0, w0=9340675.204457438, w1=5886968.575173893\n",
      "SGD iter. 25/29: loss=1.5701490241771542e+16, w0=-17814124.140192695, w1=-13265005.998754065\n",
      "SGD iter. 26/29: loss=9880899885780114.0, w0=4648428.618574843, w1=960587.5055258814\n",
      "SGD iter. 27/29: loss=436135633410652.5, w0=-1556391.5670071784, w1=-9468191.827253442\n",
      "SGD iter. 28/29: loss=3787566470563492.5, w0=4733988.014717364, w1=-4196131.767568468\n",
      "SGD iter. 29/29: loss=584292065870605.0, w0=7198141.420540393, w1=-1804372.2206855747\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.0, w1=0.00676244110158446\n",
      "SGD iter. 1/29: loss=0.7369475662959662, w0=0.2154255403783938, w1=0.25369653943941295\n",
      "SGD iter. 2/29: loss=4.44930920645616, w0=-0.4133632701943173, w1=-0.0031975078917910182\n",
      "SGD iter. 3/29: loss=2.1371038201692674, w0=0.06810472607229606, w1=0.6211916941127364\n",
      "SGD iter. 4/29: loss=19.30644959991427, w0=-2.3431042124527015, w1=-6.881008864189417\n",
      "SGD iter. 5/29: loss=2126.6759020766194, w0=11.328382543280147, w1=6.219840845628004\n",
      "SGD iter. 6/29: loss=15777.629515602066, w0=-11.67408321714583, w1=-13.96553700659851\n",
      "SGD iter. 7/29: loss=1579.0824228863573, w0=-2.274320154388473, w1=-3.705779829885529\n",
      "SGD iter. 8/29: loss=4239.391622300549, w0=-3.783954331027803, w1=-1.8650853286360916\n",
      "SGD iter. 9/29: loss=2786.3631294286693, w0=-20.80114964085181, w1=-26.02993000834006\n",
      "SGD iter. 10/29: loss=44843.73089249571, w0=106.77600702396626, w1=77.65897800568987\n",
      "SGD iter. 11/29: loss=5708737.384405953, w0=-150.5273008652988, w1=-136.88618682999743\n",
      "SGD iter. 12/29: loss=1595248.2460192826, w0=28.43364948834528, w1=-106.33036983173376\n",
      "SGD iter. 13/29: loss=3312122.3684070325, w0=-1710.3856640634906, w1=-879.8942086058399\n",
      "SGD iter. 14/29: loss=4873939075.743859, w0=54569.276114755005, w1=46731.96530936039\n",
      "SGD iter. 15/29: loss=3236305447187.734, w0=-776095.5258767165, w1=-551582.6876212517\n",
      "SGD iter. 16/29: loss=108784583846875.83, w0=2207386.9791550273, w1=4264868.841425952\n",
      "SGD iter. 17/29: loss=467865165961704.6, w0=-3180426.4392530853, w1=-986235.7300365437\n",
      "SGD iter. 18/29: loss=388788143999982.06, w0=3696025.4967639847, w1=2872341.5872484557\n",
      "SGD iter. 19/29: loss=5139013976110726.0, w0=-13485679.687270723, w1=-23590411.749590266\n",
      "SGD iter. 20/29: loss=1.0671994283815416e+16, w0=15990389.188767161, w1=6734860.329270951\n",
      "SGD iter. 21/29: loss=2226734626395719.5, w0=-4121959.454635214, w1=-14346974.538896725\n",
      "SGD iter. 22/29: loss=8729220219528876.0, w0=21102805.254421856, w1=10264661.837609813\n",
      "SGD iter. 23/29: loss=6814769454987215.0, w0=-10384881.703983784, w1=-31917143.58265777\n",
      "SGD iter. 24/29: loss=5.3345908272457496e+16, w0=70363047.45102566, w1=37264425.2175124\n",
      "SGD iter. 25/29: loss=4.379326911467463e+17, w0=-112888907.65198782, w1=-144693130.37259114\n",
      "SGD iter. 26/29: loss=6.819050324319872e+17, w0=236559894.46829703, w1=370393962.2535572\n",
      "SGD iter. 27/29: loss=1.287585640496959e+19, w0=-1795572034.9107606, w1=-1807266572.8546457\n",
      "SGD iter. 28/29: loss=1.4497497411072672e+21, w0=6451187045.885986, w1=4770363802.6051235\n",
      "SGD iter. 29/29: loss=1.3837524549620552e+21, w0=-6538121619.788448, w1=-6479273096.639198\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.0, w1=0.0107846985544424\n",
      "SGD iter. 1/29: loss=0.5480036069737543, w0=-0.3242064581952616, w1=-0.17232569025155361\n",
      "SGD iter. 2/29: loss=40.4803536385592, w0=1.5926577414732308, w1=1.1490985711926178\n",
      "SGD iter. 3/29: loss=161.91930567131857, w0=-2.1144948662165146, w1=-3.210202915847029\n",
      "SGD iter. 4/29: loss=754.8902188393906, w0=2.844933305931409, w1=0.258595455133908\n",
      "SGD iter. 5/29: loss=104.1968828205906, w0=1.2727012010953054, w1=-2.990129643119891\n",
      "SGD iter. 6/29: loss=111.66484155957824, w0=2.7580642224162766, w1=-2.4087634644522953\n",
      "SGD iter. 7/29: loss=108.75298900432865, w0=-1.237215522181636, w1=-5.355465718716718\n",
      "SGD iter. 8/29: loss=190.89246902484558, w0=2.288259552351446, w1=-3.0269938517647677\n",
      "SGD iter. 9/29: loss=158.04349639498184, w0=-1.2025377444785699, w1=-6.099763544678138\n",
      "SGD iter. 10/29: loss=125.82837250641686, w0=2.9334698605013587, w1=-1.725895130969013\n",
      "SGD iter. 11/29: loss=5706.0584211814175, w0=-6.2046039008421365, w1=-9.908163120520186\n",
      "SGD iter. 12/29: loss=1080.3846818683637, w0=-1.952429801453273, w1=-5.585694378987355\n",
      "SGD iter. 13/29: loss=1138.683492993973, w0=-0.26463582618314496, w1=-3.7480086751028887\n",
      "SGD iter. 14/29: loss=1442.2020110924507, w0=-16.123628064284972, w1=-14.52441555909262\n",
      "SGD iter. 15/29: loss=89048.47222471444, w0=22.152468410131533, w1=26.55262133719913\n",
      "SGD iter. 16/29: loss=16448.04540375547, w0=0.9200556321269175, w1=5.417076052638961\n",
      "SGD iter. 17/29: loss=32904.02908777752, w0=7.0715359985845705, w1=4.704047613490891\n",
      "SGD iter. 18/29: loss=21267.14362501795, w0=21.006428939131062, w1=18.109663659465404\n",
      "SGD iter. 19/29: loss=7555.321983090975, w0=53.29563147664117, w1=33.81587300715603\n",
      "SGD iter. 20/29: loss=556562.2232525548, w0=-163.09710137594803, w1=8.576027306945733\n",
      "SGD iter. 21/29: loss=1659844.1196412293, w0=431.9582508993892, w1=562.5658472337933\n",
      "SGD iter. 22/29: loss=63085020.09782603, w0=-772.79488574541, w1=-971.8033535598494\n",
      "SGD iter. 23/29: loss=8416238.717744607, w0=-219.5168825255132, w1=-465.3368739533478\n",
      "SGD iter. 24/29: loss=4843650.69466635, w0=-742.8080519524028, w1=-927.0588450030533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 25/29: loss=56940343.27515855, w0=32.70270009438809, w1=-246.5240831402483\n",
      "SGD iter. 26/29: loss=29341310.13196501, w0=269.9016438913178, w1=53.384734850291125\n",
      "SGD iter. 27/29: loss=5226083.193437846, w0=-56.68744220646289, w1=-309.28580779102134\n",
      "SGD iter. 28/29: loss=13894902.08647247, w0=739.6709900510548, w1=403.3924467992413\n",
      "SGD iter. 29/29: loss=12934405.607509991, w0=-527.9596365306211, w1=-795.3265761369973\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.255, w1=0.12459071524538053\n",
      "SGD iter. 1/29: loss=3.1812070176921954, w0=-0.4376954414935572, w1=-0.4886542811420022\n",
      "SGD iter. 2/29: loss=11.925685456675119, w0=0.8489657853807624, w1=3.570193499102666\n",
      "SGD iter. 3/29: loss=165.89920419319327, w0=-4.89272336105272, w1=-1.966730600198535\n",
      "SGD iter. 4/29: loss=6696.460477430954, w0=39.42240316599157, w1=51.26037523890737\n",
      "SGD iter. 5/29: loss=893239.8103079164, w0=-387.7569406171279, w1=-457.9856909516727\n",
      "SGD iter. 6/29: loss=62722848.327526614, w0=2787.296538294531, w1=2765.2848103782035\n",
      "SGD iter. 7/29: loss=1815616557.6560125, w0=-5161.928563857438, w1=-7628.690167356832\n",
      "SGD iter. 8/29: loss=580211171.6574719, w0=-5002.975797665253, w1=-6952.046035298386\n",
      "SGD iter. 9/29: loss=2250857637.35366, w0=7073.334814433218, w1=7163.394420258099\n",
      "SGD iter. 10/29: loss=1338227654.5691993, w0=-5058.390972671446, w1=-2257.528139279728\n",
      "SGD iter. 11/29: loss=881912778.9312085, w0=8714.189200724742, w1=47209.480933704945\n",
      "SGD iter. 12/29: loss=14662740217.590754, w0=-26924.97368662033, w1=7506.649707700104\n",
      "SGD iter. 13/29: loss=47401816655.706215, w0=27165.324856173465, w1=46646.16560527836\n",
      "SGD iter. 14/29: loss=36118603340.54435, w0=-5136.794545537836, w1=52945.575006288935\n",
      "SGD iter. 15/29: loss=2010511998.5720549, w0=-14756.190380431397, w1=44418.23467651883\n",
      "SGD iter. 16/29: loss=2305449409.026535, w0=-12876.675379437429, w1=42210.40933945603\n",
      "SGD iter. 17/29: loss=1386336597.7256396, w0=3035.5795018815625, w1=59578.57234392155\n",
      "SGD iter. 18/29: loss=41522369975.11118, w0=-49837.79071044167, w1=-1350.0332474852767\n",
      "SGD iter. 19/29: loss=84092695037.87346, w0=65270.94249744989, w1=72822.57316604222\n",
      "SGD iter. 20/29: loss=3296645987840.7153, w0=-1758784.166141932, w1=-6538063.798049484\n",
      "SGD iter. 21/29: loss=6043281891557461.0, w0=15549445.548939772, w1=56109347.54096307\n",
      "SGD iter. 22/29: loss=1.890500942337116e+16, w0=-32063264.92655903, w1=4105321.0961491466\n",
      "SGD iter. 23/29: loss=3.1852553683989876e+16, w0=25231355.385187984, w1=135084757.29438493\n",
      "SGD iter. 24/29: loss=1.6175640261633498e+17, w0=-117050443.35863677, w1=7673111.396466881\n",
      "SGD iter. 25/29: loss=1.6632816474911977e+18, w0=316970110.3816744, w1=357740244.1263287\n",
      "SGD iter. 26/29: loss=2.291885501290546e+19, w0=-1293645560.377256, w1=-1230507180.790522\n",
      "SGD iter. 27/29: loss=2.5142425938508418e+20, w0=2056001963.1533432, w1=1134955671.089755\n",
      "SGD iter. 28/29: loss=1.3221950703862827e+20, w0=-925753733.4560447, w1=-336333382.7682445\n",
      "SGD iter. 29/29: loss=5.729164883288201e+19, w0=2309012791.6901913, w1=5365486152.635564\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=-0.255, w1=-0.24807501547853708\n",
      "SGD iter. 1/29: loss=12.481952806333668, w0=0.0855328585142004, w1=0.07920131044265602\n",
      "SGD iter. 2/29: loss=3.9122406597525816, w0=0.39875012248887104, w1=0.4882092575010034\n",
      "SGD iter. 3/29: loss=10.820383796880789, w0=-0.7230267230280103, w1=-0.8029577948039973\n",
      "SGD iter. 4/29: loss=22.742335873796158, w0=0.253071675218698, w1=-0.02789602463384766\n",
      "SGD iter. 5/29: loss=2.1091875849370174, w0=-0.2435760475150205, w1=-0.2585390975875589\n",
      "SGD iter. 6/29: loss=10.869592296650517, w0=1.6251207214274257, w1=1.7390956612291246\n",
      "SGD iter. 7/29: loss=2046.231699346921, w0=-11.807381550251765, w1=-11.76939622415799\n",
      "SGD iter. 8/29: loss=27725.327875198975, w0=30.482977715735455, w1=29.014812388299003\n",
      "SGD iter. 9/29: loss=50177.172431376595, w0=-67.72843478540472, w1=-83.26015467366055\n",
      "SGD iter. 10/29: loss=2327230.453365013, w0=449.6924033127732, w1=110.65003625633476\n",
      "SGD iter. 11/29: loss=28025746.230626732, w0=-1695.9214139314465, w1=-3450.435120598024\n",
      "SGD iter. 12/29: loss=600366028.2421082, w0=5821.412724729335, w1=1926.2204588853647\n",
      "SGD iter. 13/29: loss=2552295116.738161, w0=-5231.927093773432, w1=-6002.787805740222\n",
      "SGD iter. 14/29: loss=760018980.1874281, w0=3633.0779961623402, w1=2133.3165742463416\n",
      "SGD iter. 15/29: loss=1705421229.236783, w0=-7225.268050075471, w1=-9321.148192563309\n",
      "SGD iter. 16/29: loss=2495052013.663096, w0=10596.284389934717, w1=6752.036118774349\n",
      "SGD iter. 17/29: loss=17571088596.60508, w0=-10402.325379191909, w1=3538.3636403858486\n",
      "SGD iter. 18/29: loss=2083565642.1994638, w0=-3684.9865482965743, w1=7245.538257735577\n",
      "SGD iter. 19/29: loss=2967185151.267224, w0=-36535.46970257033, w1=-17204.44275869708\n",
      "SGD iter. 20/29: loss=405604170653.64545, w0=135212.31421003366, w1=60743.79183646466\n",
      "SGD iter. 21/29: loss=1346092370177.7917, w0=-143784.09880538235, w1=-175579.33293786415\n",
      "SGD iter. 22/29: loss=562753091497.5388, w0=128687.99441206318, w1=180940.30178986644\n",
      "SGD iter. 23/29: loss=3257704469369.9688, w0=-382453.94511024316, w1=-410041.82365235523\n",
      "SGD iter. 24/29: loss=6236828864458.006, w0=303203.90384663804, w1=114425.44766051532\n",
      "SGD iter. 25/29: loss=2862270379262.99, w0=-204044.71888253867, w1=-276217.66339364543\n",
      "SGD iter. 26/29: loss=3855068867553.785, w0=496824.4921472276, w1=275009.2947378688\n",
      "SGD iter. 27/29: loss=25525847779329.59, w0=-704196.6323160711, w1=-895141.5961472708\n",
      "SGD iter. 28/29: loss=20327690172104.5, w0=848152.2145707842, w1=526049.7226169795\n",
      "SGD iter. 29/29: loss=170732085150493.2, w0=-2598041.2307493277, w1=-1426691.1824221988\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=-0.30500000000000005, w1=-0.3446656350185137\n",
      "SGD iter. 1/29: loss=3.551258840738896, w0=0.4682545023121035, w1=0.6674881408859514\n",
      "SGD iter. 2/29: loss=25.24771607506103, w0=-1.1922411469979992, w1=-1.3100893043698747\n",
      "SGD iter. 3/29: loss=148.02000870033427, w0=2.0251865982705297, w1=0.608685498007177\n",
      "SGD iter. 4/29: loss=99.76458466442489, w0=-2.9200669682921596, w1=-2.810647626075024\n",
      "SGD iter. 5/29: loss=4335.68431554218, w0=34.0100538656598, w1=29.328832727506473\n",
      "SGD iter. 6/29: loss=250901.65081448026, w0=-126.33973362616798, w1=-90.21410256962272\n",
      "SGD iter. 7/29: loss=941304.5714995333, w0=174.46539497103794, w1=220.12132885888192\n",
      "SGD iter. 8/29: loss=1164086.8670758172, w0=-413.89462875660723, w1=-728.9469761632288\n",
      "SGD iter. 9/29: loss=26563399.80231894, w0=1499.2812223755857, w1=2038.7462933031322\n",
      "SGD iter. 10/29: loss=178026674.19779137, w0=-3343.5477239787515, w1=-3516.4232300707863\n",
      "SGD iter. 11/29: loss=790423684.6909692, w0=6112.8294132200945, w1=5121.089588310388\n",
      "SGD iter. 12/29: loss=1814820995.8403852, w0=-13798.215854611044, w1=-11287.609142750738\n",
      "SGD iter. 13/29: loss=52683599001.73358, w0=53956.73045821255, w1=41659.07290605537\n",
      "SGD iter. 14/29: loss=135597095885.48363, w0=-75484.63204151984, w1=-72524.30213265154\n",
      "SGD iter. 15/29: loss=362493670592.6836, w0=179305.45389961288, w1=213753.9590014973\n",
      "SGD iter. 16/29: loss=4005390853341.722, w0=-695038.6453434244, w1=-659743.204524986\n",
      "SGD iter. 17/29: loss=75592428472485.11, w0=2190506.21511075, w1=7778287.830062715\n",
      "SGD iter. 18/29: loss=380465425649085.75, w0=-7226023.460349186, w1=-3550553.881645089\n",
      "SGD iter. 19/29: loss=1.3226442058133296e+16, w0=46607898.18545386, w1=38020545.02335264\n",
      "SGD iter. 20/29: loss=3.1052602238656006e+17, w0=-240480186.04185125, w1=-1058130097.4125447\n",
      "SGD iter. 21/29: loss=1.4155218404827795e+19, w0=872587851.2601357, w1=-296540795.6530787\n",
      "SGD iter. 22/29: loss=3.41683809662334e+19, w0=-540943331.5627854, w1=-1379154162.6653633\n",
      "SGD iter. 23/29: loss=3.3191997645642004e+18, w0=510238326.4516008, w1=217325056.63225818\n",
      "SGD iter. 24/29: loss=3.0080352621585355e+19, w0=-1376047319.271542, w1=-990717790.0042465\n",
      "SGD iter. 25/29: loss=8.867854663273626e+19, w0=2310011252.6432843, w1=2649195666.2914114\n",
      "SGD iter. 26/29: loss=1.0399130256710401e+21, w0=-12615502755.498077, w1=-10523152098.07426\n",
      "SGD iter. 27/29: loss=4.544060324256445e+22, w0=47806506275.19908, w1=44183131514.725586\n",
      "SGD iter. 28/29: loss=1.5460971369718728e+23, w0=-60156823194.23656, w1=-38123357213.4574\n",
      "SGD iter. 29/29: loss=4.406991685459563e+22, w0=36883703219.17409, w1=108794361130.82437\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/29: loss=0.5, w0=-0.30500000000000005, w1=-0.16473426800947297\n",
      "SGD iter. 1/29: loss=2.951972008771113, w0=0.25790630013544447, w1=0.1698248963432886\n",
      "SGD iter. 2/29: loss=1.8896279952186215, w0=-0.4169168194596804, w1=-0.890202930353874\n",
      "SGD iter. 3/29: loss=38.63746218321687, w0=2.480856179558633, w1=8.063548212367156\n",
      "SGD iter. 4/29: loss=834.1843467934284, w0=-10.121608147680838, w1=2.5429815937331126\n",
      "SGD iter. 5/29: loss=30720.977211638485, w0=58.659022551061675, w1=41.8141560845127\n",
      "SGD iter. 6/29: loss=337819.2946159098, w0=-170.4395926732934, w1=-11.224423375111073\n",
      "SGD iter. 7/29: loss=5004704.901586405, w0=814.3647396383, w1=1094.6847406621391\n",
      "SGD iter. 8/29: loss=141258757.98968586, w0=-3437.126001362152, w1=-2745.107558548145\n",
      "SGD iter. 9/29: loss=808846712.6226039, w0=8740.623585493244, w1=8213.412479516217\n",
      "SGD iter. 10/29: loss=15183565814.685268, w0=-95039.62847003971, w1=-96702.66371494166\n",
      "SGD iter. 11/29: loss=4607793514448.216, w0=573229.3777321887, w1=140137.96715171135\n",
      "SGD iter. 12/29: loss=15371743405939.32, w0=-799586.2476084232, w1=-2703451.5701733516\n",
      "SGD iter. 13/29: loss=142669156539959.12, w0=2498603.1965826373, w1=910985.8679372463\n",
      "SGD iter. 14/29: loss=93948267907774.19, w0=-1577954.7316312264, w1=-6203436.544041272\n",
      "SGD iter. 15/29: loss=752892768441101.9, w0=11038521.293194247, w1=10111652.906078449\n",
      "SGD iter. 16/29: loss=1.621369025901983e+16, w0=-65163586.32553062, w1=-59083968.11661146\n",
      "SGD iter. 17/29: loss=1.7195951347796708e+18, w0=505759342.1952417, w1=302471957.16041964\n",
      "SGD iter. 18/29: loss=5.375297969467409e+19, w0=-3752084138.740919, w1=-4946992646.240109\n",
      "SGD iter. 19/29: loss=3.754859914234036e+21, w0=25873783344.73387, w1=29735874435.393063\n",
      "SGD iter. 20/29: loss=2.4294841397343914e+23, w0=-185088988921.67853, w1=-209023021183.02136\n",
      "SGD iter. 21/29: loss=4.3033485852778995e+24, w0=574476788108.263, w1=479699598170.1388\n",
      "SGD iter. 22/29: loss=2.691246793193886e+25, w0=-1582877002976.0894, w1=-1495070483031.2505\n",
      "SGD iter. 23/29: loss=3.96199199223079e+26, w0=2162934141654.4546, w1=1791995090935.6519\n",
      "SGD iter. 24/29: loss=8.124313712155617e+25, w0=2531170826353.726, w1=-4082692487585.345\n",
      "SGD iter. 25/29: loss=7.742916085129752e+26, w0=-4575752082657.82, w1=-8787201684204.291\n",
      "SGD iter. 26/29: loss=9.823014520673635e+26, w0=7507218477685.881, w1=-2145324532207.755\n",
      "SGD iter. 27/29: loss=8.932485283842266e+27, w0=-26720469613588.383, w1=-75449991505590.14\n",
      "SGD iter. 28/29: loss=9.860673740315167e+28, w0=98588973263662.1, w1=16091668343813.797\n",
      "SGD iter. 29/29: loss=1.237693414715613e+30, w0=-205574481739885.78, w1=-251998973061853.44\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.0, w1=-0.0031126076581202687\n",
      "SGD iter. 1/29: loss=0.7042658634253508, w0=0.214911185604239, w1=0.18969855103801533\n",
      "SGD iter. 2/29: loss=6.114533532049594, w0=-0.42086132796857895, w1=-0.38513531763083586\n",
      "SGD iter. 3/29: loss=3.096952590921742, w0=0.05402194750209022, w1=-0.17541633277781057\n",
      "SGD iter. 4/29: loss=4.575973814162896, w0=-0.4494886219729791, w1=-0.9087105403275816\n",
      "SGD iter. 5/29: loss=4.192563214232905, w0=0.3747109034188404, w1=0.18554195363455006\n",
      "SGD iter. 6/29: loss=7.668487826573494, w0=-0.6714821929314, w1=-1.3162516572205831\n",
      "SGD iter. 7/29: loss=57.70447522046827, w0=1.5331433084124426, w1=1.0711115844683277\n",
      "SGD iter. 8/29: loss=101.23449987678505, w0=-2.813891459708341, w1=-2.4517654613626774\n",
      "SGD iter. 9/29: loss=892.1450498787693, w0=10.374577501597523, w1=7.894230049783586\n",
      "SGD iter. 10/29: loss=15026.159489263558, w0=-27.093815451019374, w1=-32.626423566099916\n",
      "SGD iter. 11/29: loss=37121.721974459426, w0=40.79542746040784, w1=24.65209769690931\n",
      "SGD iter. 12/29: loss=33193.180451888846, w0=-27.346646934059983, w1=-46.03352807763669\n",
      "SGD iter. 13/29: loss=231021.37417902547, w0=140.73320278422932, w1=132.02450369640263\n",
      "SGD iter. 14/29: loss=1571890.8894322338, w0=-204.63287404154784, w1=-131.54625544699184\n",
      "SGD iter. 15/29: loss=1170656.742068865, w0=282.7466273850359, w1=366.2815184120339\n",
      "SGD iter. 16/29: loss=9560293.15372016, w0=-637.8922314509819, w1=-889.8739187034239\n",
      "SGD iter. 17/29: loss=8697107.416064238, w0=603.1550985696485, w1=495.2665166919844\n",
      "SGD iter. 18/29: loss=166348452.58349723, w0=-280.23214784842844, w1=-59.597075719397935\n",
      "SGD iter. 19/29: loss=113818213.99526115, w0=-529.3431097738828, w1=-366.62904143795384\n",
      "SGD iter. 20/29: loss=74256345.1277854, w0=-1141.0854736639358, w1=-696.3008459371899\n",
      "SGD iter. 21/29: loss=20603662.92045509, w0=696.9075378312441, w1=778.5326374119727\n",
      "SGD iter. 22/29: loss=102962465.96664295, w0=-2880.626105410378, w1=-1030.2714428278712\n",
      "SGD iter. 23/29: loss=680394765.1269757, w0=5054.5610535715705, w1=5518.117562172339\n",
      "SGD iter. 24/29: loss=1179126816.4445157, w0=-10502.586609264261, w1=-12151.780230491622\n",
      "SGD iter. 25/29: loss=13048185016.033459, w0=20393.39085253266, w1=39423.25054206251\n",
      "SGD iter. 26/29: loss=18126644172.08144, w0=-41527.41543472525, w1=-42478.456951468106\n",
      "SGD iter. 27/29: loss=252771787257.34674, w0=148103.79058133415, w1=151990.8809754446\n",
      "SGD iter. 28/29: loss=1582969607809.6567, w0=-523014.316526224, w1=-909556.8233274673\n",
      "SGD iter. 29/29: loss=55715475798385.836, w0=1356186.6350170374, w1=793736.7327396979\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.0, w1=0.12579145997028757\n",
      "SGD iter. 1/29: loss=0.5915304674812015, w0=-0.3638897882143748, w1=-0.062385279688873446\n",
      "SGD iter. 2/29: loss=20.0043204724395, w0=0.38808845913484225, w1=0.6284049799458736\n",
      "SGD iter. 3/29: loss=6.035281895266421, w0=0.43053119026307024, w1=0.6790112066515601\n",
      "SGD iter. 4/29: loss=4.649613131566758, w0=0.3889658470411169, w1=0.6915224066500094\n",
      "SGD iter. 5/29: loss=5.287999342333925, w0=0.2800370109316085, w1=0.6672066308989427\n",
      "SGD iter. 6/29: loss=2.2892067375996548, w0=-0.06417147964649417, w1=0.5114760100381321\n",
      "SGD iter. 7/29: loss=5.489355234682047, w0=1.1492265260357781, w1=0.9264452175031715\n",
      "SGD iter. 8/29: loss=691.3230398404722, w0=-6.242546629995301, w1=-3.787858685688708\n",
      "SGD iter. 9/29: loss=2287.6626962495216, w0=14.337407133161667, w1=64.756654979934\n",
      "SGD iter. 10/29: loss=33469.49949990168, w0=-79.89233156424747, w1=-121.57798824114415\n",
      "SGD iter. 11/29: loss=1158820.9266145977, w0=352.6794679620724, w1=165.81324042488967\n",
      "SGD iter. 12/29: loss=13166237.429435771, w0=-897.246828138443, w1=-1676.4022921035441\n",
      "SGD iter. 13/29: loss=62000102.07560713, w0=1887.2398407303622, w1=474.4234316817558\n",
      "SGD iter. 14/29: loss=241234599.66178447, w0=-1575.4247580481037, w1=-2077.2121128984636\n",
      "SGD iter. 15/29: loss=42007981.42814501, w0=-656.6910301999067, w1=-160.52353709776435\n",
      "SGD iter. 16/29: loss=127221387.5061109, w0=-1969.006910049297, w1=-1455.6067830722066\n",
      "SGD iter. 17/29: loss=54654174.39178638, w0=635.4416610627181, w1=719.7362004926988\n",
      "SGD iter. 18/29: loss=1265916804.599363, w0=-13985.581419111459, w1=1377.0595347928197\n",
      "SGD iter. 19/29: loss=50135483887.766624, w0=52047.60717038647, w1=41766.97911727586\n",
      "SGD iter. 20/29: loss=259794958071.61786, w0=-113802.52997978084, w1=-190431.6856437688\n",
      "SGD iter. 21/29: loss=1062199964142.2349, w0=244177.9782534064, w1=65530.18461487134\n",
      "SGD iter. 22/29: loss=1105027721966.5388, w0=-182727.8912481048, w1=-426910.24508859223\n",
      "SGD iter. 23/29: loss=2483796552363.365, w0=687055.6323558199, w1=1743468.3195854733\n",
      "SGD iter. 24/29: loss=184794511756665.7, w0=-7297464.64939543, w1=-4220278.447364212\n",
      "SGD iter. 25/29: loss=3.816166577304978e+16, w0=48196317.55135156, w1=30648783.159473427\n",
      "SGD iter. 26/29: loss=1.0065956662457706e+17, w0=-81882032.68262826, w1=-58818730.158297576\n",
      "SGD iter. 27/29: loss=1.717820251861041e+18, w0=326738090.89649695, w1=1621494684.9035656\n",
      "SGD iter. 28/29: loss=1.6512510365999813e+19, w0=-798485221.3948171, w1=1120945075.026978\n",
      "SGD iter. 29/29: loss=1.2867174934693569e+19, w0=734522754.8396082, w1=2170353539.396766\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.0, w1=0.007072343986729194\n",
      "SGD iter. 1/29: loss=3.954560138639659, w0=-0.6130758807793673, w1=-1.0101770762109947\n",
      "SGD iter. 2/29: loss=25.820313833858396, w0=1.9302337928201194, w1=1.1298133372765669\n",
      "SGD iter. 3/29: loss=1441.7829130629486, w0=-27.33854456030079, w1=-19.21205312319605\n",
      "SGD iter. 4/29: loss=671473.2028238132, w0=222.31043181797477, w1=76.43961803701251\n",
      "SGD iter. 5/29: loss=1888923.6560573871, w0=-379.2712097142422, w1=-505.3300149356335\n",
      "SGD iter. 6/29: loss=6369314.361462593, w0=591.3555187426957, w1=187.5051050464777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 7/29: loss=6601779.219884556, w0=-562.7233774426579, w1=-944.6772850060374\n",
      "SGD iter. 8/29: loss=63265131.578589566, w0=2615.4307392188193, w1=2816.1371200099475\n",
      "SGD iter. 9/29: loss=512919351.69867885, w0=-8543.339693765094, w1=-9706.574468472118\n",
      "SGD iter. 10/29: loss=11245616584.12178, w0=18741.246140741394, w1=15957.164400100584\n",
      "SGD iter. 11/29: loss=6344301932.377602, w0=-26838.057927707494, w1=-29677.103788736356\n",
      "SGD iter. 12/29: loss=37587258901.7409, w0=69477.73391260218, w1=77366.4748621971\n",
      "SGD iter. 13/29: loss=865353705014.919, w0=-364525.56406806986, w1=-373958.93197954283\n",
      "SGD iter. 14/29: loss=37804183618852.45, w0=5723593.610934829, w1=11021033.509975562\n",
      "SGD iter. 15/29: loss=1.7637908151482316e+16, w0=-88575973.43780927, w1=-91583686.54271372\n",
      "SGD iter. 16/29: loss=2.48595179093635e+18, w0=265628063.50122952, w1=219588937.99412096\n",
      "SGD iter. 17/29: loss=1.0333701696471368e+18, w0=-282910707.0253198, w1=-261770863.38523883\n",
      "SGD iter. 18/29: loss=9.550371227126544e+18, w0=1228878121.3280478, w1=1043958690.5488584\n",
      "SGD iter. 19/29: loss=2.4346116417151127e+20, w0=-7282230923.90424, w1=-10370229905.708305\n",
      "SGD iter. 20/29: loss=8.091537211363753e+21, w0=13132039484.026909, w1=3840371750.1049023\n",
      "SGD iter. 21/29: loss=2.808297177686351e+21, w0=-914925779.4187202, w1=-10052843638.585455\n",
      "SGD iter. 22/29: loss=3.2110556443918763e+22, w0=13517405561.136915, w1=3496087094.7802067\n",
      "SGD iter. 23/29: loss=1.5552901702343425e+22, w0=35443983128.604034, w1=24989176818.48552\n",
      "SGD iter. 24/29: loss=2.4647950427120583e+22, w0=-37349080706.13646, w1=-37947439786.377846\n",
      "SGD iter. 25/29: loss=1.1116732763610418e+23, w0=127559159257.84132, w1=79462525652.12332\n",
      "SGD iter. 26/29: loss=2.831844638687061e+24, w0=-569988830179.0914, w1=-1203320160700.0703\n",
      "SGD iter. 27/29: loss=3.030618973412585e+25, w0=2383908922621.988, w1=1685911635047.0215\n",
      "SGD iter. 28/29: loss=1.2034811310282243e+27, w0=-18704454085294.637, w1=-11461441710499.389\n",
      "SGD iter. 29/29: loss=1.0228237735224678e+29, w0=126892669520551.67, w1=116816276975174.89\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=-0.35500000000000004, w1=-0.23621080320032972\n",
      "SGD iter. 1/29: loss=8.252590297546407, w0=0.5869151989679897, w1=0.4962944322471118\n",
      "SGD iter. 2/29: loss=37.47765024633042, w0=-2.7613779828144507, w1=-3.051837755448947\n",
      "SGD iter. 3/29: loss=1380.4235873061427, w0=31.032409253063392, w1=12.877197486670521\n",
      "SGD iter. 4/29: loss=923402.3180353665, w0=-73.5716690321226, w1=-69.7141827168324\n",
      "SGD iter. 5/29: loss=376115.96640432643, w0=-104.02417906428109, w1=-125.32558285912243\n",
      "SGD iter. 6/29: loss=1230376.965106526, w0=307.57270298676343, w1=280.47239260127486\n",
      "SGD iter. 7/29: loss=6575744.796484432, w0=-504.53040670845115, w1=-458.6699604729875\n",
      "SGD iter. 8/29: loss=3300229.5546065154, w0=249.98867327109264, w1=220.01919797138936\n",
      "SGD iter. 9/29: loss=25731308.369748406, w0=-4236.099346132175, w1=-4157.682678537241\n",
      "SGD iter. 10/29: loss=8353485601.245648, w0=21808.10384326474, w1=16190.321676268448\n",
      "SGD iter. 11/29: loss=34166094045.741882, w0=-46863.83335815726, w1=-33357.667036917635\n",
      "SGD iter. 12/29: loss=80634100346.58015, w0=73197.9115999493, w1=415.6308988058736\n",
      "SGD iter. 13/29: loss=292018302315.4674, w0=-113958.81474619004, w1=-229691.1896093184\n",
      "SGD iter. 14/29: loss=591910459363.5466, w0=237441.50575090683, w1=172247.52730013867\n",
      "SGD iter. 15/29: loss=7938192846842.001, w0=-818505.0803429226, w1=-1080230.2672434119\n",
      "SGD iter. 16/29: loss=83031098983740.78, w0=3687123.163795761, w1=2291637.013398155\n",
      "SGD iter. 17/29: loss=2196578001856465.0, w0=-22925089.20624945, w1=-23498383.875858437\n",
      "SGD iter. 18/29: loss=1.6052090120646694e+17, w0=89040253.58224505, w1=71427024.24159732\n",
      "SGD iter. 19/29: loss=2.1559444943305734e+17, w0=-166734523.38880977, w1=-91943559.0458729\n",
      "SGD iter. 20/29: loss=1.4210132504780795e+18, w0=323525830.838377, w1=341339779.85368454\n",
      "SGD iter. 21/29: loss=5.393938098819119e+18, w0=-674439038.1776782, w1=-776676443.2084929\n",
      "SGD iter. 22/29: loss=1.614344962735482e+19, w0=1367174688.494996, w1=2384637110.3909006\n",
      "SGD iter. 23/29: loss=1.89628029918813e+20, w0=-4570154178.562574, w1=-3207861854.5317516\n",
      "SGD iter. 24/29: loss=2.114945001875994e+21, w0=13715808864.339193, w1=11034408128.398678\n",
      "SGD iter. 25/29: loss=1.4483586897767207e+22, w0=-39140433180.61836, w1=-29138195473.497173\n",
      "SGD iter. 26/29: loss=1.2027682844245102e+23, w0=65861845923.541214, w1=64327545498.70233\n",
      "SGD iter. 27/29: loss=6.754657745372825e+22, w0=-74191899457.9745, w1=-71637040915.1428\n",
      "SGD iter. 28/29: loss=6.882393384531515e+23, w0=172940417054.6314, w1=77578101618.45528\n",
      "SGD iter. 29/29: loss=5.201169587962268e+23, w0=-201145943150.29462, w1=-327822107302.6678\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.0, w1=0.00093403954581189\n",
      "SGD iter. 1/29: loss=0.8542780972137933, w0=-0.17566695116495953, w1=-0.17215828419176513\n",
      "SGD iter. 2/29: loss=0.5424914574613702, w0=0.01731570765928489, w1=-0.09521862772870977\n",
      "SGD iter. 3/29: loss=1.9471950021289484, w0=-0.37341842995927577, w1=-0.5299185491538927\n",
      "SGD iter. 4/29: loss=1.7995892110789335, w0=0.1442030322662684, w1=0.10050921618853237\n",
      "SGD iter. 5/29: loss=57.55637935782956, w0=-0.8656967687586274, w1=-0.7857092131264293\n",
      "SGD iter. 6/29: loss=21.28617689643858, w0=-0.15771896461275203, w1=-0.03256645837720218\n",
      "SGD iter. 7/29: loss=70.03632178890041, w0=-6.5478120300258595, w1=-6.963228475624241\n",
      "SGD iter. 8/29: loss=10503.857159267083, w0=6.742326665539906, w1=3.8998012455174873\n",
      "SGD iter. 9/29: loss=4080.2880595277256, w0=-2.820872187019475, w1=-7.492421339339407\n",
      "SGD iter. 10/29: loss=5277.4151190205885, w0=61.724407604756124, w1=47.336481307833964\n",
      "SGD iter. 11/29: loss=2487330.4495765404, w0=-372.37608211253416, w1=-468.30259082987584\n",
      "SGD iter. 12/29: loss=12871871.411709012, w0=698.1933491745402, w1=203.44933191022665\n",
      "SGD iter. 13/29: loss=6639764.531823749, w0=-670.3778540839518, w1=-1448.4815606274567\n",
      "SGD iter. 14/29: loss=188369819.2411015, w0=4273.513073553275, w1=5085.136138921531\n",
      "SGD iter. 15/29: loss=1326715569.2278607, w0=-13773.700159929567, w1=-11308.486062420481\n",
      "SGD iter. 16/29: loss=27185485047.666885, w0=53991.617106840065, w1=44370.199603759596\n",
      "SGD iter. 17/29: loss=287113623675.94305, w0=-209315.17316610902, w1=-166076.9026625075\n",
      "SGD iter. 18/29: loss=4853484990062.23, w0=722863.5532420208, w1=934549.6843293773\n",
      "SGD iter. 19/29: loss=82212242077714.33, w0=-6223561.982223751, w1=-3190303.66879242\n",
      "SGD iter. 20/29: loss=1.863250288992663e+16, w0=54743865.95897377, w1=90456147.71765548\n",
      "SGD iter. 21/29: loss=3.4465838295989344e+17, w0=-176548214.09539023, w1=-241335093.8276201\n",
      "SGD iter. 22/29: loss=1.3304098932590162e+18, w0=576009646.5387777, w1=1326913674.026187\n",
      "SGD iter. 23/29: loss=1.1018096552301116e+20, w0=-2153084556.29712, w1=-1639692309.6175966\n",
      "SGD iter. 24/29: loss=5.876549828693409e+19, w0=1795451454.8993754, w1=1124640841.8630786\n",
      "SGD iter. 25/29: loss=2.771230145603322e+20, w0=-4222651041.4630322, w1=-5326437811.504742\n",
      "SGD iter. 26/29: loss=5.1959021208244434e+20, w0=7994288204.30538, w1=8165431343.881594\n",
      "SGD iter. 27/29: loss=2.2141967308001125e+22, w0=-41648877427.11065, w1=-21201507143.65972\n",
      "SGD iter. 28/29: loss=6.992872878947054e+22, w0=74930141118.91525, w1=316909647385.2203\n",
      "SGD iter. 29/29: loss=7.054462299906082e+23, w0=-150644537341.01242, w1=169863287003.31265\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=-0.35500000000000004, w1=-0.31212232410921065\n",
      "SGD iter. 1/29: loss=5.50496102886426, w0=1.351504996212971, w1=1.312112039089672\n",
      "SGD iter. 2/29: loss=1309.379788044169, w0=-6.85558730761674, w1=-15.169866178618093\n",
      "SGD iter. 3/29: loss=1905.097992493356, w0=12.582776574094854, w1=-0.9746694248318448\n",
      "SGD iter. 4/29: loss=5231.911440321593, w0=-37.23848814067213, w1=-32.14813350480707\n",
      "SGD iter. 5/29: loss=944580.5949710284, w0=236.5284372974782, w1=243.6874686857653\n",
      "SGD iter. 6/29: loss=2342117.6617532116, w0=-403.5877298289334, w1=-257.0268487299911\n",
      "SGD iter. 7/29: loss=4504236.316332114, w0=582.8496810182235, w1=419.15376931257447\n",
      "SGD iter. 8/29: loss=57751105.31675666, w0=-4002.4901073018, w1=-6161.551711425985\n",
      "SGD iter. 9/29: loss=3606806883.440654, w0=10426.563935568138, w1=3829.367883884478\n",
      "SGD iter. 10/29: loss=1889354052.9936955, w0=-13392.834613063522, w1=-14997.954915938928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 11/29: loss=14473580136.666721, w0=30351.79799205519, w1=37629.59323027046\n",
      "SGD iter. 12/29: loss=32257258423.8079, w0=-50235.397550854745, w1=-43653.95879209462\n",
      "SGD iter. 13/29: loss=241069805792.54095, w0=178249.3106895344, w1=95714.20365242781\n",
      "SGD iter. 14/29: loss=4785348662207.002, w0=-160543.1293572805, w1=-323913.61980981403\n",
      "SGD iter. 15/29: loss=1455254097950.1685, w0=243501.90554447199, w1=365737.97654410533\n",
      "SGD iter. 16/29: loss=8850428998724.713, w0=-561599.4132680977, w1=-702085.2079556665\n",
      "SGD iter. 17/29: loss=8614152638789.065, w0=653710.7144423383, w1=1047311.1656857239\n",
      "SGD iter. 18/29: loss=32103582108844.523, w0=-1620498.611714101, w1=-1563180.9016508968\n",
      "SGD iter. 19/29: loss=45171013446004.23, w0=1669315.0797289978, w1=1030414.1738961302\n",
      "SGD iter. 20/29: loss=288175549782719.25, w0=-5507515.033027049, w1=-7369875.925330624\n",
      "SGD iter. 21/29: loss=3229775871592172.0, w0=14632201.6415721, w1=9935375.565712996\n",
      "SGD iter. 22/29: loss=1.0041849522011072e+16, w0=-22132978.397163313, w1=-35797057.46331946\n",
      "SGD iter. 23/29: loss=3.979574309943644e+16, w0=27913051.596875135, w1=10035991.035686329\n",
      "SGD iter. 24/29: loss=1.0673530869613102e+16, w0=112989611.68660647, w1=61820541.722591326\n",
      "SGD iter. 25/29: loss=3.4013514702921257e+18, w0=-1974805002.1883633, w1=-1555720486.6301126\n",
      "SGD iter. 26/29: loss=2.935388010331163e+21, w0=16178638599.75603, w1=11870665970.306717\n",
      "SGD iter. 27/29: loss=2.0616745196546364e+22, w0=-17429358706.38987, w1=-18097509318.12183\n",
      "SGD iter. 28/29: loss=5.011433350935662e+21, w0=20307252427.594444, w1=45305700290.39015\n",
      "SGD iter. 29/29: loss=7.868012366346176e+22, w0=-70015552446.76706, w1=4692491412.752991\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.0, w1=0.0867773666725048\n",
      "SGD iter. 1/29: loss=0.6217941613607796, w0=-0.08313068993237055, w1=0.0018514543217490603\n",
      "SGD iter. 2/29: loss=0.450723769365184, w0=0.1526631693481438, w1=0.20441304826374518\n",
      "SGD iter. 3/29: loss=21.39180771560817, w0=-2.457662581453521, w1=-2.215544120427001\n",
      "SGD iter. 4/29: loss=1155.6633629334058, w0=6.618575283687726, w1=5.218680874609237\n",
      "SGD iter. 5/29: loss=774.8616775872501, w0=-10.414728068301901, w1=-10.063449443282558\n",
      "SGD iter. 6/29: loss=7045.17004049053, w0=59.38708043502541, w1=46.971381624908915\n",
      "SGD iter. 7/29: loss=1470936.8500449443, w0=-178.18890611349738, w1=-179.5525561786506\n",
      "SGD iter. 8/29: loss=444326.24682942475, w0=-822.7542555524227, w1=-763.1772191728567\n",
      "SGD iter. 9/29: loss=591835343.4507605, w0=4474.406136764002, w1=4964.902590978563\n",
      "SGD iter. 10/29: loss=345591875.4160326, w0=-6444.9343244315405, w1=-7024.027997904295\n",
      "SGD iter. 11/29: loss=7616463258.20151, w0=53143.97766668854, w1=101811.46692642286\n",
      "SGD iter. 12/29: loss=541886549955.77185, w0=-594462.8426208497, w1=-586837.943194778\n",
      "SGD iter. 13/29: loss=122657771801927.84, w0=3891393.6275393097, w1=4404370.542725957\n",
      "SGD iter. 14/29: loss=950668743779265.1, w0=-9074896.285153262, w1=-8120610.3910654085\n",
      "SGD iter. 15/29: loss=2622457233808430.5, w0=21130868.953512836, w1=30532946.86205549\n",
      "SGD iter. 16/29: loss=7.102056668826188e+16, w0=-116156966.09122626, w1=-13937015.908508476\n",
      "SGD iter. 17/29: loss=2.90943143619076e+18, w0=179166817.97540867, w1=486174780.614042\n",
      "SGD iter. 18/29: loss=1.0506326244559188e+18, w0=-431201650.7646928, w1=-41277798.089463234\n",
      "SGD iter. 19/29: loss=7.260744882673182e+18, w0=706642531.4853852, w1=1215467448.1823323\n",
      "SGD iter. 20/29: loss=2.3855929371926643e+19, w0=-1817113988.4074767, w1=-27396860.614385605\n",
      "SGD iter. 21/29: loss=1.922285952577428e+21, w0=22143958152.814022, w1=34007021237.80355\n",
      "SGD iter. 22/29: loss=7.1217892602018825e+22, w0=-143642139417.743, w1=-351893191143.0716\n",
      "SGD iter. 23/29: loss=2.8527257569642106e+24, w0=415522999779.91077, w1=242964995683.46686\n",
      "SGD iter. 24/29: loss=2.1546425241441942e+24, w0=-225279341566.1422, w1=-121127316981.88306\n",
      "SGD iter. 25/29: loss=3.4107194132129123e+25, w0=1503761411191.9893, w1=1605436851365.847\n",
      "SGD iter. 26/29: loss=6.20703678942446e+25, w0=-2812275545427.076, w1=-1276132292739.7556\n",
      "SGD iter. 27/29: loss=8.715551404882907e+26, w0=10800406404121.863, w1=13321197955588.342\n",
      "SGD iter. 28/29: loss=9.168114894005303e+27, w0=-28392453332559.574, w1=-22901299148718.297\n",
      "SGD iter. 29/29: loss=3.3563259615653546e+28, w0=110102505640027.2, w1=87181329436925.77\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.0, w1=-0.052857702982279535\n",
      "SGD iter. 1/29: loss=0.4566034761107107, w0=0.07615376494076263, w1=0.13933036819523906\n",
      "SGD iter. 2/29: loss=0.6318790314581656, w0=-0.47242386325739194, w1=-0.2508361016136117\n",
      "SGD iter. 3/29: loss=5.653440138932636, w0=1.0869091030917057, w1=1.8403273856100704\n",
      "SGD iter. 4/29: loss=256.024498375289, w0=-8.853310003756901, w1=-6.115968613854773\n",
      "SGD iter. 5/29: loss=18478.37391641017, w0=34.847860685323255, w1=18.88365940376396\n",
      "SGD iter. 6/29: loss=33724.05478536556, w0=-75.0490351888751, w1=-41.49426876739294\n",
      "SGD iter. 7/29: loss=1180392.3925393033, w0=1404.4356533448158, w1=1271.5353106417608\n",
      "SGD iter. 8/29: loss=1855619026.6909513, w0=-12989.73948040329, w1=-1412.7458777906943\n",
      "SGD iter. 9/29: loss=14024023256.012651, w0=40534.42336394052, w1=48897.74182170851\n",
      "SGD iter. 10/29: loss=128345055619.44334, w0=-62161.55257866474, w1=-28871.16621169288\n",
      "SGD iter. 11/29: loss=40342752815.01959, w0=53862.32726558559, w1=61357.16354359547\n",
      "SGD iter. 12/29: loss=498984114545.6616, w0=-68170.90706657525, w1=-45730.788295109975\n",
      "SGD iter. 13/29: loss=159267705498.84314, w0=-231850.7713354373, w1=-271236.72531067213\n",
      "SGD iter. 14/29: loss=12276087169618.834, w0=1556138.2712853504, w1=1373728.5912458303\n",
      "SGD iter. 15/29: loss=230708689792202.78, w0=-4109050.6838941053, w1=-2750988.7603473198\n",
      "SGD iter. 16/29: loss=617100822482887.1, w0=9246505.77494765, w1=9145429.658884158\n",
      "SGD iter. 17/29: loss=9937473805593100.0, w0=-36189436.763986565, w1=-182513020.9519356\n",
      "SGD iter. 18/29: loss=1.2155753987990086e+17, w0=105572227.6181908, w1=-87240279.96694276\n",
      "SGD iter. 19/29: loss=3.5550750184932413e+18, w0=-155835656.02419007, w1=-427209310.87317586\n",
      "SGD iter. 20/29: loss=9.806122094333039e+17, w0=-261426910.58045465, w1=9133184.341394544\n",
      "SGD iter. 21/29: loss=3.3880294570054586e+19, w0=2536567496.4680853, w1=3262763939.3420877\n",
      "SGD iter. 22/29: loss=7.775110749799911e+20, w0=-9834260411.680305, w1=-12091573812.13124\n",
      "SGD iter. 23/29: loss=6.039187927195284e+21, w0=57292175668.02801, w1=24738368337.666992\n",
      "SGD iter. 24/29: loss=2.328507663145767e+24, w0=-453825435519.68146, w1=-544782346200.7555\n",
      "SGD iter. 25/29: loss=1.2967466305988107e+25, w0=1049141398873.52, w1=463788607796.83264\n",
      "SGD iter. 26/29: loss=5.551857383061622e+25, w0=-4160478303913.4966, w1=-3625085463149.283\n",
      "SGD iter. 27/29: loss=4.130253713429254e+27, w0=34178326780688.652, w1=35116454344925.72\n",
      "SGD iter. 28/29: loss=2.6105742742352693e+29, w0=-200388330189815.0, w1=-162790441268248.2\n",
      "SGD iter. 29/29: loss=4.787529087955451e+30, w0=766291614315665.9, w1=1012496060750591.8\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.0, w1=0.058230514408172875\n",
      "SGD iter. 1/29: loss=12.342533526800464, w0=0.20446457166845736, w1=0.23051137512182532\n",
      "SGD iter. 2/29: loss=17.806336347538547, w0=-3.265972760509461, w1=-2.872731438469115\n",
      "SGD iter. 3/29: loss=4053.800136253909, w0=23.786109050249753, w1=34.23988996515131\n",
      "SGD iter. 4/29: loss=57369.90565934509, w0=-108.4619599893925, w1=-192.69884590205862\n",
      "SGD iter. 5/29: loss=1105044.5928145559, w0=343.270482014173, w1=143.55678783586274\n",
      "SGD iter. 6/29: loss=7234325.662834442, w0=-683.0469914522525, w1=-740.455790718858\n",
      "SGD iter. 7/29: loss=14149028.919131817, w0=1560.639141796031, w1=1981.133383035142\n",
      "SGD iter. 8/29: loss=416011080.8952625, w0=-14143.334668283851, w1=-16563.411119651173\n",
      "SGD iter. 9/29: loss=72939477316.54071, w0=125859.04788691638, w1=71347.79442834397\n",
      "SGD iter. 10/29: loss=1670483321119.8774, w0=-1197185.8669076734, w1=-980622.3001517704\n",
      "SGD iter. 11/29: loss=1513479825775414.0, w0=20409707.96487235, w1=16653460.32862567\n",
      "SGD iter. 12/29: loss=4.686531744681362e+16, w0=-83897176.26244885, w1=-116271737.91897386\n",
      "SGD iter. 13/29: loss=7.608291668583363e+17, w0=673728691.0175555, w1=788715693.8122599\n",
      "SGD iter. 14/29: loss=3.094181027715317e+20, w0=-7495348563.316768, w1=-10059691261.590134\n",
      "SGD iter. 15/29: loss=5.481723897967153e+21, w0=14366152942.507084, w1=15111435230.529285\n",
      "SGD iter. 16/29: loss=3.139085561295554e+21, w0=-23316200568.924076, w1=-14547242602.2836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 17/29: loss=2.224153191697515e+22, w0=63077228465.71524, w1=40712856031.186386\n",
      "SGD iter. 18/29: loss=8.996676541042755e+23, w0=-388926312104.5834, w1=-383644222148.85266\n",
      "SGD iter. 19/29: loss=1.5511339953549647e+25, w0=1149160276304.1963, w1=277572524458.8789\n",
      "SGD iter. 20/29: loss=5.05238875724555e+25, w0=-1898656998529.3203, w1=-3051641174713.0845\n",
      "SGD iter. 21/29: loss=1.6968501818077027e+26, w0=4236547350615.3467, w1=2684953920890.601\n",
      "SGD iter. 22/29: loss=3.968335705601639e+26, w0=-6170052972355.077, w1=-6124005005820.816\n",
      "SGD iter. 23/29: loss=5.200343525691004e+27, w0=16252925398750.48, w1=9618983346034.889\n",
      "SGD iter. 24/29: loss=8.04054318667859e+27, w0=-30880366988414.3, w1=-53213440904535.76\n",
      "SGD iter. 25/29: loss=1.7685999044871652e+29, w0=194051805778438.25, w1=218096329608389.9\n",
      "SGD iter. 26/29: loss=5.581295317090309e+30, w0=-559040664916322.5, w1=-573521517272972.2\n",
      "SGD iter. 27/29: loss=6.619893473298313e+30, w0=930590603487145.5, w1=892972119834990.8\n",
      "SGD iter. 28/29: loss=2.005252287194492e+32, w0=-1.0040942453304902e+16, w1=-1.3620258426241058e+16\n",
      "SGD iter. 29/29: loss=6.254165746353205e+34, w0=3.6361539571618216e+16, w1=2.711076038199408e+16\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=-0.405, w1=-0.2965101569666538\n",
      "SGD iter. 1/29: loss=20.6616846173416, w0=2.6526123953641996, w1=3.162589513205129\n",
      "SGD iter. 2/29: loss=2461.440721168611, w0=-17.192800438064808, w1=-17.386016383188704\n",
      "SGD iter. 3/29: loss=20485.348815206922, w0=30.519277409442076, w1=-1.39111738263564\n",
      "SGD iter. 4/29: loss=8900.097528755452, w0=-8.923270477453968, w1=-25.098433524754917\n",
      "SGD iter. 5/29: loss=55153.47143166525, w0=34.43769194385085, w1=5.372714484222282\n",
      "SGD iter. 6/29: loss=15733.5905158847, w0=31.697284202213343, w1=18.766206208393083\n",
      "SGD iter. 7/29: loss=25507.951095119457, w0=-46.439654980802246, w1=-25.030145545804704\n",
      "SGD iter. 8/29: loss=121987.00611644574, w0=83.18243719571261, w1=130.06800503826148\n",
      "SGD iter. 9/29: loss=327670.3956475859, w0=-229.50088532174374, w1=-223.88916570648243\n",
      "SGD iter. 10/29: loss=1378917.947954572, w0=391.72170160616594, w1=319.36190882200515\n",
      "SGD iter. 11/29: loss=15684511.108763507, w0=-3030.5973993157436, w1=-3267.2559273523666\n",
      "SGD iter. 12/29: loss=3878677256.6644006, w0=44449.54860676812, w1=47733.14740359215\n",
      "SGD iter. 13/29: loss=583403982510.2979, w0=-168874.88606721722, w1=-372026.8016171546\n",
      "SGD iter. 14/29: loss=870830231154.0162, w0=160029.88808747893, w1=-187150.65522042508\n",
      "SGD iter. 15/29: loss=4960081005678.078, w0=-1135681.6233945359, w1=-1456837.5011933348\n",
      "SGD iter. 16/29: loss=380684388243486.7, w0=11683086.5142815, w1=15262267.232689817\n",
      "SGD iter. 17/29: loss=3.77667985116313e+16, w0=-117507675.87891659, w1=-152105462.0283\n",
      "SGD iter. 18/29: loss=3.4845085838242463e+18, w0=260417816.71219298, w1=170203184.44040745\n",
      "SGD iter. 19/29: loss=1.0062817210408497e+18, w0=-363382671.6664835, w1=-437489907.9097689\n",
      "SGD iter. 20/29: loss=8.152642809920474e+18, w0=1513487787.9512296, w1=3686927731.4009976\n",
      "SGD iter. 21/29: loss=4.048166295277851e+20, w0=-8994045201.434252, w1=-8760739319.632679\n",
      "SGD iter. 22/29: loss=7.838592567739146e+21, w0=57933012639.4341, w1=118053345059.48792\n",
      "SGD iter. 23/29: loss=7.845955806766544e+23, w0=-260857712529.2953, w1=49572433582.01839\n",
      "SGD iter. 24/29: loss=2.3790255114054203e+24, w0=491329786658.0116, w1=744285738138.0267\n",
      "SGD iter. 25/29: loss=1.6039899122709175e+25, w0=-1877200134828.4355, w1=-2242195194627.712\n",
      "SGD iter. 26/29: loss=6.247157821330473e+26, w0=11224060076749.453, w1=10342855231560.922\n",
      "SGD iter. 27/29: loss=1.6908710461295163e+28, w0=-69873156236476.766, w1=-20733134364102.914\n",
      "SGD iter. 28/29: loss=1.645984466728351e+30, w0=380970669392656.56, w1=143928228035808.47\n",
      "SGD iter. 29/29: loss=4.7706116317726556e+30, w0=-1471030098096964.8, w1=-1805876245622773.0\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=-0.455, w1=-0.31133949265785843\n",
      "SGD iter. 1/29: loss=27.95439367293067, w0=3.011831493138773, w1=6.9285377617820965\n",
      "SGD iter. 2/29: loss=1643.587293266377, w0=-16.937639265101627, w1=-18.541627035787002\n",
      "SGD iter. 3/29: loss=17568.17907528505, w0=91.15903146428529, w1=88.62874619433381\n",
      "SGD iter. 4/29: loss=4171157.1645071455, w0=-637.150713239649, w1=-477.5786192284243\n",
      "SGD iter. 5/29: loss=19887567.66482371, w0=2071.993856817056, w1=2503.5135001414355\n",
      "SGD iter. 6/29: loss=465371831.2359725, w0=-6141.87543672941, w1=-3444.7443728455737\n",
      "SGD iter. 7/29: loss=743352861.7563144, w0=11212.452620179076, w1=4658.99786719977\n",
      "SGD iter. 8/29: loss=28477928348.869717, w0=-22909.474319080455, w1=-19346.23752134458\n",
      "SGD iter. 9/29: loss=7889355452.765348, w0=14069.23385177823, w1=10465.420259361465\n",
      "SGD iter. 10/29: loss=31493129253.913197, w0=-69223.04462296786, w1=-61581.79200257483\n",
      "SGD iter. 11/29: loss=342410555449.4108, w0=173316.52130512567, w1=327617.16557193035\n",
      "SGD iter. 12/29: loss=1050724943172.1708, w0=-692700.2558327555, w1=-1665335.5051392135\n",
      "SGD iter. 13/29: loss=117545368067321.78, w0=5300117.407963652, w1=6432687.62396715\n",
      "SGD iter. 14/29: loss=5947689088644943.0, w0=-41850164.54879324, w1=-54471088.2888369\n",
      "SGD iter. 15/29: loss=3.395132376644798e+17, w0=202115603.93179393, w1=103233117.18202335\n",
      "SGD iter. 16/29: loss=1.5461078876276503e+18, w0=-481387299.60760605, w1=-400277769.587862\n",
      "SGD iter. 17/29: loss=1.3420533353502226e+19, w0=2575783343.937346, w1=2134088278.3925285\n",
      "SGD iter. 18/29: loss=2.9657497785689896e+21, w0=-10441023508.366922, w1=-9288563861.590359\n",
      "SGD iter. 19/29: loss=2.3363282430758313e+21, w0=22954921496.0624, w1=6202376331.112841\n",
      "SGD iter. 20/29: loss=9.615447760802364e+22, w0=-74758446756.90134, w1=-58060923937.66057\n",
      "SGD iter. 21/29: loss=7.780800245571523e+22, w0=109384633241.9458, w1=107913794922.04912\n",
      "SGD iter. 22/29: loss=1.1387604640390616e+24, w0=-426572242738.0742, w1=-918615224208.0414\n",
      "SGD iter. 23/29: loss=1.1754283873482655e+25, w0=1693469794868.2764, w1=-17560856211.163208\n",
      "SGD iter. 24/29: loss=8.996195492915749e+26, w0=-39496107578227.21, w1=-40729297343474.27\n",
      "SGD iter. 25/29: loss=9.560995412813233e+29, w0=1389052453963586.5, w1=1316888436393717.5\n",
      "SGD iter. 26/29: loss=2.985166357019836e+33, w0=-8175765343371338.0, w1=-1.4439072051100906e+16\n",
      "SGD iter. 27/29: loss=1.4283894122516305e+33, w0=7072818839353504.0, w1=-7419656920586611.0\n",
      "SGD iter. 28/29: loss=6.027330878912297e+33, w0=-4.868587466251084e+16, w1=-9.277675375703154e+16\n",
      "SGD iter. 29/29: loss=4.767240776481366e+35, w0=3.753019708531242e+17, w1=1.5497986317441747e+17\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.0, w1=0.11870149935427779\n",
      "SGD iter. 1/29: loss=2.635580240378601, w0=-0.11020911179302276, w1=0.24033886597780302\n",
      "SGD iter. 2/29: loss=1.8629046597268561, w0=0.06816997408936709, w1=0.4849602865173705\n",
      "SGD iter. 3/29: loss=1.9926279437096006, w0=-0.5022391096728329, w1=-0.004603759037246358\n",
      "SGD iter. 4/29: loss=4.200894366701003, w0=1.2009327644785652, w1=1.2931941236345432\n",
      "SGD iter. 5/29: loss=234.33333653701, w0=-11.808097671314462, w1=-5.659138210716671\n",
      "SGD iter. 6/29: loss=50874.57647192689, w0=29.79889147848324, w1=54.83462869215644\n",
      "SGD iter. 7/29: loss=16726.829359035713, w0=8.064830906570464, w1=52.34455563441883\n",
      "SGD iter. 8/29: loss=42631.2577071594, w0=-9.212343922707344, w1=35.92049074887525\n",
      "SGD iter. 9/29: loss=59629.122613882784, w0=69.75065621594005, w1=122.35871996332904\n",
      "SGD iter. 10/29: loss=257149.4061818884, w0=-240.5096334751916, w1=-381.43712332245195\n",
      "SGD iter. 11/29: loss=7108967.326530161, w0=1368.6537952883812, w1=1049.6954708153169\n",
      "SGD iter. 12/29: loss=228688068.4833716, w0=-6343.768012463595, w1=-7970.127597468389\n",
      "SGD iter. 13/29: loss=3505936306.422117, w0=16624.774901965568, w1=12185.481904208216\n",
      "SGD iter. 14/29: loss=7332266622.425678, w0=-35866.96174915689, w1=-41769.97751905646\n",
      "SGD iter. 15/29: loss=133565793834.81615, w0=337585.5251102921, w1=343529.27791136847\n",
      "SGD iter. 16/29: loss=51257586547695.195, w0=-5050960.345231463, w1=-11852243.769995743\n",
      "SGD iter. 17/29: loss=5218254350748351.0, w0=29785493.621552695, w1=17761479.122391626\n",
      "SGD iter. 18/29: loss=6.923102407014515e+16, w0=-93332447.44273779, w1=-5683292.743288305\n",
      "SGD iter. 19/29: loss=4.208292440569959e+17, w0=185641483.96885642, w1=239124993.2515202\n",
      "SGD iter. 20/29: loss=1.4511047244982656e+18, w0=-719725097.0530115, w1=-337660485.4665556\n",
      "SGD iter. 21/29: loss=1.9862549279292752e+20, w0=16581769962.707338, w1=5191845988.534793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 22/29: loss=1.5527103412546854e+23, w0=-62068224911.35133, w1=-46796416808.93925\n",
      "SGD iter. 23/29: loss=4.942478067956104e+22, w0=11211697255.014526, w1=37610173370.47239\n",
      "SGD iter. 24/29: loss=1.6868808565219918e+23, w0=-782220051041.156, w1=-923566930608.1552\n",
      "SGD iter. 25/29: loss=1.464878422803965e+27, w0=24671368573873.465, w1=27474326947804.07\n",
      "SGD iter. 26/29: loss=1.6399446617787743e+29, w0=-154563278334653.9, w1=-126825684562716.06\n",
      "SGD iter. 27/29: loss=1.2612373723707114e+30, w0=677583959644744.6, w1=783702068726666.5\n",
      "SGD iter. 28/29: loss=1.0400138621305942e+32, w0=-3956847172083718.5, w1=-2888284483384271.0\n",
      "SGD iter. 29/29: loss=1.977370623162997e+33, w0=1.4800154959662414e+16, w1=2.2139835185795236e+16\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.0, w1=-0.07152966930233444\n",
      "SGD iter. 1/29: loss=1.738183173321183, w0=-0.16991202836700775, w1=-0.1970267943406654\n",
      "SGD iter. 2/29: loss=5.9196748648257485, w0=1.8790239157533215, w1=2.0968258023134863\n",
      "SGD iter. 3/29: loss=777.0924227042048, w0=-11.225870321112097, w1=-10.891871462800538\n",
      "SGD iter. 4/29: loss=8056.447152979846, w0=26.65964843680368, w1=9.220630719303568\n",
      "SGD iter. 5/29: loss=21262.604229490706, w0=-61.852243737535304, w1=-52.057397328769525\n",
      "SGD iter. 6/29: loss=138295.34698394864, w0=100.04204693380169, w1=60.86407691612429\n",
      "SGD iter. 7/29: loss=97841.74518602154, w0=-30.081549079048273, w1=-96.02844448868042\n",
      "SGD iter. 8/29: loss=613646.0914204203, w0=263.5240670062823, w1=100.13063909233928\n",
      "SGD iter. 9/29: loss=2973272.2148266146, w0=-573.8781555802689, w1=-471.3217300048718\n",
      "SGD iter. 10/29: loss=15250312.019794544, w0=1915.6258648547828, w1=1759.7220229362333\n",
      "SGD iter. 11/29: loss=562628796.6754771, w0=-7627.753919800927, w1=-18446.06692967107\n",
      "SGD iter. 12/29: loss=2905295853.843511, w0=25484.72550719248, w1=10296.512432092823\n",
      "SGD iter. 13/29: loss=25689073008.587055, w0=-70253.3404382488, w1=-81973.78438621963\n",
      "SGD iter. 14/29: loss=472605378430.2822, w0=244357.13845595406, w1=216932.18070374965\n",
      "SGD iter. 15/29: loss=2558841627433.279, w0=-625388.5597263753, w1=-439536.2124485052\n",
      "SGD iter. 16/29: loss=13789119197825.014, w0=1394879.3189290543, w1=1333311.573441244\n",
      "SGD iter. 17/29: loss=202673659695001.4, w0=-1935453.8194805174, w1=-354328.7667487366\n",
      "SGD iter. 18/29: loss=48531400739403.53, w0=-4916358.451670168, w1=-4488869.832444573\n",
      "SGD iter. 19/29: loss=9829971174685254.0, w0=31921355.360169433, w1=22780903.8479407\n",
      "SGD iter. 20/29: loss=1.0093492239334936e+17, w0=-105231321.39931533, w1=-121668068.57824542\n",
      "SGD iter. 21/29: loss=7.253749242162086e+17, w0=509236781.80145776, w1=486664906.2731155\n",
      "SGD iter. 22/29: loss=5.218502685849595e+19, w0=-5131214748.744836, w1=-5203905525.192625\n",
      "SGD iter. 23/29: loss=5.672009799610984e+21, w0=42726142262.7224, w1=43538753573.37616\n",
      "SGD iter. 24/29: loss=2.3380779881323577e+23, w0=-279605433973.46826, w1=-185052224644.1979\n",
      "SGD iter. 25/29: loss=1.469642213589825e+25, w0=3020734618818.5483, w1=12940219574904.043\n",
      "SGD iter. 26/29: loss=2.834305652685893e+27, w0=-22734077498164.82, w1=-2457794062446.6367\n",
      "SGD iter. 27/29: loss=3.356451959269864e+28, w0=90829424788283.16, w1=101530401919443.23\n",
      "SGD iter. 28/29: loss=1.271518431074736e+30, w0=-868978455283741.2, w1=-472490205215498.5\n",
      "SGD iter. 29/29: loss=4.4399896164596805e+32, w0=8085508824029018.0, w1=8232349707009510.0\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.455, w1=0.5634372024178361\n",
      "SGD iter. 1/29: loss=13.83965436231353, w0=-1.1454038542706366, w1=-1.0469946037529834\n",
      "SGD iter. 2/29: loss=27.068839761761463, w0=1.9515995724725321, w1=1.7606168559612354\n",
      "SGD iter. 3/29: loss=363.20350900072475, w0=-16.66812218353901, w1=-15.437509658961336\n",
      "SGD iter. 4/29: loss=136456.81212961354, w0=192.27278001219466, w1=123.15058773063076\n",
      "SGD iter. 5/29: loss=6788627.625439447, w0=-3659.0266028247293, w1=-3264.4270487491885\n",
      "SGD iter. 6/29: loss=13017618248.584955, w0=23692.415519366215, w1=15528.081617035994\n",
      "SGD iter. 7/29: loss=7135540719.578187, w0=-27092.071142718916, w1=-7677.496570627827\n",
      "SGD iter. 8/29: loss=86008382767.87625, w0=52973.0096586379, w1=120267.82196287213\n",
      "SGD iter. 9/29: loss=68770639212.81158, w0=-78799.46156708, w1=-7200.3751160471\n",
      "SGD iter. 10/29: loss=2606291832698.017, w0=286725.9625181914, w1=313559.5328260963\n",
      "SGD iter. 11/29: loss=1228262524022.47, w0=-288781.8051925243, w1=-139925.96060876973\n",
      "SGD iter. 12/29: loss=26860548857297.824, w0=878720.891114973, w1=283658.8809797871\n",
      "SGD iter. 13/29: loss=9052555250432.158, w0=2685431.3932570033, w1=2690338.750916901\n",
      "SGD iter. 14/29: loss=2770584617904833.0, w0=-8685032.758727796, w1=-11636411.304423563\n",
      "SGD iter. 15/29: loss=1128900475089852.5, w0=-2517070.0703559145, w1=-3986694.4448571214\n",
      "SGD iter. 16/29: loss=1290772070348120.5, w0=150596.91616174532, w1=-2322015.182956703\n",
      "SGD iter. 17/29: loss=1852619298359133.0, w0=-13045158.769495351, w1=-17862744.210297503\n",
      "SGD iter. 18/29: loss=1.1398638353534398e+16, w0=49040802.435042575, w1=53271402.53744086\n",
      "SGD iter. 19/29: loss=1.7866817470913293e+17, w0=-219386955.867087, w1=-119364956.71909371\n",
      "SGD iter. 20/29: loss=6.165490605104182e+18, w0=712734555.9579625, w1=858923851.9391161\n",
      "SGD iter. 21/29: loss=1.7847280066315923e+19, w0=-2098200609.5458927, w1=-2924816820.3455424\n",
      "SGD iter. 22/29: loss=9.089269213083507e+20, w0=18573312334.95295, w1=18356158616.374714\n",
      "SGD iter. 23/29: loss=5.056543997448906e+22, w0=-160258871622.9289, w1=-141657037730.21014\n",
      "SGD iter. 24/29: loss=7.580835705542483e+24, w0=1691689596339.0605, w1=1946813840722.5444\n",
      "SGD iter. 25/29: loss=4.24913595019496e+26, w0=-14337806341903.701, w1=-13375267008896.436\n",
      "SGD iter. 26/29: loss=8.953822130268427e+28, w0=169792002362603.84, w1=210506212360528.66\n",
      "SGD iter. 27/29: loss=3.650040675375852e+30, w0=-688838658325964.1, w1=-758336354091021.1\n",
      "SGD iter. 28/29: loss=2.647846507701824e+31, w0=1560979028715002.0, w1=601457053459803.1\n",
      "SGD iter. 29/29: loss=6.189366238656231e+31, w0=-3246236398449157.0, w1=-3704138519408075.5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.505, w1=0.5099896279850182\n",
      "SGD iter. 1/29: loss=63.61562093550542, w0=-2.9696516073008183, w1=-2.5539241952053127\n",
      "SGD iter. 2/29: loss=368.73638341594096, w0=8.401485856018793, w1=10.1547725893472\n",
      "SGD iter. 3/29: loss=8946.972889235924, w0=-66.11224372301785, w1=-70.1785819984151\n",
      "SGD iter. 4/29: loss=773841.9840994019, w0=316.8792280758395, w1=268.47529694930734\n",
      "SGD iter. 5/29: loss=4055147.234123146, w0=-871.1892182489852, w1=-611.257633109192\n",
      "SGD iter. 6/29: loss=31031736.549985845, w0=3657.442440658541, w1=4010.702287357004\n",
      "SGD iter. 7/29: loss=3067139406.135546, w0=-6979.444566377821, w1=-3251.4900345317783\n",
      "SGD iter. 8/29: loss=806318133.8932233, w0=-11611.430112117032, w1=-2484.5234431479475\n",
      "SGD iter. 9/29: loss=13820811907.589144, w0=44735.21628083696, w1=37974.588209961774\n",
      "SGD iter. 10/29: loss=107622179996.33984, w0=-197474.06158229694, w1=-188438.59046553777\n",
      "SGD iter. 11/29: loss=7289055201179.705, w0=958220.4615524929, w1=464127.91727127915\n",
      "SGD iter. 12/29: loss=34517567736653.29, w0=-3083750.683538002, w1=-3131510.1786698946\n",
      "SGD iter. 13/29: loss=941209553400251.5, w0=12753626.841026, w1=18264531.014393155\n",
      "SGD iter. 14/29: loss=6966320825020696.0, w0=-29645331.505344614, w1=-8091194.320445184\n",
      "SGD iter. 15/29: loss=1.727585707412288e+16, w0=51514427.954510644, w1=29215839.97627532\n",
      "SGD iter. 16/29: loss=1.5752291652203584e+17, w0=-166685678.92563188, w1=-111881909.18660033\n",
      "SGD iter. 17/29: loss=1.92868729458149e+18, w0=1050289376.4112056, w1=2316124620.622107\n",
      "SGD iter. 18/29: loss=2.6518887419147295e+20, w0=-8353469722.277195, w1=-16422972212.182924\n",
      "SGD iter. 19/29: loss=8.505927684526298e+21, w0=57111127377.88986, w1=66254961673.21509\n",
      "SGD iter. 20/29: loss=3.772856885866187e+23, w0=-235910500151.70175, w1=-140843950589.24673\n",
      "SGD iter. 21/29: loss=2.62986725381352e+24, w0=366543087236.22815, w1=303911534839.9486\n",
      "SGD iter. 22/29: loss=1.7128640955656082e+24, w0=-900842089135.7418, w1=-1807565489651.7578\n",
      "SGD iter. 23/29: loss=4.314790455048369e+25, w0=3263217688324.233, w1=383212983314.08057\n",
      "SGD iter. 24/29: loss=1.8735387535872754e+27, w0=-9563312524890.139, w1=-7400125163089.285\n",
      "SGD iter. 25/29: loss=1.2496661320214267e+27, w0=7566078796292.434, w1=6888329538472.852\n",
      "SGD iter. 26/29: loss=7.746393776131993e+28, w0=-161838781553742.97, w1=-104458828065395.03\n",
      "SGD iter. 27/29: loss=4.6164927105965473e+30, w0=1135159617126685.5, w1=1367637673433932.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 28/29: loss=2.208206453765074e+32, w0=-5794745863734718.0, w1=-7299074189292730.0\n",
      "SGD iter. 29/29: loss=1.9109607167090057e+33, w0=4.867164710257492e+16, w1=1.109045081848479e+17\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=-0.505, w1=-0.41806566564191067\n",
      "SGD iter. 1/29: loss=10.865600617892262, w0=1.4592787270603247, w1=2.0382523266204164\n",
      "SGD iter. 2/29: loss=78.97603731769952, w0=-4.138097892955935, w1=-1.9021163410869613\n",
      "SGD iter. 3/29: loss=2355.509733396105, w0=37.58811837223623, w1=38.678856328414305\n",
      "SGD iter. 4/29: loss=366788.098548436, w0=-727.9680320843484, w1=-707.308738999805\n",
      "SGD iter. 5/29: loss=564243306.086983, w0=11349.020015919017, w1=9406.957596678363\n",
      "SGD iter. 6/29: loss=11768556190.879663, w0=-28972.91812845691, w1=-28650.836685245744\n",
      "SGD iter. 7/29: loss=28053732505.14175, w0=93640.46503642746, w1=118597.79149998986\n",
      "SGD iter. 8/29: loss=638648149742.0813, w0=-395196.6899040409, w1=-471457.8921902056\n",
      "SGD iter. 9/29: loss=19077908445122.69, w0=2565637.3657721085, w1=736276.9026283191\n",
      "SGD iter. 10/29: loss=1068692243214318.1, w0=-7832836.804052631, w1=-4179950.8777217483\n",
      "SGD iter. 11/29: loss=989186266976920.5, w0=14040592.272238214, w1=11730386.314996425\n",
      "SGD iter. 12/29: loss=1.2395340115061698e+16, w0=-52712002.63565454, w1=-83251747.90724654\n",
      "SGD iter. 13/29: loss=2.2314577884519254e+17, w0=427690485.39309895, w1=449665051.9322948\n",
      "SGD iter. 14/29: loss=9.93974188710527e+19, w0=-10728299025.02057, w1=-12159473944.78342\n",
      "SGD iter. 15/29: loss=9.518999942467956e+22, w0=423824310148.8891, w1=239053647712.25476\n",
      "SGD iter. 16/29: loss=9.179385673761805e+25, w0=-5585429264477.599, w1=-6769784359054.368\n",
      "SGD iter. 17/29: loss=6.004178200229753e+27, w0=55557378839225.24, w1=69908915280521.12\n",
      "SGD iter. 18/29: loss=9.023142159031425e+29, w0=-436796870453902.8, w1=-258035141682551.3\n",
      "SGD iter. 19/29: loss=1.119016447232472e+31, w0=2281423349770980.0, w1=1617392015717069.0\n",
      "SGD iter. 20/29: loss=1.666486267705989e+33, w0=-1.237762849108984e+16, w1=-1.5044139796954684e+16\n",
      "SGD iter. 21/29: loss=4.9413629063539004e+33, w0=5.093146160232941e+16, w1=9.136797378191731e+16\n",
      "SGD iter. 22/29: loss=7.194405266071503e+35, w0=-5.0398134511414816e+17, w1=-5.786373644103236e+17\n",
      "SGD iter. 23/29: loss=4.132509705976245e+37, w0=2.7957050478654807e+18, w1=6.306093755240591e+18\n",
      "SGD iter. 24/29: loss=6.646297499222204e+38, w0=-1.3522994176599777e+19, w1=-7.579339769483915e+18\n",
      "SGD iter. 25/29: loss=1.2356383093941847e+40, w0=5.3844745283542655e+19, w1=6.68231984408461e+19\n",
      "SGD iter. 26/29: loss=2.6691070627103023e+41, w0=-2.5628865119360175e+20, w1=-2.5647357363156468e+20\n",
      "SGD iter. 27/29: loss=7.22570821231502e+42, w0=1.632412866903592e+21, w1=2.2037388114982197e+21\n",
      "SGD iter. 28/29: loss=3.3431543884193076e+44, w0=-1.7913672917959448e+22, w1=-1.8262901893760119e+22\n",
      "SGD iter. 29/29: loss=1.1756735794193309e+47, w0=1.696022246749515e+23, w1=1.9974410007395517e+23\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=-0.505, w1=-0.22545947577788888\n",
      "SGD iter. 1/29: loss=34.863794709125294, w0=5.681948695875306, w1=7.117213746075395\n",
      "SGD iter. 2/29: loss=10024.708109021378, w0=-48.720139845526234, w1=-32.563736695537656\n",
      "SGD iter. 3/29: loss=209291.20441860444, w0=163.20455523886167, w1=92.37344525887211\n",
      "SGD iter. 4/29: loss=652035.9545274416, w0=-374.81116893119133, w1=-422.57448806779183\n",
      "SGD iter. 5/29: loss=18402669.425254676, w0=1243.050080518761, w1=1161.1657029322398\n",
      "SGD iter. 6/29: loss=40851998.226703376, w0=-2636.6631762551306, w1=-2256.633405710467\n",
      "SGD iter. 7/29: loss=285820701.1025794, w0=8177.478551494535, w1=3076.026202734969\n",
      "SGD iter. 8/29: loss=7847873418.492053, w0=-74966.24690456557, w1=-125051.8377994285\n",
      "SGD iter. 9/29: loss=1631936467585.0588, w0=285039.8977077344, w1=177999.38406022335\n",
      "SGD iter. 10/29: loss=1624402187084.1824, w0=-708748.6152884042, w1=-729171.3909247787\n",
      "SGD iter. 11/29: loss=390481532611180.1, w0=8707244.066029055, w1=10640399.703124033\n",
      "SGD iter. 12/29: loss=4155483760906893.0, w0=-36082310.78192613, w1=-23782583.073314495\n",
      "SGD iter. 13/29: loss=3.7163222691553146e+17, w0=82480886.34170684, w1=70847106.0114263\n",
      "SGD iter. 14/29: loss=1.1497566576956917e+17, w0=-196918890.35432234, w1=-132700273.43006513\n",
      "SGD iter. 15/29: loss=1.2369503365823708e+18, w0=427473641.7930075, w1=565684127.4332067\n",
      "SGD iter. 16/29: loss=6.646728083236102e+18, w0=-1292304627.904591, w1=-1124533811.9634333\n",
      "SGD iter. 17/29: loss=1.1303408090933474e+20, w0=5975679874.951202, w1=4585994661.295552\n",
      "SGD iter. 18/29: loss=3.7857131156765446e+21, w0=-22121421205.593628, w1=-18199581302.56538\n",
      "SGD iter. 19/29: loss=1.525751712062848e+22, w0=49133788967.24887, w1=32571210240.768917\n",
      "SGD iter. 20/29: loss=6.114684918288466e+22, w0=-93860427874.69608, w1=-36625588040.339386\n",
      "SGD iter. 21/29: loss=4.2094411045467113e+23, w0=230793673355.39755, w1=226110080265.50778\n",
      "SGD iter. 22/29: loss=2.8726043550661586e+24, w0=-1068756568282.5121, w1=-958720593506.3506\n",
      "SGD iter. 23/29: loss=3.779190234579603e+26, w0=16619192513067.195, w1=15916594440666.973\n",
      "SGD iter. 24/29: loss=1.1244024934591801e+29, w0=-307491956760389.0, w1=-296983233240607.3\n",
      "SGD iter. 25/29: loss=2.3748934413414807e+31, w0=1327475485059378.5, w1=490338229417921.7\n",
      "SGD iter. 26/29: loss=2.933093659039762e+31, w0=-2545513328616550.0, w1=-1986910431407907.2\n",
      "SGD iter. 27/29: loss=1.0117734023081639e+33, w0=2.598752402378428e+16, w1=2.4533087886088988e+16\n",
      "SGD iter. 28/29: loss=1.487840693894476e+35, w0=-2.5533421932544694e+17, w1=-1.4591881142224112e+17\n",
      "SGD iter. 29/29: loss=1.26381825141322e+37, w0=1.271956112299647e+18, w1=1.328571968509163e+18\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.505, w1=0.5279139784240141\n",
      "SGD iter. 1/29: loss=19.66594676054617, w0=-2.055531337914767, w1=-2.170265611346849\n",
      "SGD iter. 2/29: loss=358.4677262006885, w0=11.160829364599545, w1=21.490389271098447\n",
      "SGD iter. 3/29: loss=25612.338314318244, w0=-58.28187830299458, w1=-27.728802394773265\n",
      "SGD iter. 4/29: loss=93234.62383650642, w0=194.46548019489063, w1=151.28192029748018\n",
      "SGD iter. 5/29: loss=15960833.458240146, w0=-1550.4989479215403, w1=-1184.2652652717184\n",
      "SGD iter. 6/29: loss=96529118.67437775, w0=5392.189680296457, w1=9003.288110960539\n",
      "SGD iter. 7/29: loss=3808213206.9261637, w0=-51559.8874859824, w1=-12233.095595175977\n",
      "SGD iter. 8/29: loss=1861415124864.3904, w0=751237.2332953138, w1=924887.5174744021\n",
      "SGD iter. 9/29: loss=104097718188770.06, w0=-1561016.39486875, w1=-543522.8868349626\n",
      "SGD iter. 10/29: loss=30953418103671.773, w0=-3385803.226401927, w1=584837.9522447161\n",
      "SGD iter. 11/29: loss=1598014868541753.0, w0=15725249.246221988, w1=28188917.406104002\n",
      "SGD iter. 12/29: loss=1.9811260027047344e+16, w0=-49964364.80928178, w1=-36055039.5158325\n",
      "SGD iter. 13/29: loss=7.27975395895122e+16, w0=123897582.66791363, w1=145486943.1061533\n",
      "SGD iter. 14/29: loss=7.604628839005143e+17, w0=-482566909.0397689, w1=-336532994.4072327\n",
      "SGD iter. 15/29: loss=2.510861973546895e+19, w0=1426796416.7275903, w1=1086526590.1894605\n",
      "SGD iter. 16/29: loss=3.709128908753256e+19, w0=-3356876795.3424864, w1=-4754195981.216093\n",
      "SGD iter. 17/29: loss=7.482122855564695e+20, w0=14967774616.019146, w1=5887834998.4087925\n",
      "SGD iter. 18/29: loss=2.953829313650753e+22, w0=-62592420932.938385, w1=-96950921034.39464\n",
      "SGD iter. 19/29: loss=1.7002882213911133e+23, w0=240507930859.89816, w1=198794800533.23206\n",
      "SGD iter. 20/29: loss=2.1504246977177703e+25, w0=-585980690360.5969, w1=-780033980681.1229\n",
      "SGD iter. 21/29: loss=7.770359922727674e+24, w0=771309656254.9795, w1=1259417718245.4614\n",
      "SGD iter. 22/29: loss=4.458972269566843e+25, w0=-2878819378686.9404, w1=-2204132185729.732\n",
      "SGD iter. 23/29: loss=5.455894251951682e+26, w0=30740542365510.934, w1=20726811956132.39\n",
      "SGD iter. 24/29: loss=9.88072955001727e+29, w0=-253858790211102.44, w1=-1176119197424113.2\n",
      "SGD iter. 25/29: loss=4.5379596772922907e+30, w0=1011789095749286.8, w1=-88411575401466.62\n",
      "SGD iter. 26/29: loss=1.4246202816654792e+32, w0=-9877239952427598.0, w1=-1.1441618841438322e+16\n",
      "SGD iter. 27/29: loss=2.824639775858815e+34, w0=3.800789073522594e+16, w1=2.612564991430932e+16\n",
      "SGD iter. 28/29: loss=1.826470405045669e+34, w0=-7.330549373311277e+16, w1=-4.922475227196644e+16\n",
      "SGD iter. 29/29: loss=1.615093472178512e+35, w0=2.3873801753704512e+17, w1=2.980194567042054e+17\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/29: loss=0.5, w0=-0.555, w1=-0.5447388776867568\n",
      "SGD iter. 1/29: loss=4.712242949429221, w0=1.2643876906729208, w1=1.0299067828208772\n",
      "SGD iter. 2/29: loss=115.04012194899651, w0=-6.5018773323902765, w1=-8.878565351336416\n",
      "SGD iter. 3/29: loss=2456.9597987050975, w0=25.183372352678607, w1=16.315876128752528\n",
      "SGD iter. 4/29: loss=18758.175388725787, w0=-69.26739364256078, w1=-29.982679629975202\n",
      "SGD iter. 5/29: loss=263647.2010405099, w0=300.5053755865542, w1=320.7630013376876\n",
      "SGD iter. 6/29: loss=13250591.266599344, w0=-819.9151799810509, w1=174.05464757707236\n",
      "SGD iter. 7/29: loss=7961242.57958584, w0=771.0708774508191, w1=1745.8165887183284\n",
      "SGD iter. 8/29: loss=277726884.4279692, w0=-5678.034062385054, w1=-3226.636875535296\n",
      "SGD iter. 9/29: loss=1953578868.7187226, w0=20020.158033620144, w1=629.569128902162\n",
      "SGD iter. 10/29: loss=18387166985.082485, w0=-109198.76114467693, w1=-135995.92958858504\n",
      "SGD iter. 11/29: loss=3136036607972.252, w0=573631.1635810141, w1=507973.223053675\n",
      "SGD iter. 12/29: loss=11050902867436.639, w0=-2026623.816358245, w1=-2842941.0551341088\n",
      "SGD iter. 13/29: loss=278871641162404.9, w0=12228528.275675096, w1=8784955.868575476\n",
      "SGD iter. 14/29: loss=6.769186836460172e+16, w0=-236600513.1944171, w1=-111336312.46021074\n",
      "SGD iter. 15/29: loss=1.1475008444036246e+19, w0=2019831415.1102238, w1=2594066933.88464\n",
      "SGD iter. 16/29: loss=7.095078227255519e+20, w0=-14481421772.666258, w1=-24073255821.57213\n",
      "SGD iter. 17/29: loss=1.84056283689341e+22, w0=195943281192.49207, w1=202934752354.93372\n",
      "SGD iter. 18/29: loss=3.867166084234157e+25, w0=-3541790802500.0186, w1=-2937898328854.2197\n",
      "SGD iter. 19/29: loss=1.3171493448227323e+27, w0=84791863807249.66, w1=128448609594590.73\n",
      "SGD iter. 20/29: loss=1.3108126126824712e+31, w0=-1333084882652387.8, w1=-1106586153001742.0\n",
      "SGD iter. 21/29: loss=5.209930952354419e+31, w0=5973633065744952.0, w1=7203482245769579.0\n",
      "SGD iter. 22/29: loss=2.226481289452157e+34, w0=-6.0101752769355224e+16, w1=-6.080781077897715e+16\n",
      "SGD iter. 23/29: loss=3.438046634389871e+35, w0=4.2108500838000166e+17, w1=3.243164667422139e+17\n",
      "SGD iter. 24/29: loss=2.7916913837816374e+37, w0=-3.03389771182016e+18, w1=-5.344266005210673e+18\n",
      "SGD iter. 25/29: loss=8.963187911060422e+38, w0=2.4123545111860826e+19, w1=2.228118110416158e+19\n",
      "SGD iter. 26/29: loss=1.3003987507260857e+41, w0=-1.5370576024503344e+20, w1=-2.7079437889906835e+20\n",
      "SGD iter. 27/29: loss=1.4073907083893458e+42, w0=8.907284155910499e+20, w1=8.418491642308885e+20\n",
      "SGD iter. 28/29: loss=9.989429487976947e+43, w0=-5.274790447734319e+21, w1=-2.1663953239617841e+21\n",
      "SGD iter. 29/29: loss=2.7708614273226136e+45, w0=2.8874241621463556e+22, w1=3.4297095162843976e+22\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.555, w1=0.578917613285589\n",
      "SGD iter. 1/29: loss=21.19098177397594, w0=-2.7575175811118013, w1=-2.002124416601662\n",
      "SGD iter. 2/29: loss=821.9392270768507, w0=16.585654508140447, w1=14.652225011983107\n",
      "SGD iter. 3/29: loss=25271.701475045375, w0=-130.8522463712019, w1=-408.28991477158246\n",
      "SGD iter. 4/29: loss=3260422.8879611557, w0=1225.883886008481, w1=1053.9606697610027\n",
      "SGD iter. 5/29: loss=164043458.48086685, w0=-8547.532658422895, w1=-5780.332260717967\n",
      "SGD iter. 6/29: loss=16678875841.505945, w0=132339.37231450004, w1=1926098.592160271\n",
      "SGD iter. 7/29: loss=21208710906399.32, w0=-2252830.467085545, w1=295136.19977842085\n",
      "SGD iter. 8/29: loss=227525557550530.4, w0=7865304.8619153155, w1=7432781.914559259\n",
      "SGD iter. 9/29: loss=6198728566470369.0, w0=-43577169.567670204, w1=-17677997.62629928\n",
      "SGD iter. 10/29: loss=2.8111352775711533e+17, w0=285637735.37900835, w1=563327860.4424607\n",
      "SGD iter. 11/29: loss=6.491096955806393e+18, w0=-1487050778.3519561, w1=-744263937.5031787\n",
      "SGD iter. 12/29: loss=2.526687937223478e+20, w0=12566133925.79575, w1=10040859191.913696\n",
      "SGD iter. 13/29: loss=2.9854093121212293e+22, w0=-68468835665.247925, w1=-57165485585.32373\n",
      "SGD iter. 14/29: loss=3.294475483234974e+23, w0=428007181699.84814, w1=396149051703.64825\n",
      "SGD iter. 15/29: loss=3.5291601407779223e+25, w0=-2050851275061.4058, w1=-1552623423642.8206\n",
      "SGD iter. 16/29: loss=1.2657896203057747e+26, w0=5681399834175.34, w1=5780028596933.592\n",
      "SGD iter. 17/29: loss=3.011019396067235e+27, w0=-19363784805566.8, w1=-16197898491288.818\n",
      "SGD iter. 18/29: loss=1.130013713397099e+28, w0=42650839305045.64, w1=14669118922721.041\n",
      "SGD iter. 19/29: loss=8.755469429811887e+28, w0=-165858206169256.2, w1=-239525108241401.25\n",
      "SGD iter. 20/29: loss=2.0617910309624823e+30, w0=555341806954981.8, w1=393350294055203.4\n",
      "SGD iter. 21/29: loss=5.861881553443392e+30, w0=-1591886152836828.0, w1=-4033317717570290.5\n",
      "SGD iter. 22/29: loss=2.0197286810754833e+32, w0=9379377497266402.0, w1=4189077604514521.5\n",
      "SGD iter. 23/29: loss=1.6679436797052521e+34, w0=-4.888947763339911e+16, w1=-7.057104412286649e+16\n",
      "SGD iter. 24/29: loss=1.5975817976273716e+35, w0=2.3653829624220054e+17, w1=1.852200226546497e+17\n",
      "SGD iter. 25/29: loss=6.491701262508059e+36, w0=-1.905980652444647e+18, w1=-1.6949068726629522e+18\n",
      "SGD iter. 26/29: loss=7.143886301870923e+38, w0=8.901169759309703e+18, w1=1.0531750912088613e+19\n",
      "SGD iter. 27/29: loss=2.5925179161464925e+39, w0=-4.021627227574528e+19, w1=-3.486933277837961e+19\n",
      "SGD iter. 28/29: loss=1.1276925242947595e+42, w0=8.141342641480814e+20, w1=9.140445876636765e+20\n",
      "SGD iter. 29/29: loss=9.183890774596232e+43, w0=-3.1987680901405344e+21, w1=-6.796477296025061e+20\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.0, w1=0.13154620382655427\n",
      "SGD iter. 1/29: loss=10.718751111348636, w0=-0.34100746624527295, w1=-0.1698409350710449\n",
      "SGD iter. 2/29: loss=21.380451701014533, w0=0.10096719849330282, w1=0.16787432493886342\n",
      "SGD iter. 3/29: loss=10.800790092009962, w0=2.8400481114347857, w1=3.3360379918353993\n",
      "SGD iter. 4/29: loss=1488.6540797392322, w0=-11.855679833973502, w1=-3.6170355122455313\n",
      "SGD iter. 5/29: loss=2211.5043569601903, w0=23.4649907215518, w1=24.564919930836616\n",
      "SGD iter. 6/29: loss=38453.12726198645, w0=-111.10825376873365, w1=-67.38966990244481\n",
      "SGD iter. 7/29: loss=1854106.6267522315, w0=1050.4019308082247, w1=777.3330022495016\n",
      "SGD iter. 8/29: loss=302418470.8816453, w0=-11610.067679402533, w1=-4341.695278052065\n",
      "SGD iter. 9/29: loss=31527334420.95116, w0=78212.34678290407, w1=105717.52492513125\n",
      "SGD iter. 10/29: loss=385895881682.4498, w0=-398037.764627749, w1=-394153.50622562814\n",
      "SGD iter. 11/29: loss=18507390818710.53, w0=2406778.559316141, w1=2229527.5225104447\n",
      "SGD iter. 12/29: loss=567587090839132.5, w0=-11809808.465736276, w1=-16361026.268806105\n",
      "SGD iter. 13/29: loss=9354351229594034.0, w0=108357228.91363123, w1=120109093.1959171\n",
      "SGD iter. 14/29: loss=1.9259000552203985e+19, w0=-3255741138.791827, w1=-4226339786.4776783\n",
      "SGD iter. 15/29: loss=1.4644561617223614e+21, w0=30386866445.76391, w1=24162947619.303185\n",
      "SGD iter. 16/29: loss=1.7680797407576504e+23, w0=-190694332771.4192, w1=-293085187695.0389\n",
      "SGD iter. 17/29: loss=3.2873778569471984e+24, w0=770981196020.0022, w1=383592139917.3565\n",
      "SGD iter. 18/29: loss=1.787243639712753e+25, w0=-1983890937119.5125, w1=-2736396772753.311\n",
      "SGD iter. 19/29: loss=3.2232551905912775e+26, w0=13519933185626.246, w1=9620103210874.158\n",
      "SGD iter. 20/29: loss=3.2738918238736634e+28, w0=-63149469170708.67, w1=-65746453054127.84\n",
      "SGD iter. 21/29: loss=1.3745775322492529e+29, w0=188883108111500.0, w1=116864412698793.56\n",
      "SGD iter. 22/29: loss=2.754438889615779e+30, w0=-968035234546977.5, w1=-1017490489065265.0\n",
      "SGD iter. 23/29: loss=1.5236352186134312e+32, w0=2745350172487800.5, w1=2437101439608374.5\n",
      "SGD iter. 24/29: loss=1.5323023909900935e+32, w0=-4051691501117835.5, w1=-3225466005951857.5\n",
      "SGD iter. 25/29: loss=2.7062394278129643e+33, w0=7.277434523236915e+16, w1=7.517843458746584e+16\n",
      "SGD iter. 26/29: loss=2.666330318113696e+36, w0=-1.12777631148348e+18, w1=-6.304475754268704e+17\n",
      "SGD iter. 27/29: loss=3.045196381270942e+38, w0=1.484627845780068e+19, w1=1.5448094193846708e+19\n",
      "SGD iter. 28/29: loss=4.426180976006011e+40, w0=-1.100740679571091e+20, w1=-2.2930010818881297e+20\n",
      "SGD iter. 29/29: loss=8.921705828366269e+41, w0=3.6347347137228105e+20, w1=1.8625255959521768e+20\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.0, w1=0.09178667637444413\n",
      "SGD iter. 1/29: loss=1.6443660249366916, w0=-1.4559333783832582, w1=-0.6322932575700823\n",
      "SGD iter. 2/29: loss=2665.722166231567, w0=51.90734610230702, w1=54.00958259648944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 3/29: loss=892376.5265871701, w0=-231.1000640241324, w1=-277.4431850083417\n",
      "SGD iter. 4/29: loss=927832.2561421898, w0=230.20401873866393, w1=77.88079352124959\n",
      "SGD iter. 5/29: loss=6999888.932339399, w0=-567.2271957749524, w1=-626.2455671251828\n",
      "SGD iter. 6/29: loss=3985061.1316548106, w0=633.5489278895975, w1=0.12098119653262529\n",
      "SGD iter. 7/29: loss=92969368.82345633, w0=-4348.381311178092, w1=-5190.567053915597\n",
      "SGD iter. 8/29: loss=1390079727.2171853, w0=33833.730145201036, w1=24170.586212778795\n",
      "SGD iter. 9/29: loss=426858361527.2039, w0=-292130.01621037174, w1=-1011344.6523031165\n",
      "SGD iter. 10/29: loss=9950934621696.166, w0=1842158.7581584891, w1=837685.5880474205\n",
      "SGD iter. 11/29: loss=278346849696757.25, w0=-10612594.053000249, w1=-7643381.387124781\n",
      "SGD iter. 12/29: loss=2.3506035421973508e+16, w0=81640425.39930856, w1=61785087.80350855\n",
      "SGD iter. 13/29: loss=7.846679471580292e+17, w0=-851239005.6938616, w1=-780778237.8765477\n",
      "SGD iter. 14/29: loss=2.046760838814763e+20, w0=6733658535.739566, w1=9639971908.509598\n",
      "SGD iter. 15/29: loss=4.849912805004952e+21, w0=-30629654473.655666, w1=-20676364621.25036\n",
      "SGD iter. 16/29: loss=3.5263196041256985e+22, w0=95832152214.49516, w1=64416181881.67364\n",
      "SGD iter. 17/29: loss=2.3433892086254024e+24, w0=-1228877311820.053, w1=-1135209687475.2568\n",
      "SGD iter. 18/29: loss=3.121966630429675e+26, w0=14622313470052.35, w1=16871289018853.309\n",
      "SGD iter. 19/29: loss=5.486498254406461e+28, w0=-92290544373305.06, w1=-93448562552831.84\n",
      "SGD iter. 20/29: loss=3.397759649407912e+29, w0=328926996602290.06, w1=269757884094489.47\n",
      "SGD iter. 21/29: loss=8.386828619270938e+30, w0=-781735574383570.0, w1=-704883004198686.5\n",
      "SGD iter. 22/29: loss=8.512473769106051e+30, w0=1715969562615409.5, w1=547341624979273.0\n",
      "SGD iter. 23/29: loss=8.061274308141055e+31, w0=-7880499341086064.0, w1=-3325500965903449.5\n",
      "SGD iter. 24/29: loss=4.883729054412033e+34, w0=7.541097773030675e+16, w1=7.351824177544627e+16\n",
      "SGD iter. 25/29: loss=2.2960953033843754e+35, w0=-3.5511835647967155e+17, w1=-1.8834252210549e+17\n",
      "SGD iter. 26/29: loss=4.833209351323045e+37, w0=3.3337452148712223e+18, w1=5.394131503764143e+18\n",
      "SGD iter. 27/29: loss=7.44656608608315e+38, w0=-1.3645268444918055e+19, w1=-1.1978197309280449e+19\n",
      "SGD iter. 28/29: loss=8.308707471199715e+39, w0=6.202257729677713e+19, w1=7.974063330519661e+19\n",
      "SGD iter. 29/29: loss=5.994163993858874e+41, w0=-4.505242442247111e+20, w1=-4.944171498779662e+20\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=-0.6050000000000001, w1=-0.24415913840892628\n",
      "SGD iter. 1/29: loss=16.78358809957717, w0=2.8405827828761114, w1=1.7114454712473324\n",
      "SGD iter. 2/29: loss=2357.987452108916, w0=-54.20979775213895, w1=-84.94325605205724\n",
      "SGD iter. 3/29: loss=812446.4779410764, w0=370.1595005258961, w1=304.69888453010077\n",
      "SGD iter. 4/29: loss=5024465.180160032, w0=-1633.7007691272006, w1=-1355.9099465603822\n",
      "SGD iter. 5/29: loss=827673288.9212276, w0=6629.10035465006, w1=7047.114963736297\n",
      "SGD iter. 6/29: loss=811170840.9035054, w0=-23552.609438501582, w1=-42664.887270784086\n",
      "SGD iter. 7/29: loss=23082590654.38846, w0=121853.49699650565, w1=254835.74513573587\n",
      "SGD iter. 8/29: loss=888774901659.9691, w0=-588033.9342362308, w1=-402226.293472397\n",
      "SGD iter. 9/29: loss=35879151428858.31, w0=2273539.433191912, w1=2339549.15550706\n",
      "SGD iter. 10/29: loss=149979971316202.5, w0=-6715849.032724081, w1=-8100974.320735741\n",
      "SGD iter. 11/29: loss=1613139530258198.2, w0=27378545.106581934, w1=23451902.141542107\n",
      "SGD iter. 12/29: loss=1.206146299562049e+17, w0=-144619918.97242466, w1=-174056397.44608498\n",
      "SGD iter. 13/29: loss=1.0670936790877326e+18, w0=824235585.2384211, w1=582938746.2138132\n",
      "SGD iter. 14/29: loss=2.5418415722388573e+20, w0=-6774655609.135448, w1=-8243668174.764609\n",
      "SGD iter. 15/29: loss=1.8597711501590248e+21, w0=31901902794.882267, w1=26196118787.850937\n",
      "SGD iter. 16/29: loss=2.2091010988751903e+23, w0=-137486022139.2453, w1=-115934957762.8261\n",
      "SGD iter. 17/29: loss=4.0735278377264406e+23, w0=422360938334.9736, w1=409408782827.34973\n",
      "SGD iter. 18/29: loss=8.027466144360851e+24, w0=-2661158970569.3076, w1=-4467356975642.743\n",
      "SGD iter. 19/29: loss=2.1735383381984004e+27, w0=37408578238972.79, w1=44579960688456.55\n",
      "SGD iter. 20/29: loss=1.9940652873879248e+29, w0=-215914093613351.38, w1=-252284685083751.5\n",
      "SGD iter. 21/29: loss=1.9022263554150487e+30, w0=657218044084613.4, w1=366825329822139.1\n",
      "SGD iter. 22/29: loss=1.1371549027073356e+31, w0=-1974072908168195.0, w1=-2216283617084459.0\n",
      "SGD iter. 23/29: loss=3.5344169739254524e+32, w0=7767369241218959.0, w1=5623714495042460.0\n",
      "SGD iter. 24/29: loss=1.9495387817827004e+33, w0=-2.5524007078753052e+16, w1=-2.658502366345319e+16\n",
      "SGD iter. 25/29: loss=7.878569379572392e+34, w0=1.0185104039569752e+17, w1=5.2490505870203784e+16\n",
      "SGD iter. 26/29: loss=2.9131687995278316e+35, w0=-3.0873032783394394e+17, w1=-2.884010697272744e+17\n",
      "SGD iter. 27/29: loss=4.2089505317296577e+36, w0=1.392156590212333e+18, w1=3.284355439094192e+18\n",
      "SGD iter. 28/29: loss=1.5996540473203294e+38, w0=-6.824654011280677e+18, w1=-2.365663935398782e+18\n",
      "SGD iter. 29/29: loss=2.559950914240019e+39, w0=4.257356257983239e+19, w1=2.8374477560120144e+19\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.0, w1=-0.531811618799372\n",
      "SGD iter. 1/29: loss=2.563102988426317, w0=0.17321260904880773, w1=0.6075898135620943\n",
      "SGD iter. 2/29: loss=9.778166215140814, w0=-2.9247328124885095, w1=-0.823141963334928\n",
      "SGD iter. 3/29: loss=1432.2299655993907, w0=55.15786284438215, w1=57.2856887985014\n",
      "SGD iter. 4/29: loss=2005753.3516777644, w0=-1776.457013533596, w1=-1244.2270335543685\n",
      "SGD iter. 5/29: loss=1283642494.182081, w0=9765.558454725251, w1=17045.91133719466\n",
      "SGD iter. 6/29: loss=2278630178.37544, w0=-32793.78529961652, w1=-37918.03173244497\n",
      "SGD iter. 7/29: loss=84001230267.72244, w0=179102.01534548512, w1=219516.7762116573\n",
      "SGD iter. 8/29: loss=2680580481585.237, w0=-1043154.8524692114, w1=-689286.0252723132\n",
      "SGD iter. 9/29: loss=96217806328668.64, w0=5130819.894466786, w1=3965322.0261098477\n",
      "SGD iter. 10/29: loss=1335732416228132.2, w0=-33984203.75587822, w1=-73422531.1001679\n",
      "SGD iter. 11/29: loss=2.8950716067843334e+17, w0=272877378.9299017, w1=233509080.67516038\n",
      "SGD iter. 12/29: loss=3.4238761281912627e+18, w0=-937291336.0461961, w1=-970046482.9174759\n",
      "SGD iter. 13/29: loss=3.039538605424349e+19, w0=2892434953.4992123, w1=1784569346.1127717\n",
      "SGD iter. 14/29: loss=3.655644697265834e+20, w0=-26470146547.884895, w1=-29982751097.212532\n",
      "SGD iter. 15/29: loss=5.203641215523415e+23, w0=772856815807.46, w1=1344325045174.747\n",
      "SGD iter. 16/29: loss=1.394215718220741e+26, w0=-6041586406468.301, w1=-5553798173328.038\n",
      "SGD iter. 17/29: loss=1.799284139553377e+27, w0=29636624697485.402, w1=18787353305772.992\n",
      "SGD iter. 18/29: loss=1.2400705941656634e+29, w0=-103378564543675.6, w1=-97269031600907.34\n",
      "SGD iter. 19/29: loss=1.676625328563246e+29, w0=183782403802618.72, w1=182471021244116.8\n",
      "SGD iter. 20/29: loss=9.419433124912037e+30, w0=-2490454278599036.5, w1=-2094488571379537.8\n",
      "SGD iter. 21/29: loss=8.231220849729963e+32, w0=1.873174879283798e+16, w1=1.8383450347651776e+16\n",
      "SGD iter. 22/29: loss=6.678606534468322e+34, w0=-1.6752381791011222e+17, w1=-1.4048516208738928e+17\n",
      "SGD iter. 23/29: loss=2.417598373483329e+36, w0=1.9593329396199268e+18, w1=1.4545799969209677e+18\n",
      "SGD iter. 24/29: loss=2.5345847698323455e+39, w0=-2.6168679013202457e+19, w1=-2.1704057869569794e+19\n",
      "SGD iter. 25/29: loss=4.0625306627675603e+40, w0=1.1119524641844987e+20, w1=8.417483914840562e+19\n",
      "SGD iter. 26/29: loss=8.83057490484711e+41, w0=-5.3181785978617e+20, w1=-3.720774634871879e+20\n",
      "SGD iter. 27/29: loss=2.753327703512951e+43, w0=2.3541709271266817e+21, w1=1.9967856456464623e+21\n",
      "SGD iter. 28/29: loss=2.4001231478054995e+44, w0=-8.969873287009597e+21, w1=-1.3834522958557465e+22\n",
      "SGD iter. 29/29: loss=4.501340215926959e+45, w0=4.001207433329268e+22, w1=3.1694202753602208e+22\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.0, w1=0.23031261477243364\n",
      "SGD iter. 1/29: loss=118.39098216101387, w0=8.398867386370766, w1=10.104181959288914\n",
      "SGD iter. 2/29: loss=8051.921982623587, w0=-51.51991932682285, w1=-103.21293482409425\n",
      "SGD iter. 3/29: loss=174142.45681993113, w0=328.5386984382704, w1=130.79868895364714\n",
      "SGD iter. 4/29: loss=24766274.77930022, w0=-2420.044337491063, w1=-2374.327262255147\n",
      "SGD iter. 5/29: loss=442888938.39992523, w0=17961.45780357412, w1=22450.81667823452\n",
      "SGD iter. 6/29: loss=67296097689.80386, w0=-60014.386202693415, w1=-40725.512132600146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 7/29: loss=41733366033.22777, w0=76768.02274990949, w1=39131.687200252265\n",
      "SGD iter. 8/29: loss=429493963068.74634, w0=-650678.114255945, w1=-946642.2231102774\n",
      "SGD iter. 9/29: loss=151075685191419.97, w0=4789653.527356777, w1=5745732.284526806\n",
      "SGD iter. 10/29: loss=878301177029114.5, w0=-21307123.251926214, w1=-18306233.81465441\n",
      "SGD iter. 11/29: loss=8.943631430958592e+16, w0=182265658.8242559, w1=132491309.47932142\n",
      "SGD iter. 12/29: loss=4.5557564385303434e+18, w0=-1590275459.1133554, w1=-1422968552.8165936\n",
      "SGD iter. 13/29: loss=5.219113072176791e+20, w0=8304603349.370247, w1=5760964914.162857\n",
      "SGD iter. 14/29: loss=1.587553923560114e+21, w0=-21733928737.54892, w1=-22347519095.968666\n",
      "SGD iter. 15/29: loss=7.743018357518912e+22, w0=137363047782.01712, w1=136195203390.71411\n",
      "SGD iter. 16/29: loss=9.745704591980154e+23, w0=-1263603342317.4429, w1=-3870617788993.244\n",
      "SGD iter. 17/29: loss=9.753391919642948e+26, w0=20084454584647.547, w1=17873150997362.64\n",
      "SGD iter. 18/29: loss=3.7333500594696544e+28, w0=-185931371198413.75, w1=-111755417537136.16\n",
      "SGD iter. 19/29: loss=7.71274157932591e+30, w0=1424730562977763.2, w1=1359583457081653.0\n",
      "SGD iter. 20/29: loss=1.1494162376143227e+32, w0=-1.1258316591963144e+16, w1=-5034572632359338.0\n",
      "SGD iter. 21/29: loss=4.67554698018699e+34, w0=1.0461946065557642e+17, w1=8.005725659159763e+16\n",
      "SGD iter. 22/29: loss=6.247610749406098e+35, w0=-3.937296656777482e+17, w1=-4.3111996370438483e+17\n",
      "SGD iter. 23/29: loss=7.08798378842985e+36, w0=2.049270108017384e+18, w1=1.8297736808941253e+18\n",
      "SGD iter. 24/29: loss=8.194856958345694e+38, w0=-1.0829759324333402e+19, w1=-4.906469384927576e+18\n",
      "SGD iter. 25/29: loss=3.325318077546456e+39, w0=3.067277438283668e+19, w1=2.203808524914657e+19\n",
      "SGD iter. 26/29: loss=4.262211884252864e+40, w0=-2.301384355406316e+20, w1=-5.130761179797014e+20\n",
      "SGD iter. 27/29: loss=2.7353346723297353e+43, w0=2.291973210166556e+21, w1=3.429185659369115e+21\n",
      "SGD iter. 28/29: loss=3.8023247304191456e+44, w0=-1.3544580754820842e+22, w1=-8.508573114070334e+21\n",
      "SGD iter. 29/29: loss=2.764788077311297e+46, w0=7.267176296815662e+22, w1=9.523400207169575e+22\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=-0.6050000000000001, w1=-0.4232736604170365\n",
      "SGD iter. 1/29: loss=11.829470021031684, w0=1.6756750540796057, w1=2.9665081750905844\n",
      "SGD iter. 2/29: loss=306.05246682019504, w0=-9.983036655060317, w1=-6.065587389579887\n",
      "SGD iter. 3/29: loss=6895.955293955571, w0=45.792795466842925, w1=40.71209686069854\n",
      "SGD iter. 4/29: loss=170537.71266433975, w0=-468.79738565863727, w1=-350.0233246035714\n",
      "SGD iter. 5/29: loss=99472451.0306089, w0=3952.1160118570638, w1=3872.212326561371\n",
      "SGD iter. 6/29: loss=1004714963.1949446, w0=-17613.606006909526, w1=-8796.10681368351\n",
      "SGD iter. 7/29: loss=22060641940.719418, w0=79354.31855414825, w1=117231.71127820502\n",
      "SGD iter. 8/29: loss=473474376019.29645, w0=-787139.1203468966, w1=-1585800.156576835\n",
      "SGD iter. 9/29: loss=241666450562852.6, w0=12529060.997332348, w1=9637839.924899591\n",
      "SGD iter. 10/29: loss=2.226473111179214e+16, w0=-292156429.0244158, w1=-329593057.4912788\n",
      "SGD iter. 11/29: loss=1.637888095725693e+20, w0=3380087020.8105516, w1=2963574792.2470684\n",
      "SGD iter. 12/29: loss=2.0056289863585523e+20, w0=-6920567475.277524, w1=-7957500873.256577\n",
      "SGD iter. 13/29: loss=3.0166245835159783e+22, w0=108122380182.20746, w1=139740042992.76776\n",
      "SGD iter. 14/29: loss=1.0685418723518871e+24, w0=-607157247799.4553, w1=-437335287206.0097\n",
      "SGD iter. 15/29: loss=2.8343903675073247e+25, w0=3954390762070.037, w1=3565564711832.2866\n",
      "SGD iter. 16/29: loss=3.971179532803277e+27, w0=-70683123628446.05, w1=-67358870237060.71\n",
      "SGD iter. 17/29: loss=1.9536560179430512e+30, w0=476844806282888.56, w1=349739860169319.25\n",
      "SGD iter. 18/29: loss=6.0345305865882e+30, w0=-1705971022339700.0, w1=-1937948729510237.2\n",
      "SGD iter. 19/29: loss=4.1107707370920385e+32, w0=1.1458626611905402e+16, w1=9694606707764510.0\n",
      "SGD iter. 20/29: loss=1.208825094797647e+34, w0=-7.273347082377554e+16, w1=-6.418657266712086e+16\n",
      "SGD iter. 21/29: loss=6.80671296867455e+35, w0=7.550367037470694e+17, w1=4.529120133142936e+17\n",
      "SGD iter. 22/29: loss=1.0840939395182437e+38, w0=-7.303043934509615e+18, w1=-1.698814196506747e+19\n",
      "SGD iter. 23/29: loss=8.878190346661651e+39, w0=4.706143325982565e+19, w1=1.7037105840024275e+19\n",
      "SGD iter. 24/29: loss=1.2328221485938455e+41, w0=-2.5084716033812126e+20, w1=-2.1017375607860452e+20\n",
      "SGD iter. 25/29: loss=1.120550493062961e+43, w0=2.363880212774092e+21, w1=2.5295335468737747e+21\n",
      "SGD iter. 26/29: loss=7.837857588205456e+44, w0=-9.676480616911442e+21, w1=-1.9663821606262176e+21\n",
      "SGD iter. 27/29: loss=1.7170666491156426e+45, w0=2.5031756540079573e+22, w1=7.2638703602277655e+22\n",
      "SGD iter. 28/29: loss=7.449093530312979e+46, w0=-3.2489479626935426e+23, w1=-6.59957970282621e+23\n",
      "SGD iter. 29/29: loss=2.569622957562087e+49, w0=1.5407312351714005e+24, w1=9.771867935787324e+23\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.0, w1=0.057941875217889884\n",
      "SGD iter. 1/29: loss=2.807113657226332, w0=2.1073510417766768, w1=1.9866723575049476\n",
      "SGD iter. 2/29: loss=1342.131138262384, w0=-11.990293127590103, w1=-10.613294831175327\n",
      "SGD iter. 3/29: loss=5009.928402485969, w0=38.79802707740555, w1=15.736857731600995\n",
      "SGD iter. 4/29: loss=66351.07582571582, w0=-129.40051881570062, w1=-124.29435454656729\n",
      "SGD iter. 5/29: loss=1070097.4356754946, w0=1297.6497732119121, w1=430.35926345317716\n",
      "SGD iter. 6/29: loss=1116902979.3838809, w0=-44312.73932184814, w1=-32387.606087178738\n",
      "SGD iter. 7/29: loss=1291135874247.1392, w0=435899.48061565903, w1=51799.481153991626\n",
      "SGD iter. 8/29: loss=4362348672194.5005, w0=-1293067.2518868665, w1=-1650369.2047925738\n",
      "SGD iter. 9/29: loss=129383731881328.1, w0=5625024.959322477, w1=4136055.0380844586\n",
      "SGD iter. 10/29: loss=1375548545644486.5, w0=-18236247.965377152, w1=-13290771.049385369\n",
      "SGD iter. 11/29: loss=1.1554842729633338e+16, w0=116127538.49724564, w1=97026165.61191648\n",
      "SGD iter. 12/29: loss=5.825277576689613e+18, w0=-2403970449.8415203, w1=-1192585704.7410157\n",
      "SGD iter. 13/29: loss=1.1674902907251132e+21, w0=48861615415.3543, w1=38914770855.18344\n",
      "SGD iter. 14/29: loss=1.7311252462192867e+24, w0=-1936832729539.6033, w1=-1611132926010.3003\n",
      "SGD iter. 15/29: loss=1.2924405867911628e+27, w0=15800415476735.37, w1=12568557014039.611\n",
      "SGD iter. 16/29: loss=1.1147766839021507e+28, w0=-81816026340253.84, w1=-79290722161786.81\n",
      "SGD iter. 17/29: loss=6.068212417671374e+29, w0=474529615924158.25, w1=431821181677338.0\n",
      "SGD iter. 18/29: loss=1.5401292643102836e+31, w0=-3887710883305195.0, w1=-3006089195447955.0\n",
      "SGD iter. 19/29: loss=4.9959827883183616e+33, w0=2.340928646555382e+16, w1=1.3611207278028952e+16\n",
      "SGD iter. 20/29: loss=1.7420313414722052e+34, w0=-8.62149235262199e+16, w1=-1.0370458080100592e+17\n",
      "SGD iter. 21/29: loss=6.381853231148882e+35, w0=5.79095266990506e+17, w1=5.481030847668901e+17\n",
      "SGD iter. 22/29: loss=4.141149199480165e+37, w0=-6.243008135543773e+18, w1=-7.161001120780559e+18\n",
      "SGD iter. 23/29: loss=6.812737930088407e+39, w0=6.907854618149256e+19, w1=7.481089843927805e+19\n",
      "SGD iter. 24/29: loss=1.3593151802695172e+42, w0=-6.638029785324357e+20, w1=-7.489465407580134e+20\n",
      "SGD iter. 25/29: loss=2.687716845575964e+43, w0=3.9911586381083663e+21, w1=3.2076498219593147e+21\n",
      "SGD iter. 26/29: loss=2.2888557304323594e+45, w0=-2.1668125960990213e+22, w1=-1.7776088305313606e+22\n",
      "SGD iter. 27/29: loss=2.3691248515748786e+46, w0=1.4096692914612988e+23, w1=1.1904771880785334e+23\n",
      "SGD iter. 28/29: loss=6.475709520626703e+48, w0=-1.0377350440038967e+24, w1=-5.579572070596238e+23\n",
      "SGD iter. 29/29: loss=3.218002252483092e+49, w0=4.4058201276516576e+24, w1=2.7554580657332593e+24\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.0, w1=0.051129449779991815\n",
      "SGD iter. 1/29: loss=7.292063007748255, w0=-2.129854312842952, w1=-1.9361020593935878\n",
      "SGD iter. 2/29: loss=538.2959819484905, w0=22.840997420038384, w1=91.25896986844138\n",
      "SGD iter. 3/29: loss=206504.33169080762, w0=-139.94475314340204, w1=-49.52900944609523\n",
      "SGD iter. 4/29: loss=211479.07189365625, w0=281.71337342740867, w1=127.63319654978268\n",
      "SGD iter. 5/29: loss=6051031.222976753, w0=-1919.7462444724276, w1=-2284.0290531412497\n",
      "SGD iter. 6/29: loss=588455625.2003324, w0=11006.412371850824, w1=12331.122543038546\n",
      "SGD iter. 7/29: loss=5202619954.509058, w0=-54156.84691788157, w1=-41057.59568585645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 8/29: loss=273619656987.78635, w0=484826.32120521757, w1=438544.6343798863\n",
      "SGD iter. 9/29: loss=69861242755341.336, w0=-3067593.503944741, w1=-6303590.413844101\n",
      "SGD iter. 10/29: loss=459333857148009.5, w0=13270303.179384815, w1=7575663.363561576\n",
      "SGD iter. 11/29: loss=1.1761803524667182e+16, w0=-67549652.07370217, w1=-40453564.70243476\n",
      "SGD iter. 12/29: loss=4.669621067826919e+17, w0=1493889453.604112, w1=1224304439.7264543\n",
      "SGD iter. 13/29: loss=2.1737859541509344e+21, w0=-21011474471.39866, w1=-18118167522.99575\n",
      "SGD iter. 14/29: loss=2.4046709289259975e+22, w0=138705848511.42218, w1=98217244588.42001\n",
      "SGD iter. 15/29: loss=4.111616931083131e+24, w0=-1514214225091.6301, w1=-864079611065.342\n",
      "SGD iter. 16/29: loss=5.081212977978398e+26, w0=10782612339314.053, w1=8609033253131.775\n",
      "SGD iter. 17/29: loss=5.874534308626538e+27, w0=-53273294117410.07, w1=-56893669676213.125\n",
      "SGD iter. 18/29: loss=3.699203543751386e+29, w0=283809360823762.6, w1=262720407309535.38\n",
      "SGD iter. 19/29: loss=2.7326797011723125e+30, w0=-1245088357096304.0, w1=-1120026537809408.4\n",
      "SGD iter. 20/29: loss=4.60515301414672e+32, w0=1.1032429805273492e+16, w1=1.0160898876016644e+16\n",
      "SGD iter. 21/29: loss=6.30423265151724e+33, w0=-5.207004894410837e+16, w1=-3.726241791400454e+16\n",
      "SGD iter. 22/29: loss=2.3750515293273664e+35, w0=4.6065364967706266e+17, w1=6.693908809141868e+17\n",
      "SGD iter. 23/29: loss=3.948573467114412e+37, w0=-5.43594216889114e+18, w1=-4.087186429655912e+18\n",
      "SGD iter. 24/29: loss=5.276501390349386e+39, w0=1.6931329221252864e+19, w1=1.5540788642047513e+19\n",
      "SGD iter. 25/29: loss=3.9623616595078825e+39, w0=-4.256371006846959e+19, w1=-3.3947579451463963e+19\n",
      "SGD iter. 26/29: loss=1.4364782748028013e+41, w0=2.232950061779909e+20, w1=9.167493037840412e+19\n",
      "SGD iter. 27/29: loss=3.5773757717483785e+42, w0=-1.7432058471768282e+21, w1=-1.3300418008647037e+21\n",
      "SGD iter. 28/29: loss=5.487463001378738e+44, w0=1.1311293307119605e+22, w1=1.2159705906312957e+22\n",
      "SGD iter. 29/29: loss=7.934683524824363e+45, w0=-4.976040932302051e+22, w1=-3.645515062289663e+22\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=-0.655, w1=-0.21199238702088136\n",
      "SGD iter. 1/29: loss=35.76284328511568, w0=4.89471979681012, w1=5.600484044844908\n",
      "SGD iter. 2/29: loss=3044.929851324104, w0=-33.7435255358621, w1=-29.35383402072807\n",
      "SGD iter. 3/29: loss=90902.42092268259, w0=350.18203037814135, w1=253.79485081281393\n",
      "SGD iter. 4/29: loss=69369077.06780806, w0=-5046.065540655533, w1=-5809.533498059521\n",
      "SGD iter. 5/29: loss=3017853899.189408, w0=31602.989134712032, w1=10424.256546781315\n",
      "SGD iter. 6/29: loss=89936977520.26933, w0=-232603.64701472595, w1=-178178.74550136618\n",
      "SGD iter. 7/29: loss=6713511145759.475, w0=4300990.636218377, w1=4664157.491003063\n",
      "SGD iter. 8/29: loss=1.1100472104918896e+16, w0=-65151058.30440397, w1=-80692554.42917943\n",
      "SGD iter. 9/29: loss=3.032191562389161e+17, w0=249396756.97634757, w1=197687584.7558571\n",
      "SGD iter. 10/29: loss=1.4649590126300416e+18, w0=-832897957.9394172, w1=-1005292252.3509592\n",
      "SGD iter. 11/29: loss=4.821840419593033e+19, w0=7481036943.943329, w1=4974782152.152967\n",
      "SGD iter. 12/29: loss=1.6471509311436028e+22, w0=-208445865064.24435, w1=-214966184909.71558\n",
      "SGD iter. 13/29: loss=4.993095792684261e+25, w0=3304275990903.664, w1=2006659002447.5637\n",
      "SGD iter. 14/29: loss=4.419737845441063e+26, w0=-14608046058069.172, w1=-12381393431629.684\n",
      "SGD iter. 15/29: loss=2.3105594830152746e+28, w0=108767761020608.84, w1=252395004854045.25\n",
      "SGD iter. 16/29: loss=1.3029511332263487e+30, w0=-938831423089843.1, w1=-470450797453399.5\n",
      "SGD iter. 17/29: loss=2.1908742984468395e+32, w0=4249891746598486.0, w1=2155017537340873.5\n",
      "SGD iter. 18/29: loss=3.417382761604291e+32, w0=-1.4178950737107086e+16, w1=-1.0931049189018974e+16\n",
      "SGD iter. 19/29: loss=2.651387927744559e+34, w0=9.701513174053523e+16, w1=9.990552696958648e+16\n",
      "SGD iter. 20/29: loss=6.491297769235089e+35, w0=-3.243237310453797e+17, w1=-4.934920719899696e+17\n",
      "SGD iter. 21/29: loss=2.287552189151596e+36, w0=8.034174959187373e+17, w1=1.677831662373076e+17\n",
      "SGD iter. 22/29: loss=4.580145131788456e+37, w0=-3.5722560030899395e+18, w1=-3.672006178756313e+18\n",
      "SGD iter. 23/29: loss=1.0197710501039992e+39, w0=3.704158715320587e+19, w1=3.6055349579494253e+19\n",
      "SGD iter. 24/29: loss=4.6342222396617615e+41, w0=-4.847669274634649e+20, w1=-6.054585036246368e+20\n",
      "SGD iter. 25/29: loss=2.289773706488198e+43, w0=4.148814264438396e+21, w1=3.0143854046767885e+21\n",
      "SGD iter. 26/29: loss=3.0941208662367e+45, w0=-1.9798443513969876e+22, w1=-2.257401941992485e+22\n",
      "SGD iter. 27/29: loss=1.0338300865653465e+46, w0=7.3135571209549406e+22, w1=4.944260306169627e+22\n",
      "SGD iter. 28/29: loss=4.235105406860853e+47, w0=-6.17604271264275e+23, w1=-5.567030538056292e+23\n",
      "SGD iter. 29/29: loss=9.966439353956811e+49, w0=3.825021126010505e+24, w1=5.044184866639834e+24\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=-0.655, w1=-0.599993501958201\n",
      "SGD iter. 1/29: loss=15.918298025831167, w0=1.236064187491447, w1=0.45405892016338956\n",
      "SGD iter. 2/29: loss=22.337762175141215, w0=-3.383035711214731, w1=-3.508997731945466\n",
      "SGD iter. 3/29: loss=1078.2259817032775, w0=15.944352922036902, w1=9.223739814007942\n",
      "SGD iter. 4/29: loss=7918.64916215355, w0=-68.32962798792006, w1=-108.95902224098984\n",
      "SGD iter. 5/29: loss=366648.24884180864, w0=307.5073141636363, w1=36.13691883571573\n",
      "SGD iter. 6/29: loss=2897661.7128814016, w0=-2937.82084381766, w1=-1057.0934518487365\n",
      "SGD iter. 7/29: loss=28000386673.640335, w0=35846.18647198368, w1=32126.22817049034\n",
      "SGD iter. 8/29: loss=15925122665.89386, w0=-105320.5359743131, w1=-114001.95739786919\n",
      "SGD iter. 9/29: loss=569791570178.3182, w0=739183.8480654212, w1=334242.55147331406\n",
      "SGD iter. 10/29: loss=203196702251637.75, w0=-4458501.76624648, w1=-5671775.246342803\n",
      "SGD iter. 11/29: loss=460323432698554.5, w0=10386787.363014687, w1=2229018.844450888\n",
      "SGD iter. 12/29: loss=1.4409400008573706e+16, w0=-46833657.69531526, w1=-43646417.88684945\n",
      "SGD iter. 13/29: loss=7.776116831935242e+16, w0=285629509.75963145, w1=190156167.90712023\n",
      "SGD iter. 14/29: loss=6.3753851870652236e+19, w0=-5519148521.116927, w1=-4683382366.015081\n",
      "SGD iter. 15/29: loss=2.8051816372125586e+21, w0=21534494948.365135, w1=13624301969.775648\n",
      "SGD iter. 16/29: loss=1.2428011810227766e+22, w0=-88637357408.15387, w1=-123438684643.34511\n",
      "SGD iter. 17/29: loss=3.7624994573994784e+23, w0=495811340557.5474, w1=522417735755.84705\n",
      "SGD iter. 18/29: loss=2.1912035448772636e+25, w0=-4244745691399.2954, w1=-7198892211673.368\n",
      "SGD iter. 19/29: loss=2.8614512173294186e+27, w0=28590645025073.22, w1=20464331355502.664\n",
      "SGD iter. 20/29: loss=3.4878878417354124e+28, w0=-136044394918831.69, w1=-137286983748429.66\n",
      "SGD iter. 21/29: loss=1.966942808384691e+30, w0=1130804743291755.8, w1=742901968092986.2\n",
      "SGD iter. 22/29: loss=2.3802830280137333e+32, w0=-2.6795503335011508e+16, w1=-1.0688227072442114e+16\n",
      "SGD iter. 23/29: loss=1.1037691495841991e+36, w0=1.2092719027320353e+18, w1=5.351145168484615e+17\n",
      "SGD iter. 24/29: loss=4.520171093205468e+38, w0=-9.272995663018711e+18, w1=-8.663400710455548e+18\n",
      "SGD iter. 25/29: loss=4.3862629361542165e+39, w0=3.1829621666646614e+19, w1=2.105772934510542e+19\n",
      "SGD iter. 26/29: loss=2.5189584398813093e+40, w0=-1.1293625847017384e+20, w1=-9.322796127594643e+19\n",
      "SGD iter. 27/29: loss=2.2577276484499553e+42, w0=1.9356099494114333e+21, w1=2.772729719805803e+21\n",
      "SGD iter. 28/29: loss=3.2233469038648103e+45, w0=-2.81565140360933e+22, w1=-7.411824201654437e+22\n",
      "SGD iter. 29/29: loss=7.133651842472729e+46, w0=1.4634106864998023e+23, w1=5.977279890687146e+22\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.0, w1=-0.2827702476856906\n",
      "SGD iter. 1/29: loss=82.38176018600124, w0=-6.762792094998229, w1=-5.236534288829272\n",
      "SGD iter. 2/29: loss=5327.32842343026, w0=52.416925038079015, w1=43.09774130082954\n",
      "SGD iter. 3/29: loss=285153.965564836, w0=-304.7460410807526, w1=-280.0556386447263\n",
      "SGD iter. 4/29: loss=3690470.1986085074, w0=1348.1100176788705, w1=1390.8202802366075\n",
      "SGD iter. 5/29: loss=158457369.18940088, w0=-7352.3335994046365, w1=-16006.280980867607\n",
      "SGD iter. 6/29: loss=4797901153.36142, w0=73453.00044001221, w1=111179.68773292104\n",
      "SGD iter. 7/29: loss=1215940210343.31, w0=-1248464.5751230905, w1=-868898.697013801\n",
      "SGD iter. 8/29: loss=359331238080233.9, w0=7950471.604670087, w1=4660975.677736153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 9/29: loss=1394266428502688.5, w0=-32379274.488517664, w1=-26212055.93497309\n",
      "SGD iter. 10/29: loss=6.352411745608219e+17, w0=968089867.4792696, w1=1222329245.5152366\n",
      "SGD iter. 11/29: loss=3.645561148463768e+20, w0=-7756984369.297433, w1=-4746656774.014442\n",
      "SGD iter. 12/29: loss=2.1433308071090434e+21, w0=29627442997.247604, w1=17923916509.737915\n",
      "SGD iter. 13/29: loss=6.9002821745886905e+22, w0=-150206111110.96436, w1=-172555852402.34348\n",
      "SGD iter. 14/29: loss=8.835841501981037e+23, w0=1098425484841.8965, w1=3639311058992.5703\n",
      "SGD iter. 15/29: loss=2.5353500005527958e+26, w0=-10614742678795.812, w1=-9201780757753.818\n",
      "SGD iter. 16/29: loss=1.0332232550973508e+28, w0=64794870510420.28, w1=42122498233057.43\n",
      "SGD iter. 17/29: loss=1.9969089360104873e+29, w0=-389661309263108.9, w1=-408983758714257.9\n",
      "SGD iter. 18/29: loss=3.3899367760043723e+31, w0=2232394639505689.0, w1=1891951742149903.5\n",
      "SGD iter. 19/29: loss=1.305792391244432e+32, w0=-9321896969707524.0, w1=-9760916129955422.0\n",
      "SGD iter. 20/29: loss=5.86401127423805e+33, w0=4.539525566119846e+16, w1=2.6017537776408416e+16\n",
      "SGD iter. 21/29: loss=9.24362544793582e+34, w0=-1.7556159188233546e+17, w1=-1.8561391128083584e+17\n",
      "SGD iter. 22/29: loss=1.667146825691147e+36, w0=1.024902558198312e+18, w1=2.491476261631361e+18\n",
      "SGD iter. 23/29: loss=7.317329586058938e+37, w0=-6.181187651549155e+18, w1=-2.5521869694368164e+18\n",
      "SGD iter. 24/29: loss=2.955601400144433e+39, w0=5.9752862644121485e+19, w1=3.5636127275390534e+19\n",
      "SGD iter. 25/29: loss=1.3169209917216679e+42, w0=-7.962287852735397e+20, w1=-8.470782266786806e+20\n",
      "SGD iter. 26/29: loss=4.896179304581726e+43, w0=5.791978014488516e+21, w1=8.557682181645394e+21\n",
      "SGD iter. 27/29: loss=3.6786194531764515e+45, w0=-5.94779249633565e+22, w1=-8.798908678689369e+22\n",
      "SGD iter. 28/29: loss=8.125265559890771e+47, w0=8.424144241429334e+23, w1=6.698188366524274e+23\n",
      "SGD iter. 29/29: loss=1.5345420272722092e+50, w0=-9.667156173080691e+24, w1=-1.6389637430327882e+25\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.0, w1=3.318862372223282\n",
      "SGD iter. 1/29: loss=27.306129341748708, w0=-3.3337154679235885, w1=0.6815523570008013\n",
      "SGD iter. 2/29: loss=409.3891116801172, w0=17.70220463837115, w1=19.859726456980933\n",
      "SGD iter. 3/29: loss=46094.5102665227, w0=-153.0377318266044, w1=-369.83798881537655\n",
      "SGD iter. 4/29: loss=3062635.922213787, w0=2333.870140962285, w1=2628.1291179198324\n",
      "SGD iter. 5/29: loss=1784911116.970666, w0=-17565.49500117213, w1=-18873.885834801695\n",
      "SGD iter. 6/29: loss=10453359412.98317, w0=82319.25758679313, w1=106546.22920917555\n",
      "SGD iter. 7/29: loss=1162146444846.7104, w0=-492255.1847368728, w1=-372394.8746149457\n",
      "SGD iter. 8/29: loss=8293690670500.34, w0=2226096.0264423927, w1=1381451.3987456479\n",
      "SGD iter. 9/29: loss=713056476912150.5, w0=-18952499.43516823, w1=-17456525.555143807\n",
      "SGD iter. 10/29: loss=3.1862138799890452e+16, w0=103240529.000584, w1=190603155.4497648\n",
      "SGD iter. 11/29: loss=4.559871309274823e+17, w0=-532850513.0605891, w1=-324928176.1525791\n",
      "SGD iter. 12/29: loss=4.908257278573952e+19, w0=2669869060.4173584, w1=1349818451.0104258\n",
      "SGD iter. 13/29: loss=1.5846762291775006e+20, w0=-12580217258.315632, w1=-12158557138.677683\n",
      "SGD iter. 14/29: loss=6.831372991598721e+22, w0=254224780178.1707, w1=319073281201.4521\n",
      "SGD iter. 15/29: loss=1.341232444121726e+25, w0=-1078062378670.9727, w1=-603876955350.4507\n",
      "SGD iter. 16/29: loss=1.6984856827145965e+25, w0=2940884649522.2593, w1=3424504468484.4546\n",
      "SGD iter. 17/29: loss=8.5929409379001e+26, w0=-30949506072663.03, w1=-13435048733198.957\n",
      "SGD iter. 18/29: loss=4.005018710259115e+29, w0=133655070068920.69, w1=180590978023323.66\n",
      "SGD iter. 19/29: loss=3.142828157252817e+29, w0=-213646587936329.3, w1=-53016741822110.34\n",
      "SGD iter. 20/29: loss=6.187257433289511e+30, w0=779248425120390.5, w1=931905214919824.9\n",
      "SGD iter. 21/29: loss=1.428215564714094e+31, w0=-3985314478706392.5, w1=-9546288043353400.0\n",
      "SGD iter. 22/29: loss=5.1084068627807847e+33, w0=4.041639160046542e+16, w1=2.5991967159821384e+16\n",
      "SGD iter. 23/29: loss=1.6888754321969614e+35, w0=-3.791651068065063e+17, w1=-5.673927325663997e+17\n",
      "SGD iter. 24/29: loss=2.139612924533066e+37, w0=2.1085380715160778e+18, w1=1.9679455258099694e+18\n",
      "SGD iter. 25/29: loss=1.4923445984962907e+38, w0=-8.926514076981334e+18, w1=-8.823656806343209e+18\n",
      "SGD iter. 26/29: loss=4.9411605898143034e+39, w0=4.0869333485052805e+19, w1=3.2548950884098413e+19\n",
      "SGD iter. 27/29: loss=6.283495407745206e+40, w0=-2.1678453427256767e+20, w1=-2.891192650330075e+18\n",
      "SGD iter. 28/29: loss=1.741837795456099e+43, w0=3.796013359133792e+21, w1=4.36601530483966e+21\n",
      "SGD iter. 29/29: loss=2.03507820924231e+45, w0=-1.835580590034286e+22, w1=-1.4180331540280083e+22\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.7050000000000001, w1=0.5984159269968705\n",
      "SGD iter. 1/29: loss=63.31579651726235, w0=-3.988335441074109, w1=-4.816639252957956\n",
      "SGD iter. 2/29: loss=1100.0779788136063, w0=18.604425954367915, w1=16.42015019348031\n",
      "SGD iter. 3/29: loss=10704.502666309714, w0=-111.91601775615328, w1=-58.07882395555092\n",
      "SGD iter. 4/29: loss=8178288.230417924, w0=1726.4907847021543, w1=1788.41245991496\n",
      "SGD iter. 5/29: loss=444910256.23779976, w0=-10646.012666915733, w1=-11988.645053624441\n",
      "SGD iter. 6/29: loss=7857883316.725569, w0=56304.279914946615, w1=125086.81977945205\n",
      "SGD iter. 7/29: loss=154853654703.02036, w0=-307855.0649801503, w1=-205236.5362686665\n",
      "SGD iter. 8/29: loss=7244389517237.745, w0=1496509.9033863614, w1=2287130.1705852747\n",
      "SGD iter. 9/29: loss=145082181054046.62, w0=-7306979.806521779, w1=-6865679.073973184\n",
      "SGD iter. 10/29: loss=3287980675044037.0, w0=42577767.18146443, w1=32623979.639325067\n",
      "SGD iter. 11/29: loss=2.3071198168400774e+17, w0=-364914226.54910606, w1=-225551540.6672095\n",
      "SGD iter. 12/29: loss=1.3111490120015153e+19, w0=2100909963.4545126, w1=1878345152.863349\n",
      "SGD iter. 13/29: loss=4.103544144602241e+20, w0=-9084489301.155853, w1=-7937189970.0110445\n",
      "SGD iter. 14/29: loss=1.7088427303043355e+21, w0=31310394526.81926, w1=36127104235.285385\n",
      "SGD iter. 15/29: loss=1.006476157358169e+23, w0=-161757209971.61542, w1=-99889195718.49185\n",
      "SGD iter. 16/29: loss=1.0639650207270349e+24, w0=588882151552.0083, w1=417380259665.5272\n",
      "SGD iter. 17/29: loss=1.688539636667612e+25, w0=-2394253067993.578, w1=-2064824800062.9624\n",
      "SGD iter. 18/29: loss=2.5469317494777405e+26, w0=13785391910259.225, w1=11794832795015.21\n",
      "SGD iter. 19/29: loss=5.225770190426309e+28, w0=-76405275110554.27, w1=-104333556523918.12\n",
      "SGD iter. 20/29: loss=1.5156379988724347e+29, w0=292861106587145.6, w1=195511961280151.25\n",
      "SGD iter. 21/29: loss=2.750972418919807e+30, w0=-1320391625921553.5, w1=-993556850892579.0\n",
      "SGD iter. 22/29: loss=2.738319347960662e+32, w0=8255666964696686.0, w1=5652316298398714.0\n",
      "SGD iter. 23/29: loss=3.032560021543987e+33, w0=-4.079221547846521e+16, w1=-3.587366017383065e+16\n",
      "SGD iter. 24/29: loss=1.982772424452346e+35, w0=5.498276623455994e+17, w1=4.982711938003502e+17\n",
      "SGD iter. 25/29: loss=9.841683517829566e+37, w0=-7.441438443335633e+18, w1=-7.85645222563711e+18\n",
      "SGD iter. 26/29: loss=5.0488325927720564e+39, w0=4.944044249746418e+19, w1=6.241913238039506e+19\n",
      "SGD iter. 27/29: loss=1.6367628653425551e+41, w0=-2.5506681039592794e+20, w1=-1.467570606317155e+20\n",
      "SGD iter. 28/29: loss=3.1156996579785983e+42, w0=1.0369463378548033e+21, w1=9.837592514783703e+20\n",
      "SGD iter. 29/29: loss=4.420605395128524e+43, w0=-3.6401641346473473e+21, w1=-1.6467560637137326e+21\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=-0.7050000000000001, w1=-0.4032494970948513\n",
      "SGD iter. 1/29: loss=19.514382347335705, w0=3.9442188835023657, w1=3.9232085045486684\n",
      "SGD iter. 2/29: loss=2108.5854588172333, w0=-49.76317352219021, w1=-105.6689321415653\n",
      "SGD iter. 3/29: loss=538941.1952867565, w0=682.4768683523622, w1=678.8730010237455\n",
      "SGD iter. 4/29: loss=58338318.52750803, w0=-5345.006191721859, w1=-6063.6828705717435\n",
      "SGD iter. 5/29: loss=5257083674.504263, w0=58480.41813951424, w1=19848.619336494703\n",
      "SGD iter. 6/29: loss=816048576521.3629, w0=-872429.804133936, w1=-1489912.1779514153\n",
      "SGD iter. 7/29: loss=149415626336892.78, w0=5580004.980874093, w1=5079151.468169192\n",
      "SGD iter. 8/29: loss=910653377558528.9, w0=-26153062.80019977, w1=-37666089.01216272\n",
      "SGD iter. 9/29: loss=8.116533376311566e+16, w0=229771443.6056977, w1=240768963.9287319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 10/29: loss=4.017264770483013e+18, w0=-1747681579.2708344, w1=-1766789842.5076659\n",
      "SGD iter. 11/29: loss=5.013226451613976e+20, w0=10285212063.505322, w1=6900080158.707058\n",
      "SGD iter. 12/29: loss=3.2653763132104864e+21, w0=-29273944792.17265, w1=-27814308157.92049\n",
      "SGD iter. 13/29: loss=1.7997331219409445e+22, w0=97387314752.5142, w1=97433662396.70103\n",
      "SGD iter. 14/29: loss=1.1392125684129604e+24, w0=-593640258534.7268, w1=-351399260403.3776\n",
      "SGD iter. 15/29: loss=1.9157248524786332e+25, w0=5640337142601.714, w1=4901949901580.986\n",
      "SGD iter. 16/29: loss=1.2901300876317689e+28, w0=-114449949626946.45, w1=-165679906701760.78\n",
      "SGD iter. 17/29: loss=2.1146067053483792e+30, w0=868603270317425.0, w1=1911390751613905.0\n",
      "SGD iter. 18/29: loss=3.096416141516842e+31, w0=-3882711785283253.0, w1=-2448186866143663.0\n",
      "SGD iter. 19/29: loss=1.0901494869406303e+33, w0=1.928348896929605e+16, w1=1.804808208133542e+16\n",
      "SGD iter. 20/29: loss=2.5153508758970453e+34, w0=-8.195448263547554e+16, w1=-3.2302700564177316e+16\n",
      "SGD iter. 21/29: loss=2.447597571514976e+35, w0=3.8899200507510566e+17, w1=2.5464129657339677e+17\n",
      "SGD iter. 22/29: loss=2.280444033659726e+37, w0=-2.2842519628534036e+18, w1=-2.2136307603397384e+18\n",
      "SGD iter. 23/29: loss=2.1089119987903317e+38, w0=1.2662758358606307e+19, w1=1.1747835664428286e+19\n",
      "SGD iter. 24/29: loss=1.6708042466050273e+40, w0=-1.0704165870789925e+20, w1=-1.0882604991312753e+20\n",
      "SGD iter. 25/29: loss=1.3344348320742404e+42, w0=2.6959824220902248e+20, w1=1.1906868195325608e+20\n",
      "SGD iter. 26/29: loss=7.810128581734609e+41, w0=-9.006345950897241e+20, w1=-8.139041304291364e+20\n",
      "SGD iter. 27/29: loss=2.5034182483979545e+43, w0=4.585834936284181e+21, w1=2.9130444244570137e+21\n",
      "SGD iter. 28/29: loss=6.727762591337606e+45, w0=-1.7672656197428967e+22, w1=-2.879520812637982e+22\n",
      "SGD iter. 29/29: loss=5.466760201058686e+45, w0=1.911025858076959e+22, w1=3.735282253954942e+21\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=0.0, w1=0.15233437995009624\n",
      "SGD iter. 1/29: loss=5.147514178959908, w0=-0.6742985001749324, w1=-0.2432928942605347\n",
      "SGD iter. 2/29: loss=5.394913448122318, w0=1.6512283128150735, w1=1.6714961740193246\n",
      "SGD iter. 3/29: loss=686.3733348084963, w0=-12.254287545830227, w1=-7.6860920442166965\n",
      "SGD iter. 4/29: loss=4156.863415394942, w0=55.28499424530983, w1=58.13570185819856\n",
      "SGD iter. 5/29: loss=609467.1940902751, w0=-951.1553022811979, w1=-717.3942736478838\n",
      "SGD iter. 6/29: loss=255928716.677954, w0=16341.001199567765, w1=11276.92557004632\n",
      "SGD iter. 7/29: loss=52917285356.465706, w0=-102778.32540670312, w1=-79224.86956550302\n",
      "SGD iter. 8/29: loss=343422405885.2349, w0=407176.8187083978, w1=265119.1441312361\n",
      "SGD iter. 9/29: loss=3241284811528.6147, w0=-1386378.7093378901, w1=-1624668.2410813002\n",
      "SGD iter. 10/29: loss=705064483123627.8, w0=7428160.919286795, w1=12158394.735090923\n",
      "SGD iter. 11/29: loss=1075265249060873.9, w0=5154365.8841752075, w1=13322641.103243046\n",
      "SGD iter. 12/29: loss=1.859923123868635e+17, w0=-14749379.007943708, w1=65296.68168232404\n",
      "SGD iter. 13/29: loss=1.3961537010690618e+17, w0=-334656037.6040173, w1=-118155953.59981772\n",
      "SGD iter. 14/29: loss=1.3929324072187795e+19, w0=3528807001.4162855, w1=2193168730.3668346\n",
      "SGD iter. 15/29: loss=1.8364974885403686e+21, w0=-23985845528.822666, w1=-24121545515.228683\n",
      "SGD iter. 16/29: loss=3.5311303734755246e+22, w0=124640977657.21191, w1=71808772121.11461\n",
      "SGD iter. 17/29: loss=6.276959146506457e+23, w0=-572764359512.7343, w1=-571126739996.3141\n",
      "SGD iter. 18/29: loss=2.6753183457537777e+25, w0=3529776623224.8135, w1=2651747177752.0264\n",
      "SGD iter. 19/29: loss=1.3330553946088118e+27, w0=-19377714559308.438, w1=-23092353329672.543\n",
      "SGD iter. 20/29: loss=1.3127600528415434e+28, w0=95304631579805.23, w1=93717383787581.38\n",
      "SGD iter. 21/29: loss=4.43249524326838e+29, w0=-781096648866401.9, w1=-762453429641290.6\n",
      "SGD iter. 22/29: loss=2.4602810727754488e+32, w0=7411467348946088.0, w1=1.4419529844911384e+16\n",
      "SGD iter. 23/29: loss=2.4522371859089474e+33, w0=-4.31814715580017e+16, w1=-1.5947265746164264e+16\n",
      "SGD iter. 24/29: loss=1.4027375547969565e+35, w0=5.1531174676364096e+17, w1=7.606668483957588e+17\n",
      "SGD iter. 25/29: loss=1.2076068164452274e+38, w0=-7.072383495597432e+18, w1=-7.763775934646447e+18\n",
      "SGD iter. 26/29: loss=6.721474939895932e+39, w0=9.245986968537393e+19, w1=5.97834021325055e+19\n",
      "SGD iter. 27/29: loss=2.220619581520931e+42, w0=-1.3856805582992866e+21, w1=-1.2373307146007628e+21\n",
      "SGD iter. 28/29: loss=4.50140195404207e+44, w0=4.240522905118238e+21, w1=2.7212013525197683e+21\n",
      "SGD iter. 29/29: loss=2.0442108772514975e+44, w0=1.3771716772570107e+22, w1=1.5081076863918315e+22\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=-0.755, w1=-0.7118881292873915\n",
      "SGD iter. 1/29: loss=35.73080464361895, w0=7.068698908704283, w1=9.440281309738342\n",
      "SGD iter. 2/29: loss=23665.28913327253, w0=-35.37714191426363, w1=-21.85168222734124\n",
      "SGD iter. 3/29: loss=14260.660316828576, w0=127.19796768074008, w1=120.81302384030577\n",
      "SGD iter. 4/29: loss=323575.5505799047, w0=-394.2637855096422, w1=-161.03372014601524\n",
      "SGD iter. 5/29: loss=3133352.364751488, w0=1617.6702900802522, w1=2856.0504769093454\n",
      "SGD iter. 6/29: loss=923260624.3939089, w0=-21302.19498706844, w1=-16285.25798559318\n",
      "SGD iter. 7/29: loss=27724066457.170498, w0=171357.78564435945, w1=303668.0591024125\n",
      "SGD iter. 8/29: loss=5049714621006.287, w0=-2315194.2657126654, w1=-1453580.075722529\n",
      "SGD iter. 9/29: loss=790283296089424.6, w0=15301497.022386996, w1=6900612.617095523\n",
      "SGD iter. 10/29: loss=8899543722893564.0, w0=-94007091.0248022, w1=-100771599.25192118\n",
      "SGD iter. 11/29: loss=1.7143317391012257e+18, w0=1695061700.6867735, w1=1483793719.406231\n",
      "SGD iter. 12/29: loss=1.1646038636296174e+21, w0=-18139005689.212944, w1=-13866360499.16523\n",
      "SGD iter. 13/29: loss=1.6536787073751828e+22, w0=185537586157.35852, w1=117270970668.00961\n",
      "SGD iter. 14/29: loss=1.8092565308322406e+25, w0=-3874134741222.325, w1=-1303837848335.8218\n",
      "SGD iter. 15/29: loss=1.458086588906705e+27, w0=51459847196899.42, w1=65610211992707.7\n",
      "SGD iter. 16/29: loss=7.379319213032268e+29, w0=-1306957782995458.5, w1=-4078747140337585.0\n",
      "SGD iter. 17/29: loss=6.1607241964572356e+32, w0=1.2998329326749548e+16, w1=1.0534511441747716e+16\n",
      "SGD iter. 18/29: loss=4.992940005780334e+33, w0=-7.029034092012283e+16, w1=-8.081556370191062e+16\n",
      "SGD iter. 19/29: loss=5.4882718303847365e+35, w0=9.923365307446157e+17, w1=7.883738417294966e+17\n",
      "SGD iter. 20/29: loss=3.579698563443218e+38, w0=-2.8558003417779847e+19, w1=-1.9841376666149523e+19\n",
      "SGD iter. 21/29: loss=2.205208663178199e+41, w0=2.8513058485176887e+20, w1=7.098992177682358e+19\n",
      "SGD iter. 22/29: loss=4.831882640056566e+42, w0=-1.5865115926121606e+21, w1=-1.329487009409951e+21\n",
      "SGD iter. 23/29: loss=1.8947213399847097e+44, w0=8.787017237282898e+21, w1=9.07463297809825e+21\n",
      "SGD iter. 24/29: loss=4.064657784896337e+45, w0=-5.282912108236524e+22, w1=-4.329473743463756e+22\n",
      "SGD iter. 25/29: loss=2.1259432872030115e+47, w0=2.7279518772034536e+23, w1=2.4578944560912254e+23\n",
      "SGD iter. 26/29: loss=2.7328415083571743e+48, w0=-1.3047158916634108e+24, w1=-1.369468515353486e+24\n",
      "SGD iter. 27/29: loss=1.3416619946882944e+50, w0=1.554443150929753e+25, w1=9.962409249121778e+24\n",
      "SGD iter. 28/29: loss=8.032951646360173e+52, w0=-1.548466621127849e+26, w1=-1.2517467339234358e+26\n",
      "SGD iter. 29/29: loss=1.0026171467600448e+54, w0=5.808625590279162e+26, w1=3.310555817316899e+25\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=-0.755, w1=-0.435713534579117\n",
      "SGD iter. 1/29: loss=12.234490345851045, w0=3.6018394507749623, w1=1.7719717880851438\n",
      "SGD iter. 2/29: loss=5979.349305444524, w0=-126.87151320697423, w1=-292.3260009996619\n",
      "SGD iter. 3/29: loss=8704959.623706967, w0=6219.014018051123, w1=2629.4909058433204\n",
      "SGD iter. 4/29: loss=60314933032.190704, w0=-161485.50398023214, w1=-123290.8916188955\n",
      "SGD iter. 5/29: loss=2969200152417.6445, w0=936251.0584079537, w1=288044.8737960642\n",
      "SGD iter. 6/29: loss=40241433005949.39, w0=-4498468.640533447, w1=-4658998.079999747\n",
      "SGD iter. 7/29: loss=533252878652987.25, w0=16621080.583942052, w1=8926461.146152105\n",
      "SGD iter. 8/29: loss=1.2499972431656074e+16, w0=-108847400.54100305, w1=-55157762.33099556\n",
      "SGD iter. 9/29: loss=2.2349249446206843e+18, w0=806553429.7947721, w1=914949575.9912407\n",
      "SGD iter. 10/29: loss=2.725330869147787e+19, w0=-3759888929.15261, w1=-3555485634.4424944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 11/29: loss=6.68131668873944e+20, w0=19005721808.85744, w1=9059223937.088326\n",
      "SGD iter. 12/29: loss=2.6721461913001627e+22, w0=-138241955950.1592, w1=-116952128381.8093\n",
      "SGD iter. 13/29: loss=1.9873250586800032e+24, w0=744600810246.3042, w1=584607394172.8461\n",
      "SGD iter. 14/29: loss=2.1820065985794926e+25, w0=-6509496354561.67, w1=-10275268442136.67\n",
      "SGD iter. 15/29: loss=4.006482868846127e+28, w0=164007671731952.75, w1=110921838402834.86\n",
      "SGD iter. 16/29: loss=2.2905808736149406e+30, w0=-842806320938163.1, w1=-1127744263942391.5\n",
      "SGD iter. 17/29: loss=2.8313805692790843e+31, w0=3951006503259906.0, w1=864796574361954.0\n",
      "SGD iter. 18/29: loss=1.4202433121187596e+33, w0=-2.948756245532175e+16, w1=-5.48060349744013e+16\n",
      "SGD iter. 19/29: loss=9.23140934588694e+34, w0=2.30262284791877e+17, w1=1.8414877767223488e+17\n",
      "SGD iter. 20/29: loss=4.974026110495739e+36, w0=-1.4044870236113377e+18, w1=-9.072785696048869e+17\n",
      "SGD iter. 21/29: loss=1.45505715885019e+38, w0=1.6156361668745556e+19, w1=7.49642627877179e+19\n",
      "SGD iter. 22/29: loss=2.2125381411390067e+41, w0=-1.2416915291094393e+21, w1=-5.657641347695888e+20\n",
      "SGD iter. 23/29: loss=4.2441074315202154e+45, w0=4.489533583500156e+22, w1=5.29299616155253e+22\n",
      "SGD iter. 24/29: loss=1.391847680530476e+47, w0=-2.592592946041624e+23, w1=-2.1292581816397505e+23\n",
      "SGD iter. 25/29: loss=4.651165788302697e+48, w0=1.8835089166594138e+24, w1=1.1921070675533606e+24\n",
      "SGD iter. 26/29: loss=4.025239220850418e+50, w0=-8.405484434446376e+24, w1=-1.2014322639429391e+25\n",
      "SGD iter. 27/29: loss=1.7827893950195133e+51, w0=4.519328773202254e+25, w1=8.302082715302978e+25\n",
      "SGD iter. 28/29: loss=2.9276915871641627e+53, w0=-2.42006543111893e+26, w1=-1.0765574456405736e+26\n",
      "SGD iter. 29/29: loss=9.668397177205656e+53, w0=6.65083456478867e+26, w1=8.184015891505452e+26\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "SGD iter. 0/29: loss=0.5, w0=-0.755, w1=-0.37016466207291215\n",
      "SGD iter. 1/29: loss=16.508593166572354, w0=2.2640474795990526, w1=0.5303265824565024\n",
      "SGD iter. 2/29: loss=259.955743945503, w0=-15.823051294061898, w1=-16.880235758186796\n",
      "SGD iter. 3/29: loss=44401.072789111495, w0=144.2187162851178, w1=123.11489522610249\n",
      "SGD iter. 4/29: loss=1472115.8322810691, w0=-655.7732718749319, w1=-604.2867118214707\n",
      "SGD iter. 5/29: loss=11931107.9465793, w0=3463.340002071941, w1=5294.375768690296\n",
      "SGD iter. 6/29: loss=1994059094.4757345, w0=-39919.03108902512, w1=-35606.08507227209\n",
      "SGD iter. 7/29: loss=187708701795.53357, w0=193648.40886340832, w1=108977.78280248171\n",
      "SGD iter. 8/29: loss=1147297904868.4414, w0=-1075112.9677695553, w1=-1916168.9214996789\n",
      "SGD iter. 9/29: loss=127040851043926.48, w0=13978753.12809941, w1=57072466.3969657\n",
      "SGD iter. 10/29: loss=3.784190881406752e+16, w0=-223602057.1685285, w1=-215923274.26550597\n",
      "SGD iter. 11/29: loss=7.622756576849044e+18, w0=2035985489.8284845, w1=1263550096.8698256\n",
      "SGD iter. 12/29: loss=2.700284086998257e+20, w0=-11540615881.915264, w1=-8397350587.749189\n",
      "SGD iter. 13/29: loss=9.087172306960852e+21, w0=60970158999.84912, w1=37925767458.00397\n",
      "SGD iter. 14/29: loss=1.6925298107208903e+23, w0=-369682805149.8107, w1=15746207580.866135\n",
      "SGD iter. 15/29: loss=3.466618190762454e+25, w0=3123071986187.569, w1=3705594942378.531\n",
      "SGD iter. 16/29: loss=5.1155574820183036e+26, w0=-31165591491718.168, w1=-110474964705798.31\n",
      "SGD iter. 17/29: loss=3.240883334113854e+29, w0=386950038475119.06, w1=169210511265578.1\n",
      "SGD iter. 18/29: loss=8.398807297048846e+30, w0=-2097380349304451.5, w1=-2886643369890878.0\n",
      "SGD iter. 19/29: loss=2.6955076398898596e+32, w0=2.023309107988006e+16, w1=3.684447429106212e+16\n",
      "SGD iter. 20/29: loss=8.755792476411016e+34, w0=-2.4961363862581526e+17, w1=-4.715155744396192e+17\n",
      "SGD iter. 21/29: loss=9.734733670171097e+36, w0=2.2895337989658468e+18, w1=2.320807940406013e+18\n",
      "SGD iter. 22/29: loss=2.7677703123076412e+38, w0=-1.3293562068949006e+19, w1=-7.740823082642343e+18\n",
      "SGD iter. 23/29: loss=1.9197118235743263e+40, w0=8.435458404670412e+19, w1=1.2312402042633445e+20\n",
      "SGD iter. 24/29: loss=3.446638197075195e+41, w0=-3.31395513714915e+20, w1=-4.14369691528741e+19\n",
      "SGD iter. 25/29: loss=2.4950906505143334e+42, w0=1.401718321791686e+21, w1=1.8519804527936316e+21\n",
      "SGD iter. 26/29: loss=5.5027735395446476e+44, w0=-1.059709837750699e+22, w1=-9.008347391542602e+21\n",
      "SGD iter. 27/29: loss=3.4792271408057815e+45, w0=4.778916374900701e+22, w1=4.523981300051074e+22\n",
      "SGD iter. 28/29: loss=3.026262054215792e+47, w0=-3.137810648137272e+23, w1=-4.609467154336648e+23\n",
      "SGD iter. 29/29: loss=5.094553440616152e+48, w0=3.119305699694873e+24, w1=4.072686656120227e+24\n",
      "4\n",
      "4,5\n",
      "For polynomial expansion up to degree, the choice of gamma which leads to the best rmse is 0.00500 with a test rmse of 0.962\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.005, 0.9619995757471793)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cross_validation_stochastic_gradient_descent(y, tx, k_fold, initial_w=initial_w, max_iters = max_iters, gammas = gammas, batch_size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e20a256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "4\n",
      "4,5\n",
      "For polynomial expansion up to degree, the choice of gamma which leads to the best rmse is 0.00000 with a test rmse of 0.840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1e-10, 0.8397247106385971)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cross_validation_ridge_regression(y, tx, k_fold, lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88b75b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-67283.15924143717, w0=-0.00814664, w1=-0.00782662272075417\n",
      "Logistic regression iter. 1/29: loss=-63268.949439100725, w0=-0.0160648205953978, w1=-0.015409278281759323\n",
      "Logistic regression iter. 2/29: loss=-59605.04894150916, w0=-0.02376471876206907, w1=-0.022759154096585174\n",
      "Logistic regression iter. 3/29: loss=-56255.532746674915, w0=-0.03125718690228187, w1=-0.02988838308018945\n",
      "Logistic regression iter. 4/29: loss=-53187.08612112823, w0=-0.03855329114133935, w1=-0.03680944698543962\n",
      "Logistic regression iter. 5/29: loss=-50369.54511139353, w0=-0.045663972437364475, w1=-0.043534733045084456\n",
      "Logistic regression iter. 6/29: loss=-47776.00992164823, w0=-0.05259982124255059, w1=-0.050076237978458775\n",
      "Logistic regression iter. 7/29: loss=-45382.7089242441, w0=-0.05937094158652156, w1=-0.0564453897664786\n",
      "Logistic regression iter. 8/29: loss=-43168.73901412943, w0=-0.06598687945073028, w1=-0.06265295424948203\n",
      "Logistic regression iter. 9/29: loss=-41115.75978175704, w0=-0.07245659468732585, w1=-0.06870899883864288\n",
      "Logistic regression iter. 10/29: loss=-39207.68487328233, w0=-0.07878846106601793, w1=-0.07462289271937514\n",
      "Logistic regression iter. 11/29: loss=-37430.39250973859, w0=-0.08499028363508314, w1=-0.08040332917641624\n",
      "Logistic regression iter. 12/29: loss=-35771.46464435744, w0=-0.09106932608031654, w1=-0.08605836042326095\n",
      "Logistic regression iter. 13/29: loss=-34219.95735783786, w0=-0.0970323432612537, w1=-0.09159543868440616\n",
      "Logistic regression iter. 14/29: loss=-32766.201547680022, w0=-0.10288561582275517, w1=-0.09702145957129818\n",
      "Logistic regression iter. 15/29: loss=-31401.631325626495, w0=-0.10863498493836675, w1=-0.1023428053178481\n",
      "Logistic regression iter. 16/29: loss=-30118.636938284537, w0=-0.11428588601077837, w1=-0.10756538643943966\n",
      "Logistic regression iter. 17/29: loss=-28910.438972596567, w0=-0.1198433806587515, w1=-0.11269468102370084\n",
      "Logistic regression iter. 18/29: loss=-27770.98081901555, w0=-0.12531218664607865, w1=-0.1177357712713506\n",
      "Logistic regression iter. 19/29: loss=-26694.836686582305, w0=-0.13069670561589888, w1=-0.12269337716160562\n",
      "Logistic regression iter. 20/29: loss=-25677.13281366231, w0=-0.1360010486230958, w1=-0.12757188727245253\n",
      "Logistic regression iter. 21/29: loss=-24713.479854817124, w0=-0.1412290595353824, w1=-0.13237538687691155\n",
      "Logistic regression iter. 22/29: loss=-23799.91472965773, w0=-0.14638433641792362, w1=-0.13710768348550087\n",
      "Logistic regression iter. 23/29: loss=-22932.850487192, w0=-0.15147025103886277, w1=-0.14177233002772172\n",
      "Logistic regression iter. 24/29: loss=-22109.032969006286, w0=-0.1564899666417938, w1=-0.14637264587151066\n",
      "Logistic regression iter. 25/29: loss=-21325.5032494485, w0=-0.16144645413128178, w1=-0.15091173587576096\n",
      "Logistic regression iter. 26/29: loss=-20579.56499482662, w0=-0.1663425068124147, w1=-0.15539250766142118\n",
      "Logistic regression iter. 27/29: loss=-19868.75602076669, w0=-0.17118075381732228, w1=-0.15981768727405277\n",
      "Logistic regression iter. 28/29: loss=-19190.823441341745, w0=-0.17596367234208007, w1=-0.16418983339678833\n",
      "Logistic regression iter. 29/29: loss=-18543.701899019263, w0=-0.18069359880735655, w1=-0.16851135025845299\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-67283.15924143717, w0=-0.00814664, w1=-0.00782662272075417\n",
      "Logistic regression iter. 1/29: loss=-63268.949439100725, w0=-0.0160648205953978, w1=-0.015409278281759323\n",
      "Logistic regression iter. 2/29: loss=-59605.04894150916, w0=-0.02376471876206907, w1=-0.022759154096585174\n",
      "Logistic regression iter. 3/29: loss=-56255.532746674915, w0=-0.03125718690228187, w1=-0.02988838308018945\n",
      "Logistic regression iter. 4/29: loss=-53187.08612112823, w0=-0.03855329114133935, w1=-0.03680944698543962\n",
      "Logistic regression iter. 5/29: loss=-50369.54511139353, w0=-0.045663972437364475, w1=-0.043534733045084456\n",
      "Logistic regression iter. 6/29: loss=-47776.00992164823, w0=-0.05259982124255059, w1=-0.050076237978458775\n",
      "Logistic regression iter. 7/29: loss=-45382.7089242441, w0=-0.05937094158652156, w1=-0.0564453897664786\n",
      "Logistic regression iter. 8/29: loss=-43168.73901412943, w0=-0.06598687945073028, w1=-0.06265295424948203\n",
      "Logistic regression iter. 9/29: loss=-41115.75978175704, w0=-0.07245659468732585, w1=-0.06870899883864288\n",
      "Logistic regression iter. 10/29: loss=-39207.68487328233, w0=-0.07878846106601793, w1=-0.07462289271937514\n",
      "Logistic regression iter. 11/29: loss=-37430.39250973859, w0=-0.08499028363508314, w1=-0.08040332917641624\n",
      "Logistic regression iter. 12/29: loss=-35771.46464435744, w0=-0.09106932608031654, w1=-0.08605836042326095\n",
      "Logistic regression iter. 13/29: loss=-34219.95735783786, w0=-0.0970323432612537, w1=-0.09159543868440616\n",
      "Logistic regression iter. 14/29: loss=-32766.201547680022, w0=-0.10288561582275517, w1=-0.09702145957129818\n",
      "Logistic regression iter. 15/29: loss=-31401.631325626495, w0=-0.10863498493836675, w1=-0.1023428053178481\n",
      "Logistic regression iter. 16/29: loss=-30118.636938284537, w0=-0.11428588601077837, w1=-0.10756538643943966\n",
      "Logistic regression iter. 17/29: loss=-28910.438972596567, w0=-0.1198433806587515, w1=-0.11269468102370084\n",
      "Logistic regression iter. 18/29: loss=-27770.98081901555, w0=-0.12531218664607865, w1=-0.1177357712713506\n",
      "Logistic regression iter. 19/29: loss=-26694.836686582305, w0=-0.13069670561589888, w1=-0.12269337716160562\n",
      "Logistic regression iter. 20/29: loss=-25677.13281366231, w0=-0.1360010486230958, w1=-0.12757188727245253\n",
      "Logistic regression iter. 21/29: loss=-24713.479854817124, w0=-0.1412290595353824, w1=-0.13237538687691155\n",
      "Logistic regression iter. 22/29: loss=-23799.91472965773, w0=-0.14638433641792362, w1=-0.13710768348550087\n",
      "Logistic regression iter. 23/29: loss=-22932.850487192, w0=-0.15147025103886277, w1=-0.14177233002772172\n",
      "Logistic regression iter. 24/29: loss=-22109.032969006286, w0=-0.1564899666417938, w1=-0.14637264587151066\n",
      "Logistic regression iter. 25/29: loss=-21325.5032494485, w0=-0.16144645413128178, w1=-0.15091173587576096\n",
      "Logistic regression iter. 26/29: loss=-20579.56499482662, w0=-0.1663425068124147, w1=-0.15539250766142118\n",
      "Logistic regression iter. 27/29: loss=-19868.75602076669, w0=-0.17118075381732228, w1=-0.15981768727405277\n",
      "Logistic regression iter. 28/29: loss=-19190.823441341745, w0=-0.17596367234208007, w1=-0.16418983339678833\n",
      "Logistic regression iter. 29/29: loss=-18543.701899019263, w0=-0.18069359880735655, w1=-0.16851135025845299\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-67283.15924143717, w0=-0.00814664, w1=-0.00782662272075417\n",
      "Logistic regression iter. 1/29: loss=-63268.949439100725, w0=-0.0160648205953978, w1=-0.015409278281759323\n",
      "Logistic regression iter. 2/29: loss=-59605.04894150916, w0=-0.02376471876206907, w1=-0.022759154096585174\n",
      "Logistic regression iter. 3/29: loss=-56255.532746674915, w0=-0.03125718690228187, w1=-0.02988838308018945\n",
      "Logistic regression iter. 4/29: loss=-53187.08612112823, w0=-0.03855329114133935, w1=-0.03680944698543962\n",
      "Logistic regression iter. 5/29: loss=-50369.54511139353, w0=-0.045663972437364475, w1=-0.043534733045084456\n",
      "Logistic regression iter. 6/29: loss=-47776.00992164823, w0=-0.05259982124255059, w1=-0.050076237978458775\n",
      "Logistic regression iter. 7/29: loss=-45382.7089242441, w0=-0.05937094158652156, w1=-0.0564453897664786\n",
      "Logistic regression iter. 8/29: loss=-43168.73901412943, w0=-0.06598687945073028, w1=-0.06265295424948203\n",
      "Logistic regression iter. 9/29: loss=-41115.75978175704, w0=-0.07245659468732585, w1=-0.06870899883864288\n",
      "Logistic regression iter. 10/29: loss=-39207.68487328233, w0=-0.07878846106601793, w1=-0.07462289271937514\n",
      "Logistic regression iter. 11/29: loss=-37430.39250973859, w0=-0.08499028363508314, w1=-0.08040332917641624\n",
      "Logistic regression iter. 12/29: loss=-35771.46464435744, w0=-0.09106932608031654, w1=-0.08605836042326095\n",
      "Logistic regression iter. 13/29: loss=-34219.95735783786, w0=-0.0970323432612537, w1=-0.09159543868440616\n",
      "Logistic regression iter. 14/29: loss=-32766.201547680022, w0=-0.10288561582275517, w1=-0.09702145957129818\n",
      "Logistic regression iter. 15/29: loss=-31401.631325626495, w0=-0.10863498493836675, w1=-0.1023428053178481\n",
      "Logistic regression iter. 16/29: loss=-30118.636938284537, w0=-0.11428588601077837, w1=-0.10756538643943966\n",
      "Logistic regression iter. 17/29: loss=-28910.438972596567, w0=-0.1198433806587515, w1=-0.11269468102370084\n",
      "Logistic regression iter. 18/29: loss=-27770.98081901555, w0=-0.12531218664607865, w1=-0.1177357712713506\n",
      "Logistic regression iter. 19/29: loss=-26694.836686582305, w0=-0.13069670561589888, w1=-0.12269337716160562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 20/29: loss=-25677.13281366231, w0=-0.1360010486230958, w1=-0.12757188727245253\n",
      "Logistic regression iter. 21/29: loss=-24713.479854817124, w0=-0.1412290595353824, w1=-0.13237538687691155\n",
      "Logistic regression iter. 22/29: loss=-23799.91472965773, w0=-0.14638433641792362, w1=-0.13710768348550087\n",
      "Logistic regression iter. 23/29: loss=-22932.850487192, w0=-0.15147025103886277, w1=-0.14177233002772172\n",
      "Logistic regression iter. 24/29: loss=-22109.032969006286, w0=-0.1564899666417938, w1=-0.14637264587151066\n",
      "Logistic regression iter. 25/29: loss=-21325.5032494485, w0=-0.16144645413128178, w1=-0.15091173587576096\n",
      "Logistic regression iter. 26/29: loss=-20579.56499482662, w0=-0.1663425068124147, w1=-0.15539250766142118\n",
      "Logistic regression iter. 27/29: loss=-19868.75602076669, w0=-0.17118075381732228, w1=-0.15981768727405277\n",
      "Logistic regression iter. 28/29: loss=-19190.823441341745, w0=-0.17596367234208007, w1=-0.16418983339678833\n",
      "Logistic regression iter. 29/29: loss=-18543.701899019263, w0=-0.18069359880735655, w1=-0.16851135025845299\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-67283.15924143717, w0=-0.00814664, w1=-0.00782662272075417\n",
      "Logistic regression iter. 1/29: loss=-63268.949439100725, w0=-0.0160648205953978, w1=-0.015409278281759323\n",
      "Logistic regression iter. 2/29: loss=-59605.04894150916, w0=-0.02376471876206907, w1=-0.022759154096585174\n",
      "Logistic regression iter. 3/29: loss=-56255.532746674915, w0=-0.03125718690228187, w1=-0.02988838308018945\n",
      "Logistic regression iter. 4/29: loss=-53187.08612112823, w0=-0.03855329114133935, w1=-0.03680944698543962\n",
      "Logistic regression iter. 5/29: loss=-50369.54511139353, w0=-0.045663972437364475, w1=-0.043534733045084456\n",
      "Logistic regression iter. 6/29: loss=-47776.00992164823, w0=-0.05259982124255059, w1=-0.050076237978458775\n",
      "Logistic regression iter. 7/29: loss=-45382.7089242441, w0=-0.05937094158652156, w1=-0.0564453897664786\n",
      "Logistic regression iter. 8/29: loss=-43168.73901412943, w0=-0.06598687945073028, w1=-0.06265295424948203\n",
      "Logistic regression iter. 9/29: loss=-41115.75978175704, w0=-0.07245659468732585, w1=-0.06870899883864288\n",
      "Logistic regression iter. 10/29: loss=-39207.68487328233, w0=-0.07878846106601793, w1=-0.07462289271937514\n",
      "Logistic regression iter. 11/29: loss=-37430.39250973859, w0=-0.08499028363508314, w1=-0.08040332917641624\n",
      "Logistic regression iter. 12/29: loss=-35771.46464435744, w0=-0.09106932608031654, w1=-0.08605836042326095\n",
      "Logistic regression iter. 13/29: loss=-34219.95735783786, w0=-0.0970323432612537, w1=-0.09159543868440616\n",
      "Logistic regression iter. 14/29: loss=-32766.201547680022, w0=-0.10288561582275517, w1=-0.09702145957129818\n",
      "Logistic regression iter. 15/29: loss=-31401.631325626495, w0=-0.10863498493836675, w1=-0.1023428053178481\n",
      "Logistic regression iter. 16/29: loss=-30118.636938284537, w0=-0.11428588601077837, w1=-0.10756538643943966\n",
      "Logistic regression iter. 17/29: loss=-28910.438972596567, w0=-0.1198433806587515, w1=-0.11269468102370084\n",
      "Logistic regression iter. 18/29: loss=-27770.98081901555, w0=-0.12531218664607865, w1=-0.1177357712713506\n",
      "Logistic regression iter. 19/29: loss=-26694.836686582305, w0=-0.13069670561589888, w1=-0.12269337716160562\n",
      "Logistic regression iter. 20/29: loss=-25677.13281366231, w0=-0.1360010486230958, w1=-0.12757188727245253\n",
      "Logistic regression iter. 21/29: loss=-24713.479854817124, w0=-0.1412290595353824, w1=-0.13237538687691155\n",
      "Logistic regression iter. 22/29: loss=-23799.91472965773, w0=-0.14638433641792362, w1=-0.13710768348550087\n",
      "Logistic regression iter. 23/29: loss=-22932.850487192, w0=-0.15147025103886277, w1=-0.14177233002772172\n",
      "Logistic regression iter. 24/29: loss=-22109.032969006286, w0=-0.1564899666417938, w1=-0.14637264587151066\n",
      "Logistic regression iter. 25/29: loss=-21325.5032494485, w0=-0.16144645413128178, w1=-0.15091173587576096\n",
      "Logistic regression iter. 26/29: loss=-20579.56499482662, w0=-0.1663425068124147, w1=-0.15539250766142118\n",
      "Logistic regression iter. 27/29: loss=-19868.75602076669, w0=-0.17118075381732228, w1=-0.15981768727405277\n",
      "Logistic regression iter. 28/29: loss=-19190.823441341745, w0=-0.17596367234208007, w1=-0.16418983339678833\n",
      "Logistic regression iter. 29/29: loss=-18543.701899019263, w0=-0.18069359880735655, w1=-0.16851135025845299\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-59193.837640854705, w0=-0.024439919999999997, w1=-0.023479868162262506\n",
      "Logistic regression iter. 1/29: loss=-49775.08074494744, w0=-0.04684364588293151, w1=-0.0447893881698648\n",
      "Logistic regression iter. 2/29: loss=-42515.956014607174, w0=-0.06753446328634655, w1=-0.06429162730505904\n",
      "Logistic regression iter. 3/29: loss=-36780.04621453514, w0=-0.08680731645362354, w1=-0.08231902869477932\n",
      "Logistic regression iter. 4/29: loss=-32146.002331606716, w0=-0.10490260291110176, w1=-0.09913971259533363\n",
      "Logistic regression iter. 5/29: loss=-28331.776643128644, w0=-0.12201096300034148, w1=-0.11496335162552232\n",
      "Logistic regression iter. 6/29: loss=-25143.391937913642, w0=-0.13828365611274612, w1=-0.12995392969855088\n",
      "Logistic regression iter. 7/29: loss=-22443.53318586731, w0=-0.15384183353218867, w1=-0.144240975920188\n",
      "Logistic regression iter. 8/29: loss=-20132.363948528167, w0=-0.16878368840710728, w1=-0.15792809938588603\n",
      "Logistic regression iter. 9/29: loss=-18135.55572870397, w0=-0.1831897626458481, w1=-0.17109923668318944\n",
      "Logistic regression iter. 10/29: loss=-16396.608766432128, w0=-0.19712686523972822, w1=-0.18382320538476946\n",
      "Logistic regression iter. 11/29: loss=-14871.786789241405, w0=-0.21065098465043136, w1=-0.1961570477530323\n",
      "Logistic regression iter. 12/29: loss=-13526.68784332194, w0=-0.22380947761124279, w1=-0.20814851520548852\n",
      "Logistic regression iter. 13/29: loss=-12333.867096409256, w0=-0.23664273475015848, w1=-0.21983793869611723\n",
      "Logistic regression iter. 14/29: loss=-11271.153338921808, w0=-0.24918546410282336, w1=-0.231259655409865\n",
      "Logistic regression iter. 15/29: loss=-10320.43365441867, w0=-0.26146769220057253, w1=-0.24244311082660833\n",
      "Logistic regression iter. 16/29: loss=-9466.76078598561, w0=-0.2735155538204229, w1=-0.2534137201782279\n",
      "Logistic regression iter. 17/29: loss=-8697.687224745068, w0=-0.2853519216639152, w1=-0.264193549311933\n",
      "Logistic regression iter. 18/29: loss=-8002.761381050383, w0=-0.2969969133825293, w1=-0.2748018583639715\n",
      "Logistic regression iter. 19/29: loss=-7373.14147220455, w0=-0.3084683035874266, w1=-0.2852555400278555\n",
      "Logistic regression iter. 20/29: loss=-6801.296144388212, w0=-0.3197818614950966, w1=-0.2955694759728812\n",
      "Logistic regression iter. 21/29: loss=-6280.7698476305595, w0=-0.3309516298115149, w1=-0.30575682907093255\n",
      "Logistic regression iter. 22/29: loss=-5805.997140582932, w0=-0.34199015676652966, w1=-0.3158292848118998\n",
      "Logistic regression iter. 23/29: loss=-5372.154381492054, w0=-0.35290869048220475, w1=-0.3257972521499732\n",
      "Logistic regression iter. 24/29: loss=-4975.040279549384, w0=-0.36371734282126683, w1=-0.3356700316959669\n",
      "Logistic regression iter. 25/29: loss=-4610.978937546836, w0=-0.37442522832467345, w1=-0.3454559574273346\n",
      "Logistic regression iter. 26/29: loss=-4276.740577463006, w0=-0.3850405826767653, w1=-0.355162516768601\n",
      "Logistic regression iter. 27/29: loss=-3969.4762830953014, w0=-0.39557086423712484, w1=-0.3647964528880291\n",
      "Logistic regression iter. 28/29: loss=-3686.6639392455804, w0=-0.4060228414814928, w1=-0.3743638522810139\n",
      "Logistic regression iter. 29/29: loss=-3426.063178823803, w0=-0.4164026686499925, w1=-0.3838702201088253\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-59193.837640854705, w0=-0.024439919999999997, w1=-0.023479868162262506\n",
      "Logistic regression iter. 1/29: loss=-49775.08074494744, w0=-0.04684364588293151, w1=-0.0447893881698648\n",
      "Logistic regression iter. 2/29: loss=-42515.956014607174, w0=-0.06753446328634655, w1=-0.06429162730505904\n",
      "Logistic regression iter. 3/29: loss=-36780.04621453514, w0=-0.08680731645362354, w1=-0.08231902869477932\n",
      "Logistic regression iter. 4/29: loss=-32146.002331606716, w0=-0.10490260291110176, w1=-0.09913971259533363\n",
      "Logistic regression iter. 5/29: loss=-28331.776643128644, w0=-0.12201096300034148, w1=-0.11496335162552232\n",
      "Logistic regression iter. 6/29: loss=-25143.391937913642, w0=-0.13828365611274612, w1=-0.12995392969855088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 7/29: loss=-22443.53318586731, w0=-0.15384183353218867, w1=-0.144240975920188\n",
      "Logistic regression iter. 8/29: loss=-20132.363948528167, w0=-0.16878368840710728, w1=-0.15792809938588603\n",
      "Logistic regression iter. 9/29: loss=-18135.55572870397, w0=-0.1831897626458481, w1=-0.17109923668318944\n",
      "Logistic regression iter. 10/29: loss=-16396.608766432128, w0=-0.19712686523972822, w1=-0.18382320538476946\n",
      "Logistic regression iter. 11/29: loss=-14871.786789241405, w0=-0.21065098465043136, w1=-0.1961570477530323\n",
      "Logistic regression iter. 12/29: loss=-13526.68784332194, w0=-0.22380947761124279, w1=-0.20814851520548852\n",
      "Logistic regression iter. 13/29: loss=-12333.867096409256, w0=-0.23664273475015848, w1=-0.21983793869611723\n",
      "Logistic regression iter. 14/29: loss=-11271.153338921808, w0=-0.24918546410282336, w1=-0.231259655409865\n",
      "Logistic regression iter. 15/29: loss=-10320.43365441867, w0=-0.26146769220057253, w1=-0.24244311082660833\n",
      "Logistic regression iter. 16/29: loss=-9466.76078598561, w0=-0.2735155538204229, w1=-0.2534137201782279\n",
      "Logistic regression iter. 17/29: loss=-8697.687224745068, w0=-0.2853519216639152, w1=-0.264193549311933\n",
      "Logistic regression iter. 18/29: loss=-8002.761381050383, w0=-0.2969969133825293, w1=-0.2748018583639715\n",
      "Logistic regression iter. 19/29: loss=-7373.14147220455, w0=-0.3084683035874266, w1=-0.2852555400278555\n",
      "Logistic regression iter. 20/29: loss=-6801.296144388212, w0=-0.3197818614950966, w1=-0.2955694759728812\n",
      "Logistic regression iter. 21/29: loss=-6280.7698476305595, w0=-0.3309516298115149, w1=-0.30575682907093255\n",
      "Logistic regression iter. 22/29: loss=-5805.997140582932, w0=-0.34199015676652966, w1=-0.3158292848118998\n",
      "Logistic regression iter. 23/29: loss=-5372.154381492054, w0=-0.35290869048220475, w1=-0.3257972521499732\n",
      "Logistic regression iter. 24/29: loss=-4975.040279549384, w0=-0.36371734282126683, w1=-0.3356700316959669\n",
      "Logistic regression iter. 25/29: loss=-4610.978937546836, w0=-0.37442522832467345, w1=-0.3454559574273346\n",
      "Logistic regression iter. 26/29: loss=-4276.740577463006, w0=-0.3850405826767653, w1=-0.355162516768601\n",
      "Logistic regression iter. 27/29: loss=-3969.4762830953014, w0=-0.39557086423712484, w1=-0.3647964528880291\n",
      "Logistic regression iter. 28/29: loss=-3686.6639392455804, w0=-0.4060228414814928, w1=-0.3743638522810139\n",
      "Logistic regression iter. 29/29: loss=-3426.063178823803, w0=-0.4164026686499925, w1=-0.3838702201088253\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-59193.837640854705, w0=-0.024439919999999997, w1=-0.023479868162262506\n",
      "Logistic regression iter. 1/29: loss=-49775.08074494744, w0=-0.04684364588293151, w1=-0.0447893881698648\n",
      "Logistic regression iter. 2/29: loss=-42515.956014607174, w0=-0.06753446328634655, w1=-0.06429162730505904\n",
      "Logistic regression iter. 3/29: loss=-36780.04621453514, w0=-0.08680731645362354, w1=-0.08231902869477932\n",
      "Logistic regression iter. 4/29: loss=-32146.002331606716, w0=-0.10490260291110176, w1=-0.09913971259533363\n",
      "Logistic regression iter. 5/29: loss=-28331.776643128644, w0=-0.12201096300034148, w1=-0.11496335162552232\n",
      "Logistic regression iter. 6/29: loss=-25143.391937913642, w0=-0.13828365611274612, w1=-0.12995392969855088\n",
      "Logistic regression iter. 7/29: loss=-22443.53318586731, w0=-0.15384183353218867, w1=-0.144240975920188\n",
      "Logistic regression iter. 8/29: loss=-20132.363948528167, w0=-0.16878368840710728, w1=-0.15792809938588603\n",
      "Logistic regression iter. 9/29: loss=-18135.55572870397, w0=-0.1831897626458481, w1=-0.17109923668318944\n",
      "Logistic regression iter. 10/29: loss=-16396.608766432128, w0=-0.19712686523972822, w1=-0.18382320538476946\n",
      "Logistic regression iter. 11/29: loss=-14871.786789241405, w0=-0.21065098465043136, w1=-0.1961570477530323\n",
      "Logistic regression iter. 12/29: loss=-13526.68784332194, w0=-0.22380947761124279, w1=-0.20814851520548852\n",
      "Logistic regression iter. 13/29: loss=-12333.867096409256, w0=-0.23664273475015848, w1=-0.21983793869611723\n",
      "Logistic regression iter. 14/29: loss=-11271.153338921808, w0=-0.24918546410282336, w1=-0.231259655409865\n",
      "Logistic regression iter. 15/29: loss=-10320.43365441867, w0=-0.26146769220057253, w1=-0.24244311082660833\n",
      "Logistic regression iter. 16/29: loss=-9466.76078598561, w0=-0.2735155538204229, w1=-0.2534137201782279\n",
      "Logistic regression iter. 17/29: loss=-8697.687224745068, w0=-0.2853519216639152, w1=-0.264193549311933\n",
      "Logistic regression iter. 18/29: loss=-8002.761381050383, w0=-0.2969969133825293, w1=-0.2748018583639715\n",
      "Logistic regression iter. 19/29: loss=-7373.14147220455, w0=-0.3084683035874266, w1=-0.2852555400278555\n",
      "Logistic regression iter. 20/29: loss=-6801.296144388212, w0=-0.3197818614950966, w1=-0.2955694759728812\n",
      "Logistic regression iter. 21/29: loss=-6280.7698476305595, w0=-0.3309516298115149, w1=-0.30575682907093255\n",
      "Logistic regression iter. 22/29: loss=-5805.997140582932, w0=-0.34199015676652966, w1=-0.3158292848118998\n",
      "Logistic regression iter. 23/29: loss=-5372.154381492054, w0=-0.35290869048220475, w1=-0.3257972521499732\n",
      "Logistic regression iter. 24/29: loss=-4975.040279549384, w0=-0.36371734282126683, w1=-0.3356700316959669\n",
      "Logistic regression iter. 25/29: loss=-4610.978937546836, w0=-0.37442522832467345, w1=-0.3454559574273346\n",
      "Logistic regression iter. 26/29: loss=-4276.740577463006, w0=-0.3850405826767653, w1=-0.355162516768601\n",
      "Logistic regression iter. 27/29: loss=-3969.4762830953014, w0=-0.39557086423712484, w1=-0.3647964528880291\n",
      "Logistic regression iter. 28/29: loss=-3686.6639392455804, w0=-0.4060228414814928, w1=-0.3743638522810139\n",
      "Logistic regression iter. 29/29: loss=-3426.063178823803, w0=-0.4164026686499925, w1=-0.3838702201088253\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-59193.837640854705, w0=-0.024439919999999997, w1=-0.023479868162262506\n",
      "Logistic regression iter. 1/29: loss=-49775.08074494744, w0=-0.04684364588293151, w1=-0.0447893881698648\n",
      "Logistic regression iter. 2/29: loss=-42515.956014607174, w0=-0.06753446328634655, w1=-0.06429162730505904\n",
      "Logistic regression iter. 3/29: loss=-36780.04621453514, w0=-0.08680731645362354, w1=-0.08231902869477932\n",
      "Logistic regression iter. 4/29: loss=-32146.002331606716, w0=-0.10490260291110176, w1=-0.09913971259533363\n",
      "Logistic regression iter. 5/29: loss=-28331.776643128644, w0=-0.12201096300034148, w1=-0.11496335162552232\n",
      "Logistic regression iter. 6/29: loss=-25143.391937913642, w0=-0.13828365611274612, w1=-0.12995392969855088\n",
      "Logistic regression iter. 7/29: loss=-22443.53318586731, w0=-0.15384183353218867, w1=-0.144240975920188\n",
      "Logistic regression iter. 8/29: loss=-20132.363948528167, w0=-0.16878368840710728, w1=-0.15792809938588603\n",
      "Logistic regression iter. 9/29: loss=-18135.55572870397, w0=-0.1831897626458481, w1=-0.17109923668318944\n",
      "Logistic regression iter. 10/29: loss=-16396.608766432128, w0=-0.19712686523972822, w1=-0.18382320538476946\n",
      "Logistic regression iter. 11/29: loss=-14871.786789241405, w0=-0.21065098465043136, w1=-0.1961570477530323\n",
      "Logistic regression iter. 12/29: loss=-13526.68784332194, w0=-0.22380947761124279, w1=-0.20814851520548852\n",
      "Logistic regression iter. 13/29: loss=-12333.867096409256, w0=-0.23664273475015848, w1=-0.21983793869611723\n",
      "Logistic regression iter. 14/29: loss=-11271.153338921808, w0=-0.24918546410282336, w1=-0.231259655409865\n",
      "Logistic regression iter. 15/29: loss=-10320.43365441867, w0=-0.26146769220057253, w1=-0.24244311082660833\n",
      "Logistic regression iter. 16/29: loss=-9466.76078598561, w0=-0.2735155538204229, w1=-0.2534137201782279\n",
      "Logistic regression iter. 17/29: loss=-8697.687224745068, w0=-0.2853519216639152, w1=-0.264193549311933\n",
      "Logistic regression iter. 18/29: loss=-8002.761381050383, w0=-0.2969969133825293, w1=-0.2748018583639715\n",
      "Logistic regression iter. 19/29: loss=-7373.14147220455, w0=-0.3084683035874266, w1=-0.2852555400278555\n",
      "Logistic regression iter. 20/29: loss=-6801.296144388212, w0=-0.3197818614950966, w1=-0.2955694759728812\n",
      "Logistic regression iter. 21/29: loss=-6280.7698476305595, w0=-0.3309516298115149, w1=-0.30575682907093255\n",
      "Logistic regression iter. 22/29: loss=-5805.997140582932, w0=-0.34199015676652966, w1=-0.3158292848118998\n",
      "Logistic regression iter. 23/29: loss=-5372.154381492054, w0=-0.35290869048220475, w1=-0.3257972521499732\n",
      "Logistic regression iter. 24/29: loss=-4975.040279549384, w0=-0.36371734282126683, w1=-0.3356700316959669\n",
      "Logistic regression iter. 25/29: loss=-4610.978937546836, w0=-0.37442522832467345, w1=-0.3454559574273346\n",
      "Logistic regression iter. 26/29: loss=-4276.740577463006, w0=-0.3850405826767653, w1=-0.355162516768601\n",
      "Logistic regression iter. 27/29: loss=-3969.4762830953014, w0=-0.39557086423712484, w1=-0.3647964528880291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 28/29: loss=-3686.6639392455804, w0=-0.4060228414814928, w1=-0.3743638522810139\n",
      "Logistic regression iter. 29/29: loss=-3426.063178823803, w0=-0.4164026686499925, w1=-0.3838702201088253\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-52019.40705437804, w0=-0.0407332, w1=-0.03913311360377084\n",
      "Logistic regression iter. 1/29: loss=-39745.64136432439, w0=-0.0759130173572517, w1=-0.07236661535315811\n",
      "Logistic regression iter. 2/29: loss=-31485.12595253955, w0=-0.10708627171301015, w1=-0.10143701645426138\n",
      "Logistic regression iter. 3/29: loss=-25568.935435655243, w0=-0.13535081862221854, w1=-0.12755970533032104\n",
      "Logistic regression iter. 4/29: loss=-21139.153516292823, w0=-0.16143197379430704, w1=-0.15151895013881778\n",
      "Logistic regression iter. 5/29: loss=-17713.071005969585, w0=-0.1858244863038502, w1=-0.1738376969646669\n",
      "Logistic regression iter. 6/29: loss=-14997.701594755796, w0=-0.20887952058207454, w1=-0.19487954592188772\n",
      "Logistic regression iter. 7/29: loss=-12804.356682335307, w0=-0.23085507035939304, w1=-0.2149067030632555\n",
      "Logistic regression iter. 8/29: loss=-11005.706864328266, w0=-0.2519461343817131, w1=-0.23411403958123816\n",
      "Logistic regression iter. 9/29: loss=-9512.54046738297, w0=-0.2723035684484487, w1=-0.25265001406333176\n",
      "Logistic regression iter. 10/29: loss=-8260.3975475112, w0=-0.29204634458756196, w1=-0.27063006756558783\n",
      "Logistic regression iter. 11/29: loss=-7201.490445978775, w0=-0.31126980778099594, w1=-0.28814551612862344\n",
      "Logistic regression iter. 12/29: loss=-6299.613004596397, w0=-0.33005140819183587, w1=-0.30526964211834906\n",
      "Logistic regression iter. 13/29: loss=-5526.818490237774, w0=-0.34845478650540956, w1=-0.3220619814391044\n",
      "Logistic regression iter. 14/29: loss=-4861.185706687283, w0=-0.3665327527286902, w1=-0.33857141297566906\n",
      "Logistic regression iter. 15/29: loss=-4285.2773027956655, w0=-0.3843295020094113, w1=-0.3548384313645576\n",
      "Logistic regression iter. 16/29: loss=-3785.051309111786, w0=-0.4018822922194345, w1=-0.37089684970143616\n",
      "Logistic regression iter. 17/29: loss=-3349.077052112823, w0=-0.41922273407370836, w1=-0.3867750959390557\n",
      "Logistic regression iter. 18/29: loss=-2967.96011360145, w0=-0.43637779722554065, w1=-0.40249721424558527\n",
      "Logistic regression iter. 19/29: loss=-2633.9137578442705, w0=-0.4533706047461089, w1=-0.41808364850528745\n",
      "Logistic regression iter. 20/29: loss=-2340.4348387163877, w0=-0.4702210675982635, w1=-0.4335518625019002\n",
      "Logistic regression iter. 21/29: loss=-2082.055454044506, w0=-0.48694639649810567, w1=-0.44891683597913706\n",
      "Logistic regression iter. 22/29: loss=-1854.150332152392, w0=-0.5035615186645912, w1=-0.4641914651792465\n",
      "Logistic regression iter. 23/29: loss=-1652.7857813723397, w0=-0.5200794199598057, w1=-0.47938688902477006\n",
      "Logistic regression iter. 24/29: loss=-1474.600022657189, w0=-0.5365114278988554, w1=-0.49451275680901113\n",
      "Logistic regression iter. 25/29: loss=-1316.7074919135957, w0=-0.5528674473520635, w1=-0.5095774494302385\n",
      "Logistic regression iter. 26/29: loss=-1176.6216455199628, w0=-0.5691561580673238, w1=-0.5245882633999606\n",
      "Logistic regression iter. 27/29: loss=-1052.1921911742938, w0=-0.585385181130859, w1=-0.5395515647772148\n",
      "Logistic regression iter. 28/29: loss=-941.5536692105801, w0=-0.6015612199695871, w1=-0.5544729186233539\n",
      "Logistic regression iter. 29/29: loss=-843.0830423880643, w0=-0.6176901803444076, w1=-0.5693571983924779\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-52019.40705437804, w0=-0.0407332, w1=-0.03913311360377084\n",
      "Logistic regression iter. 1/29: loss=-39745.64136432439, w0=-0.0759130173572517, w1=-0.07236661535315811\n",
      "Logistic regression iter. 2/29: loss=-31485.12595253955, w0=-0.10708627171301015, w1=-0.10143701645426138\n",
      "Logistic regression iter. 3/29: loss=-25568.935435655243, w0=-0.13535081862221854, w1=-0.12755970533032104\n",
      "Logistic regression iter. 4/29: loss=-21139.153516292823, w0=-0.16143197379430704, w1=-0.15151895013881778\n",
      "Logistic regression iter. 5/29: loss=-17713.071005969585, w0=-0.1858244863038502, w1=-0.1738376969646669\n",
      "Logistic regression iter. 6/29: loss=-14997.701594755796, w0=-0.20887952058207454, w1=-0.19487954592188772\n",
      "Logistic regression iter. 7/29: loss=-12804.356682335307, w0=-0.23085507035939304, w1=-0.2149067030632555\n",
      "Logistic regression iter. 8/29: loss=-11005.706864328266, w0=-0.2519461343817131, w1=-0.23411403958123816\n",
      "Logistic regression iter. 9/29: loss=-9512.54046738297, w0=-0.2723035684484487, w1=-0.25265001406333176\n",
      "Logistic regression iter. 10/29: loss=-8260.3975475112, w0=-0.29204634458756196, w1=-0.27063006756558783\n",
      "Logistic regression iter. 11/29: loss=-7201.490445978775, w0=-0.31126980778099594, w1=-0.28814551612862344\n",
      "Logistic regression iter. 12/29: loss=-6299.613004596397, w0=-0.33005140819183587, w1=-0.30526964211834906\n",
      "Logistic regression iter. 13/29: loss=-5526.818490237774, w0=-0.34845478650540956, w1=-0.3220619814391044\n",
      "Logistic regression iter. 14/29: loss=-4861.185706687283, w0=-0.3665327527286902, w1=-0.33857141297566906\n",
      "Logistic regression iter. 15/29: loss=-4285.2773027956655, w0=-0.3843295020094113, w1=-0.3548384313645576\n",
      "Logistic regression iter. 16/29: loss=-3785.051309111786, w0=-0.4018822922194345, w1=-0.37089684970143616\n",
      "Logistic regression iter. 17/29: loss=-3349.077052112823, w0=-0.41922273407370836, w1=-0.3867750959390557\n",
      "Logistic regression iter. 18/29: loss=-2967.96011360145, w0=-0.43637779722554065, w1=-0.40249721424558527\n",
      "Logistic regression iter. 19/29: loss=-2633.9137578442705, w0=-0.4533706047461089, w1=-0.41808364850528745\n",
      "Logistic regression iter. 20/29: loss=-2340.4348387163877, w0=-0.4702210675982635, w1=-0.4335518625019002\n",
      "Logistic regression iter. 21/29: loss=-2082.055454044506, w0=-0.48694639649810567, w1=-0.44891683597913706\n",
      "Logistic regression iter. 22/29: loss=-1854.150332152392, w0=-0.5035615186645912, w1=-0.4641914651792465\n",
      "Logistic regression iter. 23/29: loss=-1652.7857813723397, w0=-0.5200794199598057, w1=-0.47938688902477006\n",
      "Logistic regression iter. 24/29: loss=-1474.600022657189, w0=-0.5365114278988554, w1=-0.49451275680901113\n",
      "Logistic regression iter. 25/29: loss=-1316.7074919135957, w0=-0.5528674473520635, w1=-0.5095774494302385\n",
      "Logistic regression iter. 26/29: loss=-1176.6216455199628, w0=-0.5691561580673238, w1=-0.5245882633999606\n",
      "Logistic regression iter. 27/29: loss=-1052.1921911742938, w0=-0.585385181130859, w1=-0.5395515647772148\n",
      "Logistic regression iter. 28/29: loss=-941.5536692105801, w0=-0.6015612199695871, w1=-0.5544729186233539\n",
      "Logistic regression iter. 29/29: loss=-843.0830423880643, w0=-0.6176901803444076, w1=-0.5693571983924779\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-52019.40705437804, w0=-0.0407332, w1=-0.03913311360377084\n",
      "Logistic regression iter. 1/29: loss=-39745.64136432439, w0=-0.0759130173572517, w1=-0.07236661535315811\n",
      "Logistic regression iter. 2/29: loss=-31485.12595253955, w0=-0.10708627171301015, w1=-0.10143701645426138\n",
      "Logistic regression iter. 3/29: loss=-25568.935435655243, w0=-0.13535081862221854, w1=-0.12755970533032104\n",
      "Logistic regression iter. 4/29: loss=-21139.153516292823, w0=-0.16143197379430704, w1=-0.15151895013881778\n",
      "Logistic regression iter. 5/29: loss=-17713.071005969585, w0=-0.1858244863038502, w1=-0.1738376969646669\n",
      "Logistic regression iter. 6/29: loss=-14997.701594755796, w0=-0.20887952058207454, w1=-0.19487954592188772\n",
      "Logistic regression iter. 7/29: loss=-12804.356682335307, w0=-0.23085507035939304, w1=-0.2149067030632555\n",
      "Logistic regression iter. 8/29: loss=-11005.706864328266, w0=-0.2519461343817131, w1=-0.23411403958123816\n",
      "Logistic regression iter. 9/29: loss=-9512.54046738297, w0=-0.2723035684484487, w1=-0.25265001406333176\n",
      "Logistic regression iter. 10/29: loss=-8260.3975475112, w0=-0.29204634458756196, w1=-0.27063006756558783\n",
      "Logistic regression iter. 11/29: loss=-7201.490445978775, w0=-0.31126980778099594, w1=-0.28814551612862344\n",
      "Logistic regression iter. 12/29: loss=-6299.613004596397, w0=-0.33005140819183587, w1=-0.30526964211834906\n",
      "Logistic regression iter. 13/29: loss=-5526.818490237774, w0=-0.34845478650540956, w1=-0.3220619814391044\n",
      "Logistic regression iter. 14/29: loss=-4861.185706687283, w0=-0.3665327527286902, w1=-0.33857141297566906\n",
      "Logistic regression iter. 15/29: loss=-4285.2773027956655, w0=-0.3843295020094113, w1=-0.3548384313645576\n",
      "Logistic regression iter. 16/29: loss=-3785.051309111786, w0=-0.4018822922194345, w1=-0.37089684970143616\n",
      "Logistic regression iter. 17/29: loss=-3349.077052112823, w0=-0.41922273407370836, w1=-0.3867750959390557\n",
      "Logistic regression iter. 18/29: loss=-2967.96011360145, w0=-0.43637779722554065, w1=-0.40249721424558527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 19/29: loss=-2633.9137578442705, w0=-0.4533706047461089, w1=-0.41808364850528745\n",
      "Logistic regression iter. 20/29: loss=-2340.4348387163877, w0=-0.4702210675982635, w1=-0.4335518625019002\n",
      "Logistic regression iter. 21/29: loss=-2082.055454044506, w0=-0.48694639649810567, w1=-0.44891683597913706\n",
      "Logistic regression iter. 22/29: loss=-1854.150332152392, w0=-0.5035615186645912, w1=-0.4641914651792465\n",
      "Logistic regression iter. 23/29: loss=-1652.7857813723397, w0=-0.5200794199598057, w1=-0.47938688902477006\n",
      "Logistic regression iter. 24/29: loss=-1474.600022657189, w0=-0.5365114278988554, w1=-0.49451275680901113\n",
      "Logistic regression iter. 25/29: loss=-1316.7074919135957, w0=-0.5528674473520635, w1=-0.5095774494302385\n",
      "Logistic regression iter. 26/29: loss=-1176.6216455199628, w0=-0.5691561580673238, w1=-0.5245882633999606\n",
      "Logistic regression iter. 27/29: loss=-1052.1921911742938, w0=-0.585385181130859, w1=-0.5395515647772148\n",
      "Logistic regression iter. 28/29: loss=-941.5536692105801, w0=-0.6015612199695871, w1=-0.5544729186233539\n",
      "Logistic regression iter. 29/29: loss=-843.0830423880643, w0=-0.6176901803444076, w1=-0.5693571983924779\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-52019.40705437804, w0=-0.0407332, w1=-0.03913311360377084\n",
      "Logistic regression iter. 1/29: loss=-39745.64136432439, w0=-0.0759130173572517, w1=-0.07236661535315811\n",
      "Logistic regression iter. 2/29: loss=-31485.12595253955, w0=-0.10708627171301015, w1=-0.10143701645426138\n",
      "Logistic regression iter. 3/29: loss=-25568.935435655243, w0=-0.13535081862221854, w1=-0.12755970533032104\n",
      "Logistic regression iter. 4/29: loss=-21139.153516292823, w0=-0.16143197379430704, w1=-0.15151895013881778\n",
      "Logistic regression iter. 5/29: loss=-17713.071005969585, w0=-0.1858244863038502, w1=-0.1738376969646669\n",
      "Logistic regression iter. 6/29: loss=-14997.701594755796, w0=-0.20887952058207454, w1=-0.19487954592188772\n",
      "Logistic regression iter. 7/29: loss=-12804.356682335307, w0=-0.23085507035939304, w1=-0.2149067030632555\n",
      "Logistic regression iter. 8/29: loss=-11005.706864328266, w0=-0.2519461343817131, w1=-0.23411403958123816\n",
      "Logistic regression iter. 9/29: loss=-9512.54046738297, w0=-0.2723035684484487, w1=-0.25265001406333176\n",
      "Logistic regression iter. 10/29: loss=-8260.3975475112, w0=-0.29204634458756196, w1=-0.27063006756558783\n",
      "Logistic regression iter. 11/29: loss=-7201.490445978775, w0=-0.31126980778099594, w1=-0.28814551612862344\n",
      "Logistic regression iter. 12/29: loss=-6299.613004596397, w0=-0.33005140819183587, w1=-0.30526964211834906\n",
      "Logistic regression iter. 13/29: loss=-5526.818490237774, w0=-0.34845478650540956, w1=-0.3220619814391044\n",
      "Logistic regression iter. 14/29: loss=-4861.185706687283, w0=-0.3665327527286902, w1=-0.33857141297566906\n",
      "Logistic regression iter. 15/29: loss=-4285.2773027956655, w0=-0.3843295020094113, w1=-0.3548384313645576\n",
      "Logistic regression iter. 16/29: loss=-3785.051309111786, w0=-0.4018822922194345, w1=-0.37089684970143616\n",
      "Logistic regression iter. 17/29: loss=-3349.077052112823, w0=-0.41922273407370836, w1=-0.3867750959390557\n",
      "Logistic regression iter. 18/29: loss=-2967.96011360145, w0=-0.43637779722554065, w1=-0.40249721424558527\n",
      "Logistic regression iter. 19/29: loss=-2633.9137578442705, w0=-0.4533706047461089, w1=-0.41808364850528745\n",
      "Logistic regression iter. 20/29: loss=-2340.4348387163877, w0=-0.4702210675982635, w1=-0.4335518625019002\n",
      "Logistic regression iter. 21/29: loss=-2082.055454044506, w0=-0.48694639649810567, w1=-0.44891683597913706\n",
      "Logistic regression iter. 22/29: loss=-1854.150332152392, w0=-0.5035615186645912, w1=-0.4641914651792465\n",
      "Logistic regression iter. 23/29: loss=-1652.7857813723397, w0=-0.5200794199598057, w1=-0.47938688902477006\n",
      "Logistic regression iter. 24/29: loss=-1474.600022657189, w0=-0.5365114278988554, w1=-0.49451275680901113\n",
      "Logistic regression iter. 25/29: loss=-1316.7074919135957, w0=-0.5528674473520635, w1=-0.5095774494302385\n",
      "Logistic regression iter. 26/29: loss=-1176.6216455199628, w0=-0.5691561580673238, w1=-0.5245882633999606\n",
      "Logistic regression iter. 27/29: loss=-1052.1921911742938, w0=-0.585385181130859, w1=-0.5395515647772148\n",
      "Logistic regression iter. 28/29: loss=-941.5536692105801, w0=-0.6015612199695871, w1=-0.5544729186233539\n",
      "Logistic regression iter. 29/29: loss=-843.0830423880643, w0=-0.6176901803444076, w1=-0.5693571983924779\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-45692.09283358659, w0=-0.05702647999999999, w1=-0.05478635904527918\n",
      "Logistic regression iter. 1/29: loss=-32184.02836384989, w0=-0.10343977608401092, w1=-0.09834557016781398\n",
      "Logistic regression iter. 2/29: loss=-23992.506194783622, w0=-0.14334706141249892, w1=-0.13525478849599154\n",
      "Logistic regression iter. 3/29: loss=-18515.7879659924, w0=-0.1790128128822412, w1=-0.1679687539863441\n",
      "Logistic regression iter. 4/29: loss=-14624.642942381925, w0=-0.2117210010273044, w1=-0.19783581136821912\n",
      "Logistic regression iter. 5/29: loss=-11744.286523709647, w0=-0.24226867191588014, w1=-0.22567005795160122\n",
      "Logistic regression iter. 6/29: loss=-9548.661751744121, w0=-0.2711853955702702, w1=-0.25199954719503326\n",
      "Logistic regression iter. 7/29: loss=-7837.969726784717, w0=-0.2988409370049333, w1=-0.27718529441831463\n",
      "Logistic regression iter. 8/29: loss=-6482.505209116671, w0=-0.325503280853461, w1=-0.3014840810630689\n",
      "Logistic regression iter. 9/29: loss=-5394.207099071152, w0=-0.35137230598656277, w1=-0.3250842181853914\n",
      "Logistic regression iter. 10/29: loss=-4511.110870932312, w0=-0.3766005048232228, w1=-0.3481271740697073\n",
      "Logistic regression iter. 11/29: loss=-3788.3264494291116, w0=-0.4013063452862308, w1=-0.3707213024819384\n",
      "Logistic regression iter. 12/29: loss=-3192.5410509330977, w0=-0.4255832271535748, w1=-0.39295091036831464\n",
      "Logistic regression iter. 13/29: loss=-2698.5310983265745, w0=-0.44950568207242564, w1=-0.4148824493732538\n",
      "Logistic regression iter. 14/29: loss=-2286.871524296862, w0=-0.47313378419724844, w1=-0.4365688638179542\n",
      "Logistic regression iter. 15/29: loss=-1942.3862547792578, w0=-0.4965163618974804, w1=-0.45805271812200077\n",
      "Logistic regression iter. 16/29: loss=-1653.072695104922, w0=-0.5196933838545567, w1=-0.47936849315644886\n",
      "Logistic regression iter. 17/29: loss=-1409.3381097490706, w0=-0.5426977628412758, w1=-0.5005443027025751\n",
      "Logistic regression iter. 18/29: loss=-1203.4464551449223, w0=-0.5655567400156982, w1=-0.5216031964665339\n",
      "Logistic regression iter. 19/29: loss=-1029.1104389943582, w0=-0.5882929613192416, w1=-0.5425641626524281\n",
      "Logistic regression iter. 20/29: loss=-881.1858371282218, w0=-0.6109253240901017, w1=-0.563442908486405\n",
      "Logistic regression iter. 21/29: loss=-755.4391384968238, w0=-0.6334696496240482, w1=-0.5842524741420398\n",
      "Logistic regression iter. 22/29: loss=-648.3686529393315, w0=-0.6559392221422562, w1=-0.6050037199842182\n",
      "Logistic regression iter. 23/29: loss=-557.065192416052, w0=-0.6783452240048862, w1=-0.6257057163279451\n",
      "Logistic regression iter. 24/29: loss=-479.1024524836669, w0=-0.7006970894940575, w1=-0.6463660573778539\n",
      "Logistic regression iter. 25/29: loss=-412.4499674675671, w0=-0.7230027940869685, w1=-0.6669911156382399\n",
      "Logistic regression iter. 26/29: loss=-355.40342192887516, w0=-0.7452690921978598, w1=-0.6875862491881605\n",
      "Logistic regression iter. 27/29: loss=-306.528448122077, w0=-0.7675017134517647, w1=-0.7081559713546791\n",
      "Logistic regression iter. 28/29: loss=-264.61500309526957, w0=-0.7897055253688818, w1=-0.7287040901885031\n",
      "Logistic regression iter. 29/29: loss=-228.64011801268558, w0=-0.8118846686830582, w1=-0.7492338235438566\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-45692.09283358659, w0=-0.05702647999999999, w1=-0.05478635904527918\n",
      "Logistic regression iter. 1/29: loss=-32184.02836384989, w0=-0.10343977608401092, w1=-0.09834557016781398\n",
      "Logistic regression iter. 2/29: loss=-23992.506194783622, w0=-0.14334706141249892, w1=-0.13525478849599154\n",
      "Logistic regression iter. 3/29: loss=-18515.7879659924, w0=-0.1790128128822412, w1=-0.1679687539863441\n",
      "Logistic regression iter. 4/29: loss=-14624.642942381925, w0=-0.2117210010273044, w1=-0.19783581136821912\n",
      "Logistic regression iter. 5/29: loss=-11744.286523709647, w0=-0.24226867191588014, w1=-0.22567005795160122\n",
      "Logistic regression iter. 6/29: loss=-9548.661751744121, w0=-0.2711853955702702, w1=-0.25199954719503326\n",
      "Logistic regression iter. 7/29: loss=-7837.969726784717, w0=-0.2988409370049333, w1=-0.27718529441831463\n",
      "Logistic regression iter. 8/29: loss=-6482.505209116671, w0=-0.325503280853461, w1=-0.3014840810630689\n",
      "Logistic regression iter. 9/29: loss=-5394.207099071152, w0=-0.35137230598656277, w1=-0.3250842181853914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 10/29: loss=-4511.110870932312, w0=-0.3766005048232228, w1=-0.3481271740697073\n",
      "Logistic regression iter. 11/29: loss=-3788.3264494291116, w0=-0.4013063452862308, w1=-0.3707213024819384\n",
      "Logistic regression iter. 12/29: loss=-3192.5410509330977, w0=-0.4255832271535748, w1=-0.39295091036831464\n",
      "Logistic regression iter. 13/29: loss=-2698.5310983265745, w0=-0.44950568207242564, w1=-0.4148824493732538\n",
      "Logistic regression iter. 14/29: loss=-2286.871524296862, w0=-0.47313378419724844, w1=-0.4365688638179542\n",
      "Logistic regression iter. 15/29: loss=-1942.3862547792578, w0=-0.4965163618974804, w1=-0.45805271812200077\n",
      "Logistic regression iter. 16/29: loss=-1653.072695104922, w0=-0.5196933838545567, w1=-0.47936849315644886\n",
      "Logistic regression iter. 17/29: loss=-1409.3381097490706, w0=-0.5426977628412758, w1=-0.5005443027025751\n",
      "Logistic regression iter. 18/29: loss=-1203.4464551449223, w0=-0.5655567400156982, w1=-0.5216031964665339\n",
      "Logistic regression iter. 19/29: loss=-1029.1104389943582, w0=-0.5882929613192416, w1=-0.5425641626524281\n",
      "Logistic regression iter. 20/29: loss=-881.1858371282218, w0=-0.6109253240901017, w1=-0.563442908486405\n",
      "Logistic regression iter. 21/29: loss=-755.4391384968238, w0=-0.6334696496240482, w1=-0.5842524741420398\n",
      "Logistic regression iter. 22/29: loss=-648.3686529393315, w0=-0.6559392221422562, w1=-0.6050037199842182\n",
      "Logistic regression iter. 23/29: loss=-557.065192416052, w0=-0.6783452240048862, w1=-0.6257057163279451\n",
      "Logistic regression iter. 24/29: loss=-479.1024524836669, w0=-0.7006970894940575, w1=-0.6463660573778539\n",
      "Logistic regression iter. 25/29: loss=-412.4499674675671, w0=-0.7230027940869685, w1=-0.6669911156382399\n",
      "Logistic regression iter. 26/29: loss=-355.40342192887516, w0=-0.7452690921978598, w1=-0.6875862491881605\n",
      "Logistic regression iter. 27/29: loss=-306.528448122077, w0=-0.7675017134517647, w1=-0.7081559713546791\n",
      "Logistic regression iter. 28/29: loss=-264.61500309526957, w0=-0.7897055253688818, w1=-0.7287040901885031\n",
      "Logistic regression iter. 29/29: loss=-228.64011801268558, w0=-0.8118846686830582, w1=-0.7492338235438566\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-45692.09283358659, w0=-0.05702647999999999, w1=-0.05478635904527918\n",
      "Logistic regression iter. 1/29: loss=-32184.02836384989, w0=-0.10343977608401092, w1=-0.09834557016781398\n",
      "Logistic regression iter. 2/29: loss=-23992.506194783622, w0=-0.14334706141249892, w1=-0.13525478849599154\n",
      "Logistic regression iter. 3/29: loss=-18515.7879659924, w0=-0.1790128128822412, w1=-0.1679687539863441\n",
      "Logistic regression iter. 4/29: loss=-14624.642942381925, w0=-0.2117210010273044, w1=-0.19783581136821912\n",
      "Logistic regression iter. 5/29: loss=-11744.286523709647, w0=-0.24226867191588014, w1=-0.22567005795160122\n",
      "Logistic regression iter. 6/29: loss=-9548.661751744121, w0=-0.2711853955702702, w1=-0.25199954719503326\n",
      "Logistic regression iter. 7/29: loss=-7837.969726784717, w0=-0.2988409370049333, w1=-0.27718529441831463\n",
      "Logistic regression iter. 8/29: loss=-6482.505209116671, w0=-0.325503280853461, w1=-0.3014840810630689\n",
      "Logistic regression iter. 9/29: loss=-5394.207099071152, w0=-0.35137230598656277, w1=-0.3250842181853914\n",
      "Logistic regression iter. 10/29: loss=-4511.110870932312, w0=-0.3766005048232228, w1=-0.3481271740697073\n",
      "Logistic regression iter. 11/29: loss=-3788.3264494291116, w0=-0.4013063452862308, w1=-0.3707213024819384\n",
      "Logistic regression iter. 12/29: loss=-3192.5410509330977, w0=-0.4255832271535748, w1=-0.39295091036831464\n",
      "Logistic regression iter. 13/29: loss=-2698.5310983265745, w0=-0.44950568207242564, w1=-0.4148824493732538\n",
      "Logistic regression iter. 14/29: loss=-2286.871524296862, w0=-0.47313378419724844, w1=-0.4365688638179542\n",
      "Logistic regression iter. 15/29: loss=-1942.3862547792578, w0=-0.4965163618974804, w1=-0.45805271812200077\n",
      "Logistic regression iter. 16/29: loss=-1653.072695104922, w0=-0.5196933838545567, w1=-0.47936849315644886\n",
      "Logistic regression iter. 17/29: loss=-1409.3381097490706, w0=-0.5426977628412758, w1=-0.5005443027025751\n",
      "Logistic regression iter. 18/29: loss=-1203.4464551449223, w0=-0.5655567400156982, w1=-0.5216031964665339\n",
      "Logistic regression iter. 19/29: loss=-1029.1104389943582, w0=-0.5882929613192416, w1=-0.5425641626524281\n",
      "Logistic regression iter. 20/29: loss=-881.1858371282218, w0=-0.6109253240901017, w1=-0.563442908486405\n",
      "Logistic regression iter. 21/29: loss=-755.4391384968238, w0=-0.6334696496240482, w1=-0.5842524741420398\n",
      "Logistic regression iter. 22/29: loss=-648.3686529393315, w0=-0.6559392221422562, w1=-0.6050037199842182\n",
      "Logistic regression iter. 23/29: loss=-557.065192416052, w0=-0.6783452240048862, w1=-0.6257057163279451\n",
      "Logistic regression iter. 24/29: loss=-479.1024524836669, w0=-0.7006970894940575, w1=-0.6463660573778539\n",
      "Logistic regression iter. 25/29: loss=-412.4499674675671, w0=-0.7230027940869685, w1=-0.6669911156382399\n",
      "Logistic regression iter. 26/29: loss=-355.40342192887516, w0=-0.7452690921978598, w1=-0.6875862491881605\n",
      "Logistic regression iter. 27/29: loss=-306.528448122077, w0=-0.7675017134517647, w1=-0.7081559713546791\n",
      "Logistic regression iter. 28/29: loss=-264.61500309526957, w0=-0.7897055253688818, w1=-0.7287040901885031\n",
      "Logistic regression iter. 29/29: loss=-228.64011801268558, w0=-0.8118846686830582, w1=-0.7492338235438566\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-45692.09283358659, w0=-0.05702647999999999, w1=-0.05478635904527918\n",
      "Logistic regression iter. 1/29: loss=-32184.02836384989, w0=-0.10343977608401092, w1=-0.09834557016781398\n",
      "Logistic regression iter. 2/29: loss=-23992.506194783622, w0=-0.14334706141249892, w1=-0.13525478849599154\n",
      "Logistic regression iter. 3/29: loss=-18515.7879659924, w0=-0.1790128128822412, w1=-0.1679687539863441\n",
      "Logistic regression iter. 4/29: loss=-14624.642942381925, w0=-0.2117210010273044, w1=-0.19783581136821912\n",
      "Logistic regression iter. 5/29: loss=-11744.286523709647, w0=-0.24226867191588014, w1=-0.22567005795160122\n",
      "Logistic regression iter. 6/29: loss=-9548.661751744121, w0=-0.2711853955702702, w1=-0.25199954719503326\n",
      "Logistic regression iter. 7/29: loss=-7837.969726784717, w0=-0.2988409370049333, w1=-0.27718529441831463\n",
      "Logistic regression iter. 8/29: loss=-6482.505209116671, w0=-0.325503280853461, w1=-0.3014840810630689\n",
      "Logistic regression iter. 9/29: loss=-5394.207099071152, w0=-0.35137230598656277, w1=-0.3250842181853914\n",
      "Logistic regression iter. 10/29: loss=-4511.110870932312, w0=-0.3766005048232228, w1=-0.3481271740697073\n",
      "Logistic regression iter. 11/29: loss=-3788.3264494291116, w0=-0.4013063452862308, w1=-0.3707213024819384\n",
      "Logistic regression iter. 12/29: loss=-3192.5410509330977, w0=-0.4255832271535748, w1=-0.39295091036831464\n",
      "Logistic regression iter. 13/29: loss=-2698.5310983265745, w0=-0.44950568207242564, w1=-0.4148824493732538\n",
      "Logistic regression iter. 14/29: loss=-2286.871524296862, w0=-0.47313378419724844, w1=-0.4365688638179542\n",
      "Logistic regression iter. 15/29: loss=-1942.3862547792578, w0=-0.4965163618974804, w1=-0.45805271812200077\n",
      "Logistic regression iter. 16/29: loss=-1653.072695104922, w0=-0.5196933838545567, w1=-0.47936849315644886\n",
      "Logistic regression iter. 17/29: loss=-1409.3381097490706, w0=-0.5426977628412758, w1=-0.5005443027025751\n",
      "Logistic regression iter. 18/29: loss=-1203.4464551449223, w0=-0.5655567400156982, w1=-0.5216031964665339\n",
      "Logistic regression iter. 19/29: loss=-1029.1104389943582, w0=-0.5882929613192416, w1=-0.5425641626524281\n",
      "Logistic regression iter. 20/29: loss=-881.1858371282218, w0=-0.6109253240901017, w1=-0.563442908486405\n",
      "Logistic regression iter. 21/29: loss=-755.4391384968238, w0=-0.6334696496240482, w1=-0.5842524741420398\n",
      "Logistic regression iter. 22/29: loss=-648.3686529393315, w0=-0.6559392221422562, w1=-0.6050037199842182\n",
      "Logistic regression iter. 23/29: loss=-557.065192416052, w0=-0.6783452240048862, w1=-0.6257057163279451\n",
      "Logistic regression iter. 24/29: loss=-479.1024524836669, w0=-0.7006970894940575, w1=-0.6463660573778539\n",
      "Logistic regression iter. 25/29: loss=-412.4499674675671, w0=-0.7230027940869685, w1=-0.6669911156382399\n",
      "Logistic regression iter. 26/29: loss=-355.40342192887516, w0=-0.7452690921978598, w1=-0.6875862491881605\n",
      "Logistic regression iter. 27/29: loss=-306.528448122077, w0=-0.7675017134517647, w1=-0.7081559713546791\n",
      "Logistic regression iter. 28/29: loss=-264.61500309526957, w0=-0.7897055253688818, w1=-0.7287040901885031\n",
      "Logistic regression iter. 29/29: loss=-228.64011801268558, w0=-0.8118846686830582, w1=-0.7492338235438566\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-40131.58077465162, w0=-0.07331975999999998, w1=-0.07043960448678752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 1/29: loss=-26374.812823635522, w0=-0.12962358173872446, w1=-0.1229635156399213\n",
      "Logistic regression iter. 2/29: loss=-18674.25428678834, w0=-0.1770731288017027, w1=-0.16658337755167973\n",
      "Logistic regression iter. 3/29: loss=-13785.757249832091, w0=-0.21922253046059914, w1=-0.20507441561876627\n",
      "Logistic regression iter. 4/29: loss=-10452.099716719069, w0=-0.2578796066567813, w1=-0.24028629832120935\n",
      "Logistic regression iter. 5/29: loss=-8071.862352519257, w0=-0.29409812631701293, w1=-0.273264278580847\n",
      "Logistic regression iter. 6/29: loss=-6317.639635364862, w0=-0.3285475565413156, w1=-0.3046552684788699\n",
      "Logistic regression iter. 7/29: loss=-4994.8020018446305, w0=-0.3616787512352437, w1=-0.3348851843204744\n",
      "Logistic regression iter. 8/29: loss=-3980.0062657087333, w0=-0.39380825745449655, w1=-0.36424714488061805\n",
      "Logistic regression iter. 9/29: loss=-3191.2002168754193, w0=-0.42516529843462736, w1=-0.39294962848081433\n",
      "Logistic regression iter. 10/29: loss=-2571.695954718348, w0=-0.45591981323049596, w1=-0.421144685430492\n",
      "Logistic regression iter. 11/29: loss=-2081.1348700677495, w0=-0.48620011122108053, w1=-0.4489454010402897\n",
      "Logistic regression iter. 12/29: loss=-1690.0809573586919, w0=-0.5161044755830844, w1=-0.47643719227428705\n",
      "Logistic regression iter. 13/29: loss=-1376.6390895574843, w0=-0.5457090617096844, w1=-0.5036853829732365\n",
      "Logistic regression iter. 14/29: loss=-1124.2595021413788, w0=-0.5750734313833018, w1=-0.5307404364808659\n",
      "Logistic regression iter. 15/29: loss=-920.2662969721074, w0=-0.6042445247229261, w1=-0.5576416602880561\n",
      "Logistic regression iter. 16/29: loss=-754.8438109316891, w0=-0.6332595686620515, w1=-0.5844198834279432\n",
      "Logistic regression iter. 17/29: loss=-620.3216470168891, w0=-0.6621482427483498, w1=-0.6110994251406618\n",
      "Logistic regression iter. 18/29: loss=-510.65991745025013, w0=-0.6909343147697969, w1=-0.6376995635658904\n",
      "Logistic regression iter. 19/29: loss=-421.0719921344147, w0=-0.7196368906862572, w1=-0.6642356449214036\n",
      "Logistic regression iter. 20/29: loss=-347.74374037054935, w0=-0.7482713793783263, w1=-0.6907199298805475\n",
      "Logistic regression iter. 21/29: loss=-287.6217933428408, w0=-0.7768502435716903, w1=-0.7171622451099496\n",
      "Logistic regression iter. 22/29: loss=-238.2520209133383, w0=-0.8053835885126344, w1=-0.7435704885878868\n",
      "Logistic regression iter. 23/29: loss=-197.65509243925342, w0=-0.833879626263154, w1=-0.7699510240385651\n",
      "Logistic regression iter. 24/29: loss=-164.2297889059077, w0=-0.8623450438056586, w1=-0.7963089905189079\n",
      "Logistic regression iter. 25/29: loss=-136.67732469561687, w0=-0.8907852961963287, w1=-0.8226485465753629\n",
      "Logistic regression iter. 26/29: loss=-113.9417374637658, w0=-0.919204840937544, w1=-0.8489730636049755\n",
      "Logistic regression iter. 27/29: loss=-95.16267638644757, w0=-0.9476073259931833, w1=-0.8752852795516126\n",
      "Logistic regression iter. 28/29: loss=-79.63783151247478, w0=-0.9759957410676491, w1=-0.90158742147151\n",
      "Logistic regression iter. 29/29: loss=-66.79291093095632, w0=-1.0043725396500507, w1=-0.9278813035572122\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-40131.58077465162, w0=-0.07331975999999998, w1=-0.07043960448678752\n",
      "Logistic regression iter. 1/29: loss=-26374.812823635522, w0=-0.12962358173872446, w1=-0.1229635156399213\n",
      "Logistic regression iter. 2/29: loss=-18674.25428678834, w0=-0.1770731288017027, w1=-0.16658337755167973\n",
      "Logistic regression iter. 3/29: loss=-13785.757249832091, w0=-0.21922253046059914, w1=-0.20507441561876627\n",
      "Logistic regression iter. 4/29: loss=-10452.099716719069, w0=-0.2578796066567813, w1=-0.24028629832120935\n",
      "Logistic regression iter. 5/29: loss=-8071.862352519257, w0=-0.29409812631701293, w1=-0.273264278580847\n",
      "Logistic regression iter. 6/29: loss=-6317.639635364862, w0=-0.3285475565413156, w1=-0.3046552684788699\n",
      "Logistic regression iter. 7/29: loss=-4994.8020018446305, w0=-0.3616787512352437, w1=-0.3348851843204744\n",
      "Logistic regression iter. 8/29: loss=-3980.0062657087333, w0=-0.39380825745449655, w1=-0.36424714488061805\n",
      "Logistic regression iter. 9/29: loss=-3191.2002168754193, w0=-0.42516529843462736, w1=-0.39294962848081433\n",
      "Logistic regression iter. 10/29: loss=-2571.695954718348, w0=-0.45591981323049596, w1=-0.421144685430492\n",
      "Logistic regression iter. 11/29: loss=-2081.1348700677495, w0=-0.48620011122108053, w1=-0.4489454010402897\n",
      "Logistic regression iter. 12/29: loss=-1690.0809573586919, w0=-0.5161044755830844, w1=-0.47643719227428705\n",
      "Logistic regression iter. 13/29: loss=-1376.6390895574843, w0=-0.5457090617096844, w1=-0.5036853829732365\n",
      "Logistic regression iter. 14/29: loss=-1124.2595021413788, w0=-0.5750734313833018, w1=-0.5307404364808659\n",
      "Logistic regression iter. 15/29: loss=-920.2662969721074, w0=-0.6042445247229261, w1=-0.5576416602880561\n",
      "Logistic regression iter. 16/29: loss=-754.8438109316891, w0=-0.6332595686620515, w1=-0.5844198834279432\n",
      "Logistic regression iter. 17/29: loss=-620.3216470168891, w0=-0.6621482427483498, w1=-0.6110994251406618\n",
      "Logistic regression iter. 18/29: loss=-510.65991745025013, w0=-0.6909343147697969, w1=-0.6376995635658904\n",
      "Logistic regression iter. 19/29: loss=-421.0719921344147, w0=-0.7196368906862572, w1=-0.6642356449214036\n",
      "Logistic regression iter. 20/29: loss=-347.74374037054935, w0=-0.7482713793783263, w1=-0.6907199298805475\n",
      "Logistic regression iter. 21/29: loss=-287.6217933428408, w0=-0.7768502435716903, w1=-0.7171622451099496\n",
      "Logistic regression iter. 22/29: loss=-238.2520209133383, w0=-0.8053835885126344, w1=-0.7435704885878868\n",
      "Logistic regression iter. 23/29: loss=-197.65509243925342, w0=-0.833879626263154, w1=-0.7699510240385651\n",
      "Logistic regression iter. 24/29: loss=-164.2297889059077, w0=-0.8623450438056586, w1=-0.7963089905189079\n",
      "Logistic regression iter. 25/29: loss=-136.67732469561687, w0=-0.8907852961963287, w1=-0.8226485465753629\n",
      "Logistic regression iter. 26/29: loss=-113.9417374637658, w0=-0.919204840937544, w1=-0.8489730636049755\n",
      "Logistic regression iter. 27/29: loss=-95.16267638644757, w0=-0.9476073259931833, w1=-0.8752852795516126\n",
      "Logistic regression iter. 28/29: loss=-79.63783151247478, w0=-0.9759957410676491, w1=-0.90158742147151\n",
      "Logistic regression iter. 29/29: loss=-66.79291093095632, w0=-1.0043725396500507, w1=-0.9278813035572122\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-40131.58077465162, w0=-0.07331975999999998, w1=-0.07043960448678752\n",
      "Logistic regression iter. 1/29: loss=-26374.812823635522, w0=-0.12962358173872446, w1=-0.1229635156399213\n",
      "Logistic regression iter. 2/29: loss=-18674.25428678834, w0=-0.1770731288017027, w1=-0.16658337755167973\n",
      "Logistic regression iter. 3/29: loss=-13785.757249832091, w0=-0.21922253046059914, w1=-0.20507441561876627\n",
      "Logistic regression iter. 4/29: loss=-10452.099716719069, w0=-0.2578796066567813, w1=-0.24028629832120935\n",
      "Logistic regression iter. 5/29: loss=-8071.862352519257, w0=-0.29409812631701293, w1=-0.273264278580847\n",
      "Logistic regression iter. 6/29: loss=-6317.639635364862, w0=-0.3285475565413156, w1=-0.3046552684788699\n",
      "Logistic regression iter. 7/29: loss=-4994.8020018446305, w0=-0.3616787512352437, w1=-0.3348851843204744\n",
      "Logistic regression iter. 8/29: loss=-3980.0062657087333, w0=-0.39380825745449655, w1=-0.36424714488061805\n",
      "Logistic regression iter. 9/29: loss=-3191.2002168754193, w0=-0.42516529843462736, w1=-0.39294962848081433\n",
      "Logistic regression iter. 10/29: loss=-2571.695954718348, w0=-0.45591981323049596, w1=-0.421144685430492\n",
      "Logistic regression iter. 11/29: loss=-2081.1348700677495, w0=-0.48620011122108053, w1=-0.4489454010402897\n",
      "Logistic regression iter. 12/29: loss=-1690.0809573586919, w0=-0.5161044755830844, w1=-0.47643719227428705\n",
      "Logistic regression iter. 13/29: loss=-1376.6390895574843, w0=-0.5457090617096844, w1=-0.5036853829732365\n",
      "Logistic regression iter. 14/29: loss=-1124.2595021413788, w0=-0.5750734313833018, w1=-0.5307404364808659\n",
      "Logistic regression iter. 15/29: loss=-920.2662969721074, w0=-0.6042445247229261, w1=-0.5576416602880561\n",
      "Logistic regression iter. 16/29: loss=-754.8438109316891, w0=-0.6332595686620515, w1=-0.5844198834279432\n",
      "Logistic regression iter. 17/29: loss=-620.3216470168891, w0=-0.6621482427483498, w1=-0.6110994251406618\n",
      "Logistic regression iter. 18/29: loss=-510.65991745025013, w0=-0.6909343147697969, w1=-0.6376995635658904\n",
      "Logistic regression iter. 19/29: loss=-421.0719921344147, w0=-0.7196368906862572, w1=-0.6642356449214036\n",
      "Logistic regression iter. 20/29: loss=-347.74374037054935, w0=-0.7482713793783263, w1=-0.6907199298805475\n",
      "Logistic regression iter. 21/29: loss=-287.6217933428408, w0=-0.7768502435716903, w1=-0.7171622451099496\n",
      "Logistic regression iter. 22/29: loss=-238.2520209133383, w0=-0.8053835885126344, w1=-0.7435704885878868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 23/29: loss=-197.65509243925342, w0=-0.833879626263154, w1=-0.7699510240385651\n",
      "Logistic regression iter. 24/29: loss=-164.2297889059077, w0=-0.8623450438056586, w1=-0.7963089905189079\n",
      "Logistic regression iter. 25/29: loss=-136.67732469561687, w0=-0.8907852961963287, w1=-0.8226485465753629\n",
      "Logistic regression iter. 26/29: loss=-113.9417374637658, w0=-0.919204840937544, w1=-0.8489730636049755\n",
      "Logistic regression iter. 27/29: loss=-95.16267638644757, w0=-0.9476073259931833, w1=-0.8752852795516126\n",
      "Logistic regression iter. 28/29: loss=-79.63783151247478, w0=-0.9759957410676491, w1=-0.90158742147151\n",
      "Logistic regression iter. 29/29: loss=-66.79291093095632, w0=-1.0043725396500507, w1=-0.9278813035572122\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-40131.58077465162, w0=-0.07331975999999998, w1=-0.07043960448678752\n",
      "Logistic regression iter. 1/29: loss=-26374.812823635522, w0=-0.12962358173872446, w1=-0.1229635156399213\n",
      "Logistic regression iter. 2/29: loss=-18674.25428678834, w0=-0.1770731288017027, w1=-0.16658337755167973\n",
      "Logistic regression iter. 3/29: loss=-13785.757249832091, w0=-0.21922253046059914, w1=-0.20507441561876627\n",
      "Logistic regression iter. 4/29: loss=-10452.099716719069, w0=-0.2578796066567813, w1=-0.24028629832120935\n",
      "Logistic regression iter. 5/29: loss=-8071.862352519257, w0=-0.29409812631701293, w1=-0.273264278580847\n",
      "Logistic regression iter. 6/29: loss=-6317.639635364862, w0=-0.3285475565413156, w1=-0.3046552684788699\n",
      "Logistic regression iter. 7/29: loss=-4994.8020018446305, w0=-0.3616787512352437, w1=-0.3348851843204744\n",
      "Logistic regression iter. 8/29: loss=-3980.0062657087333, w0=-0.39380825745449655, w1=-0.36424714488061805\n",
      "Logistic regression iter. 9/29: loss=-3191.2002168754193, w0=-0.42516529843462736, w1=-0.39294962848081433\n",
      "Logistic regression iter. 10/29: loss=-2571.695954718348, w0=-0.45591981323049596, w1=-0.421144685430492\n",
      "Logistic regression iter. 11/29: loss=-2081.1348700677495, w0=-0.48620011122108053, w1=-0.4489454010402897\n",
      "Logistic regression iter. 12/29: loss=-1690.0809573586919, w0=-0.5161044755830844, w1=-0.47643719227428705\n",
      "Logistic regression iter. 13/29: loss=-1376.6390895574843, w0=-0.5457090617096844, w1=-0.5036853829732365\n",
      "Logistic regression iter. 14/29: loss=-1124.2595021413788, w0=-0.5750734313833018, w1=-0.5307404364808659\n",
      "Logistic regression iter. 15/29: loss=-920.2662969721074, w0=-0.6042445247229261, w1=-0.5576416602880561\n",
      "Logistic regression iter. 16/29: loss=-754.8438109316891, w0=-0.6332595686620515, w1=-0.5844198834279432\n",
      "Logistic regression iter. 17/29: loss=-620.3216470168891, w0=-0.6621482427483498, w1=-0.6110994251406618\n",
      "Logistic regression iter. 18/29: loss=-510.65991745025013, w0=-0.6909343147697969, w1=-0.6376995635658904\n",
      "Logistic regression iter. 19/29: loss=-421.0719921344147, w0=-0.7196368906862572, w1=-0.6642356449214036\n",
      "Logistic regression iter. 20/29: loss=-347.74374037054935, w0=-0.7482713793783263, w1=-0.6907199298805475\n",
      "Logistic regression iter. 21/29: loss=-287.6217933428408, w0=-0.7768502435716903, w1=-0.7171622451099496\n",
      "Logistic regression iter. 22/29: loss=-238.2520209133383, w0=-0.8053835885126344, w1=-0.7435704885878868\n",
      "Logistic regression iter. 23/29: loss=-197.65509243925342, w0=-0.833879626263154, w1=-0.7699510240385651\n",
      "Logistic regression iter. 24/29: loss=-164.2297889059077, w0=-0.8623450438056586, w1=-0.7963089905189079\n",
      "Logistic regression iter. 25/29: loss=-136.67732469561687, w0=-0.8907852961963287, w1=-0.8226485465753629\n",
      "Logistic regression iter. 26/29: loss=-113.9417374637658, w0=-0.919204840937544, w1=-0.8489730636049755\n",
      "Logistic regression iter. 27/29: loss=-95.16267638644757, w0=-0.9476073259931833, w1=-0.8752852795516126\n",
      "Logistic regression iter. 28/29: loss=-79.63783151247478, w0=-0.9759957410676491, w1=-0.90158742147151\n",
      "Logistic regression iter. 29/29: loss=-66.79291093095632, w0=-1.0043725396500507, w1=-0.9278813035572122\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-35255.19436149345, w0=-0.08961303999999998, w1=-0.08609284992829584\n",
      "Logistic regression iter. 1/29: loss=-21827.962660096528, w0=-0.15467003227361603, w1=-0.14645693942327676\n",
      "Logistic regression iter. 2/29: loss=-14766.042447935963, w0=-0.20885018007558093, w1=-0.196053446717477\n",
      "Logistic regression iter. 3/29: loss=-10466.618656523651, w0=-0.2569468062530283, w1=-0.23987776500347438\n",
      "Logistic regression iter. 4/29: loss=-7636.034953763296, w0=-0.3012159122301108, w1=-0.2801851651021257\n",
      "Logistic regression iter. 5/29: loss=-5680.046014483774, w0=-0.3429143149870357, w1=-0.31819100668718636\n",
      "Logistic regression iter. 6/29: loss=-4283.837629550895, w0=-0.3828136437223465, w1=-0.35462114025982466\n",
      "Logistic regression iter. 7/29: loss=-3264.123001379957, w0=-0.42141833367517206, w1=-0.3899389391529329\n",
      "Logistic regression iter. 8/29: loss=-2506.7906988541185, w0=-0.45907273320293734, w1=-0.42445416646848283\n",
      "Logistic regression iter. 9/29: loss=-1937.1808947169704, w0=-0.4960193336167298, w1=-0.45838089172608587\n",
      "Logistic regression iter. 10/29: loss=-1504.569240338777, w0=-0.5324328714056553, w1=-0.4918707463016471\n",
      "Logistic regression iter. 11/29: loss=-1173.4754927511833, w0=-0.5684414694726623, w1=-0.5250331655143446\n",
      "Logistic regression iter. 12/29: loss=-918.511331658161, w0=-0.6041403403394329, w1=-0.5579482849614044\n",
      "Logistic regression iter. 13/29: loss=-721.180907947942, w0=-0.6396009876769055, w1=-0.59067545978127\n",
      "Logistic regression iter. 14/29: loss=-567.8158365597302, w0=-0.6748775620245117, w1=-0.6232590579229174\n",
      "Logistic regression iter. 15/29: loss=-448.1987187961255, w0=-0.7100113515547354, w1=-0.6557324926916797\n",
      "Logistic regression iter. 16/29: loss=-354.62053698043906, w0=-0.7450340136601498, w1=-0.6881210832032583\n",
      "Logistic regression iter. 17/29: loss=-281.2204465875647, w0=-0.7799699350843274, w1=-0.7204441148212466\n",
      "Logistic regression iter. 18/29: loss=-223.51458160622158, w0=-0.8148379764581363, w1=-0.752716342087779\n",
      "Logistic regression iter. 19/29: loss=-178.0544479208593, w0=-0.8496527745583369, w1=-0.7849490963957314\n",
      "Logistic regression iter. 20/29: loss=-142.17601176007594, w0=-0.8844257223318048, w1=-0.8171511093941208\n",
      "Logistic regression iter. 21/29: loss=-113.81338791501409, w0=-0.9191657114115374, w1=-0.8493291294999857\n",
      "Logistic regression iter. 22/29: loss=-91.35923215161333, w0=-0.9538796978815779, w1=-0.8814883863218219\n",
      "Logistic regression iter. 23/29: loss=-73.5593287988362, w0=-0.9885731354457685, w1=-0.9136329423409507\n",
      "Logistic regression iter. 24/29: loss=-59.432483577641555, w0=-1.0232503084517734, w1=-0.9457659604225503\n",
      "Logistic regression iter. 25/29: loss=-48.209312786201444, w0=-1.0579145888464878, w1=-0.9778899081051444\n",
      "Logistic regression iter. 26/29: loss=-39.28525153365507, w0=-1.092568635067873, w1=-1.010006714154743\n",
      "Logistic regression iter. 27/29: loss=-32.18433140629942, w0=-1.1272145464291863, w1=-1.0421178889122291\n",
      "Logistic regression iter. 28/29: loss=-26.53116045411891, w0=-1.16185398326076, w1=-1.074224617068342\n",
      "Logistic regression iter. 29/29: loss=-22.029180391882846, w0=-1.196488260621099, w1=-1.1063278293669199\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-35255.19436149345, w0=-0.08961303999999998, w1=-0.08609284992829584\n",
      "Logistic regression iter. 1/29: loss=-21827.962660096528, w0=-0.15467003227361603, w1=-0.14645693942327676\n",
      "Logistic regression iter. 2/29: loss=-14766.042447935963, w0=-0.20885018007558093, w1=-0.196053446717477\n",
      "Logistic regression iter. 3/29: loss=-10466.618656523651, w0=-0.2569468062530283, w1=-0.23987776500347438\n",
      "Logistic regression iter. 4/29: loss=-7636.034953763296, w0=-0.3012159122301108, w1=-0.2801851651021257\n",
      "Logistic regression iter. 5/29: loss=-5680.046014483774, w0=-0.3429143149870357, w1=-0.31819100668718636\n",
      "Logistic regression iter. 6/29: loss=-4283.837629550895, w0=-0.3828136437223465, w1=-0.35462114025982466\n",
      "Logistic regression iter. 7/29: loss=-3264.123001379957, w0=-0.42141833367517206, w1=-0.3899389391529329\n",
      "Logistic regression iter. 8/29: loss=-2506.7906988541185, w0=-0.45907273320293734, w1=-0.42445416646848283\n",
      "Logistic regression iter. 9/29: loss=-1937.1808947169704, w0=-0.4960193336167298, w1=-0.45838089172608587\n",
      "Logistic regression iter. 10/29: loss=-1504.569240338777, w0=-0.5324328714056553, w1=-0.4918707463016471\n",
      "Logistic regression iter. 11/29: loss=-1173.4754927511833, w0=-0.5684414694726623, w1=-0.5250331655143446\n",
      "Logistic regression iter. 12/29: loss=-918.511331658161, w0=-0.6041403403394329, w1=-0.5579482849614044\n",
      "Logistic regression iter. 13/29: loss=-721.180907947942, w0=-0.6396009876769055, w1=-0.59067545978127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 14/29: loss=-567.8158365597302, w0=-0.6748775620245117, w1=-0.6232590579229174\n",
      "Logistic regression iter. 15/29: loss=-448.1987187961255, w0=-0.7100113515547354, w1=-0.6557324926916797\n",
      "Logistic regression iter. 16/29: loss=-354.62053698043906, w0=-0.7450340136601498, w1=-0.6881210832032583\n",
      "Logistic regression iter. 17/29: loss=-281.2204465875647, w0=-0.7799699350843274, w1=-0.7204441148212466\n",
      "Logistic regression iter. 18/29: loss=-223.51458160622158, w0=-0.8148379764581363, w1=-0.752716342087779\n",
      "Logistic regression iter. 19/29: loss=-178.0544479208593, w0=-0.8496527745583369, w1=-0.7849490963957314\n",
      "Logistic regression iter. 20/29: loss=-142.17601176007594, w0=-0.8844257223318048, w1=-0.8171511093941208\n",
      "Logistic regression iter. 21/29: loss=-113.81338791501409, w0=-0.9191657114115374, w1=-0.8493291294999857\n",
      "Logistic regression iter. 22/29: loss=-91.35923215161333, w0=-0.9538796978815779, w1=-0.8814883863218219\n",
      "Logistic regression iter. 23/29: loss=-73.5593287988362, w0=-0.9885731354457685, w1=-0.9136329423409507\n",
      "Logistic regression iter. 24/29: loss=-59.432483577641555, w0=-1.0232503084517734, w1=-0.9457659604225503\n",
      "Logistic regression iter. 25/29: loss=-48.209312786201444, w0=-1.0579145888464878, w1=-0.9778899081051444\n",
      "Logistic regression iter. 26/29: loss=-39.28525153365507, w0=-1.092568635067873, w1=-1.010006714154743\n",
      "Logistic regression iter. 27/29: loss=-32.18433140629942, w0=-1.1272145464291863, w1=-1.0421178889122291\n",
      "Logistic regression iter. 28/29: loss=-26.53116045411891, w0=-1.16185398326076, w1=-1.074224617068342\n",
      "Logistic regression iter. 29/29: loss=-22.029180391882846, w0=-1.196488260621099, w1=-1.1063278293669199\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-35255.19436149345, w0=-0.08961303999999998, w1=-0.08609284992829584\n",
      "Logistic regression iter. 1/29: loss=-21827.962660096528, w0=-0.15467003227361603, w1=-0.14645693942327676\n",
      "Logistic regression iter. 2/29: loss=-14766.042447935963, w0=-0.20885018007558093, w1=-0.196053446717477\n",
      "Logistic regression iter. 3/29: loss=-10466.618656523651, w0=-0.2569468062530283, w1=-0.23987776500347438\n",
      "Logistic regression iter. 4/29: loss=-7636.034953763296, w0=-0.3012159122301108, w1=-0.2801851651021257\n",
      "Logistic regression iter. 5/29: loss=-5680.046014483774, w0=-0.3429143149870357, w1=-0.31819100668718636\n",
      "Logistic regression iter. 6/29: loss=-4283.837629550895, w0=-0.3828136437223465, w1=-0.35462114025982466\n",
      "Logistic regression iter. 7/29: loss=-3264.123001379957, w0=-0.42141833367517206, w1=-0.3899389391529329\n",
      "Logistic regression iter. 8/29: loss=-2506.7906988541185, w0=-0.45907273320293734, w1=-0.42445416646848283\n",
      "Logistic regression iter. 9/29: loss=-1937.1808947169704, w0=-0.4960193336167298, w1=-0.45838089172608587\n",
      "Logistic regression iter. 10/29: loss=-1504.569240338777, w0=-0.5324328714056553, w1=-0.4918707463016471\n",
      "Logistic regression iter. 11/29: loss=-1173.4754927511833, w0=-0.5684414694726623, w1=-0.5250331655143446\n",
      "Logistic regression iter. 12/29: loss=-918.511331658161, w0=-0.6041403403394329, w1=-0.5579482849614044\n",
      "Logistic regression iter. 13/29: loss=-721.180907947942, w0=-0.6396009876769055, w1=-0.59067545978127\n",
      "Logistic regression iter. 14/29: loss=-567.8158365597302, w0=-0.6748775620245117, w1=-0.6232590579229174\n",
      "Logistic regression iter. 15/29: loss=-448.1987187961255, w0=-0.7100113515547354, w1=-0.6557324926916797\n",
      "Logistic regression iter. 16/29: loss=-354.62053698043906, w0=-0.7450340136601498, w1=-0.6881210832032583\n",
      "Logistic regression iter. 17/29: loss=-281.2204465875647, w0=-0.7799699350843274, w1=-0.7204441148212466\n",
      "Logistic regression iter. 18/29: loss=-223.51458160622158, w0=-0.8148379764581363, w1=-0.752716342087779\n",
      "Logistic regression iter. 19/29: loss=-178.0544479208593, w0=-0.8496527745583369, w1=-0.7849490963957314\n",
      "Logistic regression iter. 20/29: loss=-142.17601176007594, w0=-0.8844257223318048, w1=-0.8171511093941208\n",
      "Logistic regression iter. 21/29: loss=-113.81338791501409, w0=-0.9191657114115374, w1=-0.8493291294999857\n",
      "Logistic regression iter. 22/29: loss=-91.35923215161333, w0=-0.9538796978815779, w1=-0.8814883863218219\n",
      "Logistic regression iter. 23/29: loss=-73.5593287988362, w0=-0.9885731354457685, w1=-0.9136329423409507\n",
      "Logistic regression iter. 24/29: loss=-59.432483577641555, w0=-1.0232503084517734, w1=-0.9457659604225503\n",
      "Logistic regression iter. 25/29: loss=-48.209312786201444, w0=-1.0579145888464878, w1=-0.9778899081051444\n",
      "Logistic regression iter. 26/29: loss=-39.28525153365507, w0=-1.092568635067873, w1=-1.010006714154743\n",
      "Logistic regression iter. 27/29: loss=-32.18433140629942, w0=-1.1272145464291863, w1=-1.0421178889122291\n",
      "Logistic regression iter. 28/29: loss=-26.53116045411891, w0=-1.16185398326076, w1=-1.074224617068342\n",
      "Logistic regression iter. 29/29: loss=-22.029180391882846, w0=-1.196488260621099, w1=-1.1063278293669199\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-35255.19436149345, w0=-0.08961303999999998, w1=-0.08609284992829584\n",
      "Logistic regression iter. 1/29: loss=-21827.962660096528, w0=-0.15467003227361603, w1=-0.14645693942327676\n",
      "Logistic regression iter. 2/29: loss=-14766.042447935963, w0=-0.20885018007558093, w1=-0.196053446717477\n",
      "Logistic regression iter. 3/29: loss=-10466.618656523651, w0=-0.2569468062530283, w1=-0.23987776500347438\n",
      "Logistic regression iter. 4/29: loss=-7636.034953763296, w0=-0.3012159122301108, w1=-0.2801851651021257\n",
      "Logistic regression iter. 5/29: loss=-5680.046014483774, w0=-0.3429143149870357, w1=-0.31819100668718636\n",
      "Logistic regression iter. 6/29: loss=-4283.837629550895, w0=-0.3828136437223465, w1=-0.35462114025982466\n",
      "Logistic regression iter. 7/29: loss=-3264.123001379957, w0=-0.42141833367517206, w1=-0.3899389391529329\n",
      "Logistic regression iter. 8/29: loss=-2506.7906988541185, w0=-0.45907273320293734, w1=-0.42445416646848283\n",
      "Logistic regression iter. 9/29: loss=-1937.1808947169704, w0=-0.4960193336167298, w1=-0.45838089172608587\n",
      "Logistic regression iter. 10/29: loss=-1504.569240338777, w0=-0.5324328714056553, w1=-0.4918707463016471\n",
      "Logistic regression iter. 11/29: loss=-1173.4754927511833, w0=-0.5684414694726623, w1=-0.5250331655143446\n",
      "Logistic regression iter. 12/29: loss=-918.511331658161, w0=-0.6041403403394329, w1=-0.5579482849614044\n",
      "Logistic regression iter. 13/29: loss=-721.180907947942, w0=-0.6396009876769055, w1=-0.59067545978127\n",
      "Logistic regression iter. 14/29: loss=-567.8158365597302, w0=-0.6748775620245117, w1=-0.6232590579229174\n",
      "Logistic regression iter. 15/29: loss=-448.1987187961255, w0=-0.7100113515547354, w1=-0.6557324926916797\n",
      "Logistic regression iter. 16/29: loss=-354.62053698043906, w0=-0.7450340136601498, w1=-0.6881210832032583\n",
      "Logistic regression iter. 17/29: loss=-281.2204465875647, w0=-0.7799699350843274, w1=-0.7204441148212466\n",
      "Logistic regression iter. 18/29: loss=-223.51458160622158, w0=-0.8148379764581363, w1=-0.752716342087779\n",
      "Logistic regression iter. 19/29: loss=-178.0544479208593, w0=-0.8496527745583369, w1=-0.7849490963957314\n",
      "Logistic regression iter. 20/29: loss=-142.17601176007594, w0=-0.8844257223318048, w1=-0.8171511093941208\n",
      "Logistic regression iter. 21/29: loss=-113.81338791501409, w0=-0.9191657114115374, w1=-0.8493291294999857\n",
      "Logistic regression iter. 22/29: loss=-91.35923215161333, w0=-0.9538796978815779, w1=-0.8814883863218219\n",
      "Logistic regression iter. 23/29: loss=-73.5593287988362, w0=-0.9885731354457685, w1=-0.9136329423409507\n",
      "Logistic regression iter. 24/29: loss=-59.432483577641555, w0=-1.0232503084517734, w1=-0.9457659604225503\n",
      "Logistic regression iter. 25/29: loss=-48.209312786201444, w0=-1.0579145888464878, w1=-0.9778899081051444\n",
      "Logistic regression iter. 26/29: loss=-39.28525153365507, w0=-1.092568635067873, w1=-1.010006714154743\n",
      "Logistic regression iter. 27/29: loss=-32.18433140629942, w0=-1.1272145464291863, w1=-1.0421178889122291\n",
      "Logistic regression iter. 28/29: loss=-26.53116045411891, w0=-1.16185398326076, w1=-1.074224617068342\n",
      "Logistic regression iter. 29/29: loss=-22.029180391882846, w0=-1.196488260621099, w1=-1.1063278293669199\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-30983.632465623676, w0=-0.10590631999999997, w1=-0.10174609536980418\n",
      "Logistic regression iter. 1/29: loss=-18208.98833857784, w0=-0.17877438901524778, w1=-0.1690435027501958\n",
      "Logistic regression iter. 2/29: loss=-11815.221586998558, w0=-0.23913072475000618, w1=-0.22413924082909165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 3/29: loss=-8059.565916617365, w0=-0.29286282125932467, w1=-0.27306063653585005\n",
      "Logistic regression iter. 4/29: loss=-5665.33465265241, w0=-0.3425898246292231, w1=-0.31837188666688876\n",
      "Logistic regression iter. 5/29: loss=-4062.029470923858, w0=-0.38972075234056025, w1=-0.36140525212256\n",
      "Logistic regression iter. 6/29: loss=-2953.236697749741, w0=-0.4350961031713225, w1=-0.4029329482859001\n",
      "Logistic regression iter. 7/29: loss=-2169.1765365254055, w0=-0.47925059284881516, w1=-0.4434342906158852\n",
      "Logistic regression iter. 8/29: loss=-1605.7964513996876, w0=-0.522539376101057, w1=-0.4832207901175474\n",
      "Logistic regression iter. 9/29: loss=-1196.1388050555406, w0=-0.5652055576580277, w1=-0.5225015449307259\n",
      "Logistic regression iter. 10/29: loss=-895.5374194614936, w0=-0.6074192020035039, w1=-0.5614202381107761\n",
      "Logistic regression iter. 11/29: loss=-673.3809039110167, w0=-0.6493012115717335, w1=-0.6000773429281409\n",
      "Logistic regression iter. 12/29: loss=-508.2550391104173, w0=-0.6909385969299853, w1=-0.6385440616072979\n",
      "Logistic regression iter. 13/29: loss=-384.940809167526, w0=-0.7323945757140842, w1=-0.6768713822358605\n",
      "Logistic regression iter. 14/29: loss=-292.4878619693848, w0=-0.7737154270988337, w1=-0.7150961233175412\n",
      "Logistic regression iter. 15/29: loss=-222.93991807487845, w0=-0.8149352388941956, w1=-0.7532450532190993\n",
      "Logistic regression iter. 16/29: loss=-170.47057659793765, w0=-0.8560792474772695, w1=-0.7913377443634027\n",
      "Logistic regression iter. 17/29: loss=-130.78570583411337, w0=-0.8971662171844734, w1=-0.8293885769708669\n",
      "Logistic regression iter. 18/29: loss=-100.70356858576713, w0=-0.9382101524078864, w1=-0.8674081607646255\n",
      "Logistic regression iter. 19/29: loss=-77.85601442782334, w0=-0.9792215395462557, w1=-0.9054043525014002\n",
      "Logistic regression iter. 20/29: loss=-60.47360329442373, w0=-1.0202082539365556, w1=-0.943382989498008\n",
      "Logistic regression iter. 21/29: loss=-47.229760856186985, w0=-1.061176225843233, w1=-0.9813484216462023\n",
      "Logistic regression iter. 22/29: loss=-37.12694708701217, w0=-1.102129931844217, w1=-1.0193038992888228\n",
      "Logistic regression iter. 23/29: loss=-29.41301859544731, w0=-1.143072758883824, w1=-1.057251857292533\n",
      "Logistic regression iter. 24/29: loss=-23.5194664725624, w0=-1.1840072749661026, w1=-1.0951941239303349\n",
      "Logistic regression iter. 25/29: loss=-19.015610581029737, w0=-1.2249354310814657, w1=-1.1331320750257559\n",
      "Logistic regression iter. 26/29: loss=-15.574499553530284, w0=-1.2658587122769074, w1=-1.171066748071643\n",
      "Logistic regression iter. 27/29: loss=-12.947440294637799, w0=-1.3067782509810233, w1=-1.2089989269669488\n",
      "Logistic regression iter. 28/29: loss=-10.944916252687559, w0=-1.3476949122250375, w1=-1.246929205108439\n",
      "Logistic regression iter. 29/29: loss=-9.422253216511548, w0=-1.3886093578774088, w1=-1.2848580324857084\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-30983.632465623676, w0=-0.10590631999999997, w1=-0.10174609536980418\n",
      "Logistic regression iter. 1/29: loss=-18208.98833857784, w0=-0.17877438901524778, w1=-0.1690435027501958\n",
      "Logistic regression iter. 2/29: loss=-11815.221586998558, w0=-0.23913072475000618, w1=-0.22413924082909165\n",
      "Logistic regression iter. 3/29: loss=-8059.565916617365, w0=-0.29286282125932467, w1=-0.27306063653585005\n",
      "Logistic regression iter. 4/29: loss=-5665.33465265241, w0=-0.3425898246292231, w1=-0.31837188666688876\n",
      "Logistic regression iter. 5/29: loss=-4062.029470923858, w0=-0.38972075234056025, w1=-0.36140525212256\n",
      "Logistic regression iter. 6/29: loss=-2953.236697749741, w0=-0.4350961031713225, w1=-0.4029329482859001\n",
      "Logistic regression iter. 7/29: loss=-2169.1765365254055, w0=-0.47925059284881516, w1=-0.4434342906158852\n",
      "Logistic regression iter. 8/29: loss=-1605.7964513996876, w0=-0.522539376101057, w1=-0.4832207901175474\n",
      "Logistic regression iter. 9/29: loss=-1196.1388050555406, w0=-0.5652055576580277, w1=-0.5225015449307259\n",
      "Logistic regression iter. 10/29: loss=-895.5374194614936, w0=-0.6074192020035039, w1=-0.5614202381107761\n",
      "Logistic regression iter. 11/29: loss=-673.3809039110167, w0=-0.6493012115717335, w1=-0.6000773429281409\n",
      "Logistic regression iter. 12/29: loss=-508.2550391104173, w0=-0.6909385969299853, w1=-0.6385440616072979\n",
      "Logistic regression iter. 13/29: loss=-384.940809167526, w0=-0.7323945757140842, w1=-0.6768713822358605\n",
      "Logistic regression iter. 14/29: loss=-292.4878619693848, w0=-0.7737154270988337, w1=-0.7150961233175412\n",
      "Logistic regression iter. 15/29: loss=-222.93991807487845, w0=-0.8149352388941956, w1=-0.7532450532190993\n",
      "Logistic regression iter. 16/29: loss=-170.47057659793765, w0=-0.8560792474772695, w1=-0.7913377443634027\n",
      "Logistic regression iter. 17/29: loss=-130.78570583411337, w0=-0.8971662171844734, w1=-0.8293885769708669\n",
      "Logistic regression iter. 18/29: loss=-100.70356858576713, w0=-0.9382101524078864, w1=-0.8674081607646255\n",
      "Logistic regression iter. 19/29: loss=-77.85601442782334, w0=-0.9792215395462557, w1=-0.9054043525014002\n",
      "Logistic regression iter. 20/29: loss=-60.47360329442373, w0=-1.0202082539365556, w1=-0.943382989498008\n",
      "Logistic regression iter. 21/29: loss=-47.229760856186985, w0=-1.061176225843233, w1=-0.9813484216462023\n",
      "Logistic regression iter. 22/29: loss=-37.12694708701217, w0=-1.102129931844217, w1=-1.0193038992888228\n",
      "Logistic regression iter. 23/29: loss=-29.41301859544731, w0=-1.143072758883824, w1=-1.057251857292533\n",
      "Logistic regression iter. 24/29: loss=-23.5194664725624, w0=-1.1840072749661026, w1=-1.0951941239303349\n",
      "Logistic regression iter. 25/29: loss=-19.015610581029737, w0=-1.2249354310814657, w1=-1.1331320750257559\n",
      "Logistic regression iter. 26/29: loss=-15.574499553530284, w0=-1.2658587122769074, w1=-1.171066748071643\n",
      "Logistic regression iter. 27/29: loss=-12.947440294637799, w0=-1.3067782509810233, w1=-1.2089989269669488\n",
      "Logistic regression iter. 28/29: loss=-10.944916252687559, w0=-1.3476949122250375, w1=-1.246929205108439\n",
      "Logistic regression iter. 29/29: loss=-9.422253216511548, w0=-1.3886093578774088, w1=-1.2848580324857084\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-30983.632465623676, w0=-0.10590631999999997, w1=-0.10174609536980418\n",
      "Logistic regression iter. 1/29: loss=-18208.98833857784, w0=-0.17877438901524778, w1=-0.1690435027501958\n",
      "Logistic regression iter. 2/29: loss=-11815.221586998558, w0=-0.23913072475000618, w1=-0.22413924082909165\n",
      "Logistic regression iter. 3/29: loss=-8059.565916617365, w0=-0.29286282125932467, w1=-0.27306063653585005\n",
      "Logistic regression iter. 4/29: loss=-5665.33465265241, w0=-0.3425898246292231, w1=-0.31837188666688876\n",
      "Logistic regression iter. 5/29: loss=-4062.029470923858, w0=-0.38972075234056025, w1=-0.36140525212256\n",
      "Logistic regression iter. 6/29: loss=-2953.236697749741, w0=-0.4350961031713225, w1=-0.4029329482859001\n",
      "Logistic regression iter. 7/29: loss=-2169.1765365254055, w0=-0.47925059284881516, w1=-0.4434342906158852\n",
      "Logistic regression iter. 8/29: loss=-1605.7964513996876, w0=-0.522539376101057, w1=-0.4832207901175474\n",
      "Logistic regression iter. 9/29: loss=-1196.1388050555406, w0=-0.5652055576580277, w1=-0.5225015449307259\n",
      "Logistic regression iter. 10/29: loss=-895.5374194614936, w0=-0.6074192020035039, w1=-0.5614202381107761\n",
      "Logistic regression iter. 11/29: loss=-673.3809039110167, w0=-0.6493012115717335, w1=-0.6000773429281409\n",
      "Logistic regression iter. 12/29: loss=-508.2550391104173, w0=-0.6909385969299853, w1=-0.6385440616072979\n",
      "Logistic regression iter. 13/29: loss=-384.940809167526, w0=-0.7323945757140842, w1=-0.6768713822358605\n",
      "Logistic regression iter. 14/29: loss=-292.4878619693848, w0=-0.7737154270988337, w1=-0.7150961233175412\n",
      "Logistic regression iter. 15/29: loss=-222.93991807487845, w0=-0.8149352388941956, w1=-0.7532450532190993\n",
      "Logistic regression iter. 16/29: loss=-170.47057659793765, w0=-0.8560792474772695, w1=-0.7913377443634027\n",
      "Logistic regression iter. 17/29: loss=-130.78570583411337, w0=-0.8971662171844734, w1=-0.8293885769708669\n",
      "Logistic regression iter. 18/29: loss=-100.70356858576713, w0=-0.9382101524078864, w1=-0.8674081607646255\n",
      "Logistic regression iter. 19/29: loss=-77.85601442782334, w0=-0.9792215395462557, w1=-0.9054043525014002\n",
      "Logistic regression iter. 20/29: loss=-60.47360329442373, w0=-1.0202082539365556, w1=-0.943382989498008\n",
      "Logistic regression iter. 21/29: loss=-47.229760856186985, w0=-1.061176225843233, w1=-0.9813484216462023\n",
      "Logistic regression iter. 22/29: loss=-37.12694708701217, w0=-1.102129931844217, w1=-1.0193038992888228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 23/29: loss=-29.41301859544731, w0=-1.143072758883824, w1=-1.057251857292533\n",
      "Logistic regression iter. 24/29: loss=-23.5194664725624, w0=-1.1840072749661026, w1=-1.0951941239303349\n",
      "Logistic regression iter. 25/29: loss=-19.015610581029737, w0=-1.2249354310814657, w1=-1.1331320750257559\n",
      "Logistic regression iter. 26/29: loss=-15.574499553530284, w0=-1.2658587122769074, w1=-1.171066748071643\n",
      "Logistic regression iter. 27/29: loss=-12.947440294637799, w0=-1.3067782509810233, w1=-1.2089989269669488\n",
      "Logistic regression iter. 28/29: loss=-10.944916252687559, w0=-1.3476949122250375, w1=-1.246929205108439\n",
      "Logistic regression iter. 29/29: loss=-9.422253216511548, w0=-1.3886093578774088, w1=-1.2848580324857084\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-30983.632465623676, w0=-0.10590631999999997, w1=-0.10174609536980418\n",
      "Logistic regression iter. 1/29: loss=-18208.98833857784, w0=-0.17877438901524778, w1=-0.1690435027501958\n",
      "Logistic regression iter. 2/29: loss=-11815.221586998558, w0=-0.23913072475000618, w1=-0.22413924082909165\n",
      "Logistic regression iter. 3/29: loss=-8059.565916617365, w0=-0.29286282125932467, w1=-0.27306063653585005\n",
      "Logistic regression iter. 4/29: loss=-5665.33465265241, w0=-0.3425898246292231, w1=-0.31837188666688876\n",
      "Logistic regression iter. 5/29: loss=-4062.029470923858, w0=-0.38972075234056025, w1=-0.36140525212256\n",
      "Logistic regression iter. 6/29: loss=-2953.236697749741, w0=-0.4350961031713225, w1=-0.4029329482859001\n",
      "Logistic regression iter. 7/29: loss=-2169.1765365254055, w0=-0.47925059284881516, w1=-0.4434342906158852\n",
      "Logistic regression iter. 8/29: loss=-1605.7964513996876, w0=-0.522539376101057, w1=-0.4832207901175474\n",
      "Logistic regression iter. 9/29: loss=-1196.1388050555406, w0=-0.5652055576580277, w1=-0.5225015449307259\n",
      "Logistic regression iter. 10/29: loss=-895.5374194614936, w0=-0.6074192020035039, w1=-0.5614202381107761\n",
      "Logistic regression iter. 11/29: loss=-673.3809039110167, w0=-0.6493012115717335, w1=-0.6000773429281409\n",
      "Logistic regression iter. 12/29: loss=-508.2550391104173, w0=-0.6909385969299853, w1=-0.6385440616072979\n",
      "Logistic regression iter. 13/29: loss=-384.940809167526, w0=-0.7323945757140842, w1=-0.6768713822358605\n",
      "Logistic regression iter. 14/29: loss=-292.4878619693848, w0=-0.7737154270988337, w1=-0.7150961233175412\n",
      "Logistic regression iter. 15/29: loss=-222.93991807487845, w0=-0.8149352388941956, w1=-0.7532450532190993\n",
      "Logistic regression iter. 16/29: loss=-170.47057659793765, w0=-0.8560792474772695, w1=-0.7913377443634027\n",
      "Logistic regression iter. 17/29: loss=-130.78570583411337, w0=-0.8971662171844734, w1=-0.8293885769708669\n",
      "Logistic regression iter. 18/29: loss=-100.70356858576713, w0=-0.9382101524078864, w1=-0.8674081607646255\n",
      "Logistic regression iter. 19/29: loss=-77.85601442782334, w0=-0.9792215395462557, w1=-0.9054043525014002\n",
      "Logistic regression iter. 20/29: loss=-60.47360329442373, w0=-1.0202082539365556, w1=-0.943382989498008\n",
      "Logistic regression iter. 21/29: loss=-47.229760856186985, w0=-1.061176225843233, w1=-0.9813484216462023\n",
      "Logistic regression iter. 22/29: loss=-37.12694708701217, w0=-1.102129931844217, w1=-1.0193038992888228\n",
      "Logistic regression iter. 23/29: loss=-29.41301859544731, w0=-1.143072758883824, w1=-1.057251857292533\n",
      "Logistic regression iter. 24/29: loss=-23.5194664725624, w0=-1.1840072749661026, w1=-1.0951941239303349\n",
      "Logistic regression iter. 25/29: loss=-19.015610581029737, w0=-1.2249354310814657, w1=-1.1331320750257559\n",
      "Logistic regression iter. 26/29: loss=-15.574499553530284, w0=-1.2658587122769074, w1=-1.171066748071643\n",
      "Logistic regression iter. 27/29: loss=-12.947440294637799, w0=-1.3067782509810233, w1=-1.2089989269669488\n",
      "Logistic regression iter. 28/29: loss=-10.944916252687559, w0=-1.3476949122250375, w1=-1.246929205108439\n",
      "Logistic regression iter. 29/29: loss=-9.422253216511548, w0=-1.3886093578774088, w1=-1.2848580324857084\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-27243.754925203448, w0=-0.12219959999999999, w1=-0.11739934081131254\n",
      "Logistic regression iter. 1/29: loss=-15286.68414623766, w0=-0.2021132379396433, w1=-0.19091424472194102\n",
      "Logistic regression iter. 2/29: loss=-9539.605418364348, w0=-0.2682678589849694, w1=-0.2512013722382666\n",
      "Logistic regression iter. 3/29: loss=-6270.76822064651, w0=-0.3274614916863114, w1=-0.30510484191385484\n",
      "Logistic regression iter. 4/29: loss=-4250.025362445414, w0=-0.38259457815474146, w1=-0.35541085888962837\n",
      "Logistic regression iter. 5/29: loss=-2938.274513667166, w0=-0.435183060855647, w1=-0.4035241494661324\n",
      "Logistic regression iter. 6/29: loss=-2059.660660091669, w0=-0.48610775817926416, w1=-0.450237100470424\n",
      "Logistic regression iter. 7/29: loss=-1458.4977643357242, w0=-0.535914316952219, w1=-0.49602825721256033\n",
      "Logistic regression iter. 8/29: loss=-1040.9006629127618, w0=-0.5849551577219122, w1=-0.5411998351537977\n",
      "Logistic regression iter. 9/29: loss=-747.5608321377078, w0=-0.6334644833284653, w1=-0.585948644957309\n",
      "Logistic regression iter. 10/29: loss=-539.7425770440514, w0=-0.6816011109526666, w1=-0.6304056946209077\n",
      "Logistic regression iter. 11/29: loss=-391.5227982053212, w0=-0.7294743419824424, w1=-0.6746596107148601\n",
      "Logistic regression iter. 12/29: loss=-285.23551676526995, w0=-0.7771602445347281, w1=-0.7187710898902467\n",
      "Logistic regression iter. 13/29: loss=-208.67508214465158, w0=-0.8247122200008337, w1=-0.7627821025003723\n",
      "Logistic regression iter. 14/29: loss=-153.31821807040734, w0=-0.8721680192073803, w1=-0.8067218958081206\n",
      "Logistic regression iter. 15/29: loss=-113.16275261940044, w0=-0.9195544828527128, w1=-0.8506109819780977\n",
      "Logistic regression iter. 16/29: loss=-83.95300858139875, w0=-0.9668907875562083, w1=-0.8944638253251155\n",
      "Logistic regression iter. 17/29: loss=-62.654549448473055, w0=-1.0141906921038968, w1=-0.9382906736216733\n",
      "Logistic regression iter. 18/29: loss=-47.093370064484695, w0=-1.06146410498355, w1=-0.9820988175372625\n",
      "Logistic regression iter. 19/29: loss=-35.70547685458058, w0=-1.1087181858586084, w1=-1.025893463339914\n",
      "Logistic regression iter. 20/29: loss=-27.361625935759896, w0=-1.155958124039683, w1=-1.0696783414709814\n",
      "Logistic regression iter. 21/29: loss=-21.24381551050683, w0=-1.2031876914047936, w1=-1.1134561332583752\n",
      "Logistic regression iter. 22/29: loss=-16.75774614075528, w0=-1.250409636822647, w1=-1.1572287715504401\n",
      "Logistic regression iter. 23/29: loss=-13.470467804277623, w0=-1.2976259685989169, w1=-1.2009976534275677\n",
      "Logistic regression iter. 24/29: loss=-11.065776251896661, w0=-1.3448381574414237, w1=-1.2447637912853222\n",
      "Logistic regression iter. 25/29: loss=-9.312184985616, w0=-1.392047282776224, w1=-1.288527920522504\n",
      "Logistic regression iter. 26/29: loss=-8.039848858109485, w0=-1.4392541385375934, w1=-1.3322905765476938\n",
      "Logistic regression iter. 27/29: loss=-7.1238858134256935, w0=-1.4864593098677954, w1=-1.3760521500121055\n",
      "Logistic regression iter. 28/29: loss=-6.472288438284742, w0=-1.5336632288702248, w1=-1.4198129265372854\n",
      "Logistic regression iter. 29/29: loss=-6.017139005371988, w0=-1.5808662152359216, w1=-1.463573115366344\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-27243.754925203448, w0=-0.12219959999999999, w1=-0.11739934081131254\n",
      "Logistic regression iter. 1/29: loss=-15286.68414623766, w0=-0.2021132379396433, w1=-0.19091424472194102\n",
      "Logistic regression iter. 2/29: loss=-9539.605418364348, w0=-0.2682678589849694, w1=-0.2512013722382666\n",
      "Logistic regression iter. 3/29: loss=-6270.76822064651, w0=-0.3274614916863114, w1=-0.30510484191385484\n",
      "Logistic regression iter. 4/29: loss=-4250.025362445414, w0=-0.38259457815474146, w1=-0.35541085888962837\n",
      "Logistic regression iter. 5/29: loss=-2938.274513667166, w0=-0.435183060855647, w1=-0.4035241494661324\n",
      "Logistic regression iter. 6/29: loss=-2059.660660091669, w0=-0.48610775817926416, w1=-0.450237100470424\n",
      "Logistic regression iter. 7/29: loss=-1458.4977643357242, w0=-0.535914316952219, w1=-0.49602825721256033\n",
      "Logistic regression iter. 8/29: loss=-1040.9006629127618, w0=-0.5849551577219122, w1=-0.5411998351537977\n",
      "Logistic regression iter. 9/29: loss=-747.5608321377078, w0=-0.6334644833284653, w1=-0.585948644957309\n",
      "Logistic regression iter. 10/29: loss=-539.7425770440514, w0=-0.6816011109526666, w1=-0.6304056946209077\n",
      "Logistic regression iter. 11/29: loss=-391.5227982053212, w0=-0.7294743419824424, w1=-0.6746596107148601\n",
      "Logistic regression iter. 12/29: loss=-285.23551676526995, w0=-0.7771602445347281, w1=-0.7187710898902467\n",
      "Logistic regression iter. 13/29: loss=-208.67508214465158, w0=-0.8247122200008337, w1=-0.7627821025003723\n",
      "Logistic regression iter. 14/29: loss=-153.31821807040734, w0=-0.8721680192073803, w1=-0.8067218958081206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 15/29: loss=-113.16275261940044, w0=-0.9195544828527128, w1=-0.8506109819780977\n",
      "Logistic regression iter. 16/29: loss=-83.95300858139875, w0=-0.9668907875562083, w1=-0.8944638253251155\n",
      "Logistic regression iter. 17/29: loss=-62.654549448473055, w0=-1.0141906921038968, w1=-0.9382906736216733\n",
      "Logistic regression iter. 18/29: loss=-47.093370064484695, w0=-1.06146410498355, w1=-0.9820988175372625\n",
      "Logistic regression iter. 19/29: loss=-35.70547685458058, w0=-1.1087181858586084, w1=-1.025893463339914\n",
      "Logistic regression iter. 20/29: loss=-27.361625935759896, w0=-1.155958124039683, w1=-1.0696783414709814\n",
      "Logistic regression iter. 21/29: loss=-21.24381551050683, w0=-1.2031876914047936, w1=-1.1134561332583752\n",
      "Logistic regression iter. 22/29: loss=-16.75774614075528, w0=-1.250409636822647, w1=-1.1572287715504401\n",
      "Logistic regression iter. 23/29: loss=-13.470467804277623, w0=-1.2976259685989169, w1=-1.2009976534275677\n",
      "Logistic regression iter. 24/29: loss=-11.065776251896661, w0=-1.3448381574414237, w1=-1.2447637912853222\n",
      "Logistic regression iter. 25/29: loss=-9.312184985616, w0=-1.392047282776224, w1=-1.288527920522504\n",
      "Logistic regression iter. 26/29: loss=-8.039848858109485, w0=-1.4392541385375934, w1=-1.3322905765476938\n",
      "Logistic regression iter. 27/29: loss=-7.1238858134256935, w0=-1.4864593098677954, w1=-1.3760521500121055\n",
      "Logistic regression iter. 28/29: loss=-6.472288438284742, w0=-1.5336632288702248, w1=-1.4198129265372854\n",
      "Logistic regression iter. 29/29: loss=-6.017139005371988, w0=-1.5808662152359216, w1=-1.463573115366344\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-27243.754925203448, w0=-0.12219959999999999, w1=-0.11739934081131254\n",
      "Logistic regression iter. 1/29: loss=-15286.68414623766, w0=-0.2021132379396433, w1=-0.19091424472194102\n",
      "Logistic regression iter. 2/29: loss=-9539.605418364348, w0=-0.2682678589849694, w1=-0.2512013722382666\n",
      "Logistic regression iter. 3/29: loss=-6270.76822064651, w0=-0.3274614916863114, w1=-0.30510484191385484\n",
      "Logistic regression iter. 4/29: loss=-4250.025362445414, w0=-0.38259457815474146, w1=-0.35541085888962837\n",
      "Logistic regression iter. 5/29: loss=-2938.274513667166, w0=-0.435183060855647, w1=-0.4035241494661324\n",
      "Logistic regression iter. 6/29: loss=-2059.660660091669, w0=-0.48610775817926416, w1=-0.450237100470424\n",
      "Logistic regression iter. 7/29: loss=-1458.4977643357242, w0=-0.535914316952219, w1=-0.49602825721256033\n",
      "Logistic regression iter. 8/29: loss=-1040.9006629127618, w0=-0.5849551577219122, w1=-0.5411998351537977\n",
      "Logistic regression iter. 9/29: loss=-747.5608321377078, w0=-0.6334644833284653, w1=-0.585948644957309\n",
      "Logistic regression iter. 10/29: loss=-539.7425770440514, w0=-0.6816011109526666, w1=-0.6304056946209077\n",
      "Logistic regression iter. 11/29: loss=-391.5227982053212, w0=-0.7294743419824424, w1=-0.6746596107148601\n",
      "Logistic regression iter. 12/29: loss=-285.23551676526995, w0=-0.7771602445347281, w1=-0.7187710898902467\n",
      "Logistic regression iter. 13/29: loss=-208.67508214465158, w0=-0.8247122200008337, w1=-0.7627821025003723\n",
      "Logistic regression iter. 14/29: loss=-153.31821807040734, w0=-0.8721680192073803, w1=-0.8067218958081206\n",
      "Logistic regression iter. 15/29: loss=-113.16275261940044, w0=-0.9195544828527128, w1=-0.8506109819780977\n",
      "Logistic regression iter. 16/29: loss=-83.95300858139875, w0=-0.9668907875562083, w1=-0.8944638253251155\n",
      "Logistic regression iter. 17/29: loss=-62.654549448473055, w0=-1.0141906921038968, w1=-0.9382906736216733\n",
      "Logistic regression iter. 18/29: loss=-47.093370064484695, w0=-1.06146410498355, w1=-0.9820988175372625\n",
      "Logistic regression iter. 19/29: loss=-35.70547685458058, w0=-1.1087181858586084, w1=-1.025893463339914\n",
      "Logistic regression iter. 20/29: loss=-27.361625935759896, w0=-1.155958124039683, w1=-1.0696783414709814\n",
      "Logistic regression iter. 21/29: loss=-21.24381551050683, w0=-1.2031876914047936, w1=-1.1134561332583752\n",
      "Logistic regression iter. 22/29: loss=-16.75774614075528, w0=-1.250409636822647, w1=-1.1572287715504401\n",
      "Logistic regression iter. 23/29: loss=-13.470467804277623, w0=-1.2976259685989169, w1=-1.2009976534275677\n",
      "Logistic regression iter. 24/29: loss=-11.065776251896661, w0=-1.3448381574414237, w1=-1.2447637912853222\n",
      "Logistic regression iter. 25/29: loss=-9.312184985616, w0=-1.392047282776224, w1=-1.288527920522504\n",
      "Logistic regression iter. 26/29: loss=-8.039848858109485, w0=-1.4392541385375934, w1=-1.3322905765476938\n",
      "Logistic regression iter. 27/29: loss=-7.1238858134256935, w0=-1.4864593098677954, w1=-1.3760521500121055\n",
      "Logistic regression iter. 28/29: loss=-6.472288438284742, w0=-1.5336632288702248, w1=-1.4198129265372854\n",
      "Logistic regression iter. 29/29: loss=-6.017139005371988, w0=-1.5808662152359216, w1=-1.463573115366344\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-27243.754925203448, w0=-0.12219959999999999, w1=-0.11739934081131254\n",
      "Logistic regression iter. 1/29: loss=-15286.68414623766, w0=-0.2021132379396433, w1=-0.19091424472194102\n",
      "Logistic regression iter. 2/29: loss=-9539.605418364348, w0=-0.2682678589849694, w1=-0.2512013722382666\n",
      "Logistic regression iter. 3/29: loss=-6270.76822064651, w0=-0.3274614916863114, w1=-0.30510484191385484\n",
      "Logistic regression iter. 4/29: loss=-4250.025362445414, w0=-0.38259457815474146, w1=-0.35541085888962837\n",
      "Logistic regression iter. 5/29: loss=-2938.274513667166, w0=-0.435183060855647, w1=-0.4035241494661324\n",
      "Logistic regression iter. 6/29: loss=-2059.660660091669, w0=-0.48610775817926416, w1=-0.450237100470424\n",
      "Logistic regression iter. 7/29: loss=-1458.4977643357242, w0=-0.535914316952219, w1=-0.49602825721256033\n",
      "Logistic regression iter. 8/29: loss=-1040.9006629127618, w0=-0.5849551577219122, w1=-0.5411998351537977\n",
      "Logistic regression iter. 9/29: loss=-747.5608321377078, w0=-0.6334644833284653, w1=-0.585948644957309\n",
      "Logistic regression iter. 10/29: loss=-539.7425770440514, w0=-0.6816011109526666, w1=-0.6304056946209077\n",
      "Logistic regression iter. 11/29: loss=-391.5227982053212, w0=-0.7294743419824424, w1=-0.6746596107148601\n",
      "Logistic regression iter. 12/29: loss=-285.23551676526995, w0=-0.7771602445347281, w1=-0.7187710898902467\n",
      "Logistic regression iter. 13/29: loss=-208.67508214465158, w0=-0.8247122200008337, w1=-0.7627821025003723\n",
      "Logistic regression iter. 14/29: loss=-153.31821807040734, w0=-0.8721680192073803, w1=-0.8067218958081206\n",
      "Logistic regression iter. 15/29: loss=-113.16275261940044, w0=-0.9195544828527128, w1=-0.8506109819780977\n",
      "Logistic regression iter. 16/29: loss=-83.95300858139875, w0=-0.9668907875562083, w1=-0.8944638253251155\n",
      "Logistic regression iter. 17/29: loss=-62.654549448473055, w0=-1.0141906921038968, w1=-0.9382906736216733\n",
      "Logistic regression iter. 18/29: loss=-47.093370064484695, w0=-1.06146410498355, w1=-0.9820988175372625\n",
      "Logistic regression iter. 19/29: loss=-35.70547685458058, w0=-1.1087181858586084, w1=-1.025893463339914\n",
      "Logistic regression iter. 20/29: loss=-27.361625935759896, w0=-1.155958124039683, w1=-1.0696783414709814\n",
      "Logistic regression iter. 21/29: loss=-21.24381551050683, w0=-1.2031876914047936, w1=-1.1134561332583752\n",
      "Logistic regression iter. 22/29: loss=-16.75774614075528, w0=-1.250409636822647, w1=-1.1572287715504401\n",
      "Logistic regression iter. 23/29: loss=-13.470467804277623, w0=-1.2976259685989169, w1=-1.2009976534275677\n",
      "Logistic regression iter. 24/29: loss=-11.065776251896661, w0=-1.3448381574414237, w1=-1.2447637912853222\n",
      "Logistic regression iter. 25/29: loss=-9.312184985616, w0=-1.392047282776224, w1=-1.288527920522504\n",
      "Logistic regression iter. 26/29: loss=-8.039848858109485, w0=-1.4392541385375934, w1=-1.3322905765476938\n",
      "Logistic regression iter. 27/29: loss=-7.1238858134256935, w0=-1.4864593098677954, w1=-1.3760521500121055\n",
      "Logistic regression iter. 28/29: loss=-6.472288438284742, w0=-1.5336632288702248, w1=-1.4198129265372854\n",
      "Logistic regression iter. 29/29: loss=-6.017139005371988, w0=-1.5808662152359216, w1=-1.463573115366344\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-23969.640375646522, w0=-0.13849287999999998, w1=-0.13305258625282088\n",
      "Logistic regression iter. 1/29: loss=-12898.223385359397, w0=-0.22484122590463645, w1=-0.21223171024567838\n",
      "Logistic regression iter. 2/29: loss=-7755.322746550251, w0=-0.296539939032285, w1=-0.27751696244002994\n",
      "Logistic regression iter. 3/29: loss=-4916.724920813895, w0=-0.36110852853424147, w1=-0.33636054652894287\n",
      "Logistic regression iter. 4/29: loss=-3214.1670746416407, w0=-0.4216545096324757, w1=-0.39169602446168117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 5/29: loss=-2143.023640486093, w0=-0.4797613295523677, w1=-0.4449639692075111\n",
      "Logistic regression iter. 6/29: loss=-1448.5275087154707, w0=-0.536325975333989, w1=-0.496956022979811\n",
      "Logistic regression iter. 7/29: loss=-989.0452235036571, w0=-0.5918892923944584, w1=-0.548137207492144\n",
      "Logistic regression iter. 8/29: loss=-680.6829176371152, w0=-0.646790602622026, w1=-0.598792602084176\n",
      "Logistic regression iter. 9/29: loss=-471.55170531666124, w0=-0.7012485411539962, w1=-0.649102090621675\n",
      "Logistic regression iter. 10/29: loss=-328.5719507448201, w0=-0.7554066337921543, w1=-0.6991815156083467\n",
      "Logistic regression iter. 11/29: loss=-230.19263127394797, w0=-0.8093603900953747, w1=-0.7491066033072978\n",
      "Logistic regression iter. 12/29: loss=-162.14781595517744, w0=-0.8631740258674302, w1=-0.7989274250253211\n",
      "Logistic regression iter. 13/29: loss=-114.8795948984989, w0=-0.9168910685439392, w1=-0.8486773894438554\n",
      "Logistic regression iter. 14/29: loss=-81.92410616575253, w0=-0.9705412184411493, w1=-0.8983789531268611\n",
      "Logistic regression iter. 15/29: loss=-58.87704634140015, w0=-1.0241448545502285, w1=-0.9480473052989741\n",
      "Logistic regression iter. 16/29: loss=-42.71883803455826, w0=-1.0777160277509825, w1=-0.9976927752284743\n",
      "Logistic regression iter. 17/29: loss=-31.368404982901364, w0=-1.1312644674225254, w1=-1.0473224207548044\n",
      "Logistic regression iter. 18/29: loss=-23.38495237023902, w0=-1.1847969366868067, w1=-1.0969410850815844\n",
      "Logistic regression iter. 19/29: loss=-17.766949851949693, w0=-1.2383181534525474, w1=-1.1465521046800844\n",
      "Logistic regression iter. 20/29: loss=-13.815678477853032, w0=-1.2918314197408263, w1=-1.1961577863098858\n",
      "Logistic regression iter. 21/29: loss=-11.04205426559841, w0=-1.3453390537250718, w1=-1.245759730144389\n",
      "Logistic regression iter. 22/29: loss=-9.102677639830995, w0=-1.3988426875932376, w1=-1.2953590496778986\n",
      "Logistic regression iter. 23/29: loss=-7.7557444024721995, w0=-1.4523434736992016, w1=-1.344956522024068\n",
      "Logistic regression iter. 24/29: loss=-6.83052745616734, w0=-1.505842227750839, w1=-1.3945526910415085\n",
      "Logistic regression iter. 25/29: loss=-6.206174976001072, w0=-1.5593395285963711, w1=-1.4441479383489606\n",
      "Logistic regression iter. 26/29: loss=-5.796931457006138, w0=-1.6128357879818385, w1=-1.4937425323939002\n",
      "Logistic regression iter. 27/29: loss=-5.541803724124996, w0=-1.666331299460272, w1=-1.543336662464716\n",
      "Logistic regression iter. 28/29: loss=-5.397313884490942, w0=-1.7198262727794902, w1=-1.592930462337104\n",
      "Logistic regression iter. 29/29: loss=-5.3324030764806345, w0=-1.773320858124471, w1=-1.6425240267604702\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-23969.640375646522, w0=-0.13849287999999998, w1=-0.13305258625282088\n",
      "Logistic regression iter. 1/29: loss=-12898.223385359397, w0=-0.22484122590463645, w1=-0.21223171024567838\n",
      "Logistic regression iter. 2/29: loss=-7755.322746550251, w0=-0.296539939032285, w1=-0.27751696244002994\n",
      "Logistic regression iter. 3/29: loss=-4916.724920813895, w0=-0.36110852853424147, w1=-0.33636054652894287\n",
      "Logistic regression iter. 4/29: loss=-3214.1670746416407, w0=-0.4216545096324757, w1=-0.39169602446168117\n",
      "Logistic regression iter. 5/29: loss=-2143.023640486093, w0=-0.4797613295523677, w1=-0.4449639692075111\n",
      "Logistic regression iter. 6/29: loss=-1448.5275087154707, w0=-0.536325975333989, w1=-0.496956022979811\n",
      "Logistic regression iter. 7/29: loss=-989.0452235036571, w0=-0.5918892923944584, w1=-0.548137207492144\n",
      "Logistic regression iter. 8/29: loss=-680.6829176371152, w0=-0.646790602622026, w1=-0.598792602084176\n",
      "Logistic regression iter. 9/29: loss=-471.55170531666124, w0=-0.7012485411539962, w1=-0.649102090621675\n",
      "Logistic regression iter. 10/29: loss=-328.5719507448201, w0=-0.7554066337921543, w1=-0.6991815156083467\n",
      "Logistic regression iter. 11/29: loss=-230.19263127394797, w0=-0.8093603900953747, w1=-0.7491066033072978\n",
      "Logistic regression iter. 12/29: loss=-162.14781595517744, w0=-0.8631740258674302, w1=-0.7989274250253211\n",
      "Logistic regression iter. 13/29: loss=-114.8795948984989, w0=-0.9168910685439392, w1=-0.8486773894438554\n",
      "Logistic regression iter. 14/29: loss=-81.92410616575253, w0=-0.9705412184411493, w1=-0.8983789531268611\n",
      "Logistic regression iter. 15/29: loss=-58.87704634140015, w0=-1.0241448545502285, w1=-0.9480473052989741\n",
      "Logistic regression iter. 16/29: loss=-42.71883803455826, w0=-1.0777160277509825, w1=-0.9976927752284743\n",
      "Logistic regression iter. 17/29: loss=-31.368404982901364, w0=-1.1312644674225254, w1=-1.0473224207548044\n",
      "Logistic regression iter. 18/29: loss=-23.38495237023902, w0=-1.1847969366868067, w1=-1.0969410850815844\n",
      "Logistic regression iter. 19/29: loss=-17.766949851949693, w0=-1.2383181534525474, w1=-1.1465521046800844\n",
      "Logistic regression iter. 20/29: loss=-13.815678477853032, w0=-1.2918314197408263, w1=-1.1961577863098858\n",
      "Logistic regression iter. 21/29: loss=-11.04205426559841, w0=-1.3453390537250718, w1=-1.245759730144389\n",
      "Logistic regression iter. 22/29: loss=-9.102677639830995, w0=-1.3988426875932376, w1=-1.2953590496778986\n",
      "Logistic regression iter. 23/29: loss=-7.7557444024721995, w0=-1.4523434736992016, w1=-1.344956522024068\n",
      "Logistic regression iter. 24/29: loss=-6.83052745616734, w0=-1.505842227750839, w1=-1.3945526910415085\n",
      "Logistic regression iter. 25/29: loss=-6.206174976001072, w0=-1.5593395285963711, w1=-1.4441479383489606\n",
      "Logistic regression iter. 26/29: loss=-5.796931457006138, w0=-1.6128357879818385, w1=-1.4937425323939002\n",
      "Logistic regression iter. 27/29: loss=-5.541803724124996, w0=-1.666331299460272, w1=-1.543336662464716\n",
      "Logistic regression iter. 28/29: loss=-5.397313884490942, w0=-1.7198262727794902, w1=-1.592930462337104\n",
      "Logistic regression iter. 29/29: loss=-5.3324030764806345, w0=-1.773320858124471, w1=-1.6425240267604702\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-23969.640375646522, w0=-0.13849287999999998, w1=-0.13305258625282088\n",
      "Logistic regression iter. 1/29: loss=-12898.223385359397, w0=-0.22484122590463645, w1=-0.21223171024567838\n",
      "Logistic regression iter. 2/29: loss=-7755.322746550251, w0=-0.296539939032285, w1=-0.27751696244002994\n",
      "Logistic regression iter. 3/29: loss=-4916.724920813895, w0=-0.36110852853424147, w1=-0.33636054652894287\n",
      "Logistic regression iter. 4/29: loss=-3214.1670746416407, w0=-0.4216545096324757, w1=-0.39169602446168117\n",
      "Logistic regression iter. 5/29: loss=-2143.023640486093, w0=-0.4797613295523677, w1=-0.4449639692075111\n",
      "Logistic regression iter. 6/29: loss=-1448.5275087154707, w0=-0.536325975333989, w1=-0.496956022979811\n",
      "Logistic regression iter. 7/29: loss=-989.0452235036571, w0=-0.5918892923944584, w1=-0.548137207492144\n",
      "Logistic regression iter. 8/29: loss=-680.6829176371152, w0=-0.646790602622026, w1=-0.598792602084176\n",
      "Logistic regression iter. 9/29: loss=-471.55170531666124, w0=-0.7012485411539962, w1=-0.649102090621675\n",
      "Logistic regression iter. 10/29: loss=-328.5719507448201, w0=-0.7554066337921543, w1=-0.6991815156083467\n",
      "Logistic regression iter. 11/29: loss=-230.19263127394797, w0=-0.8093603900953747, w1=-0.7491066033072978\n",
      "Logistic regression iter. 12/29: loss=-162.14781595517744, w0=-0.8631740258674302, w1=-0.7989274250253211\n",
      "Logistic regression iter. 13/29: loss=-114.8795948984989, w0=-0.9168910685439392, w1=-0.8486773894438554\n",
      "Logistic regression iter. 14/29: loss=-81.92410616575253, w0=-0.9705412184411493, w1=-0.8983789531268611\n",
      "Logistic regression iter. 15/29: loss=-58.87704634140015, w0=-1.0241448545502285, w1=-0.9480473052989741\n",
      "Logistic regression iter. 16/29: loss=-42.71883803455826, w0=-1.0777160277509825, w1=-0.9976927752284743\n",
      "Logistic regression iter. 17/29: loss=-31.368404982901364, w0=-1.1312644674225254, w1=-1.0473224207548044\n",
      "Logistic regression iter. 18/29: loss=-23.38495237023902, w0=-1.1847969366868067, w1=-1.0969410850815844\n",
      "Logistic regression iter. 19/29: loss=-17.766949851949693, w0=-1.2383181534525474, w1=-1.1465521046800844\n",
      "Logistic regression iter. 20/29: loss=-13.815678477853032, w0=-1.2918314197408263, w1=-1.1961577863098858\n",
      "Logistic regression iter. 21/29: loss=-11.04205426559841, w0=-1.3453390537250718, w1=-1.245759730144389\n",
      "Logistic regression iter. 22/29: loss=-9.102677639830995, w0=-1.3988426875932376, w1=-1.2953590496778986\n",
      "Logistic regression iter. 23/29: loss=-7.7557444024721995, w0=-1.4523434736992016, w1=-1.344956522024068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 24/29: loss=-6.83052745616734, w0=-1.505842227750839, w1=-1.3945526910415085\n",
      "Logistic regression iter. 25/29: loss=-6.206174976001072, w0=-1.5593395285963711, w1=-1.4441479383489606\n",
      "Logistic regression iter. 26/29: loss=-5.796931457006138, w0=-1.6128357879818385, w1=-1.4937425323939002\n",
      "Logistic regression iter. 27/29: loss=-5.541803724124996, w0=-1.666331299460272, w1=-1.543336662464716\n",
      "Logistic regression iter. 28/29: loss=-5.397313884490942, w0=-1.7198262727794902, w1=-1.592930462337104\n",
      "Logistic regression iter. 29/29: loss=-5.3324030764806345, w0=-1.773320858124471, w1=-1.6425240267604702\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-23969.640375646522, w0=-0.13849287999999998, w1=-0.13305258625282088\n",
      "Logistic regression iter. 1/29: loss=-12898.223385359397, w0=-0.22484122590463645, w1=-0.21223171024567838\n",
      "Logistic regression iter. 2/29: loss=-7755.322746550251, w0=-0.296539939032285, w1=-0.27751696244002994\n",
      "Logistic regression iter. 3/29: loss=-4916.724920813895, w0=-0.36110852853424147, w1=-0.33636054652894287\n",
      "Logistic regression iter. 4/29: loss=-3214.1670746416407, w0=-0.4216545096324757, w1=-0.39169602446168117\n",
      "Logistic regression iter. 5/29: loss=-2143.023640486093, w0=-0.4797613295523677, w1=-0.4449639692075111\n",
      "Logistic regression iter. 6/29: loss=-1448.5275087154707, w0=-0.536325975333989, w1=-0.496956022979811\n",
      "Logistic regression iter. 7/29: loss=-989.0452235036571, w0=-0.5918892923944584, w1=-0.548137207492144\n",
      "Logistic regression iter. 8/29: loss=-680.6829176371152, w0=-0.646790602622026, w1=-0.598792602084176\n",
      "Logistic regression iter. 9/29: loss=-471.55170531666124, w0=-0.7012485411539962, w1=-0.649102090621675\n",
      "Logistic regression iter. 10/29: loss=-328.5719507448201, w0=-0.7554066337921543, w1=-0.6991815156083467\n",
      "Logistic regression iter. 11/29: loss=-230.19263127394797, w0=-0.8093603900953747, w1=-0.7491066033072978\n",
      "Logistic regression iter. 12/29: loss=-162.14781595517744, w0=-0.8631740258674302, w1=-0.7989274250253211\n",
      "Logistic regression iter. 13/29: loss=-114.8795948984989, w0=-0.9168910685439392, w1=-0.8486773894438554\n",
      "Logistic regression iter. 14/29: loss=-81.92410616575253, w0=-0.9705412184411493, w1=-0.8983789531268611\n",
      "Logistic regression iter. 15/29: loss=-58.87704634140015, w0=-1.0241448545502285, w1=-0.9480473052989741\n",
      "Logistic regression iter. 16/29: loss=-42.71883803455826, w0=-1.0777160277509825, w1=-0.9976927752284743\n",
      "Logistic regression iter. 17/29: loss=-31.368404982901364, w0=-1.1312644674225254, w1=-1.0473224207548044\n",
      "Logistic regression iter. 18/29: loss=-23.38495237023902, w0=-1.1847969366868067, w1=-1.0969410850815844\n",
      "Logistic regression iter. 19/29: loss=-17.766949851949693, w0=-1.2383181534525474, w1=-1.1465521046800844\n",
      "Logistic regression iter. 20/29: loss=-13.815678477853032, w0=-1.2918314197408263, w1=-1.1961577863098858\n",
      "Logistic regression iter. 21/29: loss=-11.04205426559841, w0=-1.3453390537250718, w1=-1.245759730144389\n",
      "Logistic regression iter. 22/29: loss=-9.102677639830995, w0=-1.3988426875932376, w1=-1.2953590496778986\n",
      "Logistic regression iter. 23/29: loss=-7.7557444024721995, w0=-1.4523434736992016, w1=-1.344956522024068\n",
      "Logistic regression iter. 24/29: loss=-6.83052745616734, w0=-1.505842227750839, w1=-1.3945526910415085\n",
      "Logistic regression iter. 25/29: loss=-6.206174976001072, w0=-1.5593395285963711, w1=-1.4441479383489606\n",
      "Logistic regression iter. 26/29: loss=-5.796931457006138, w0=-1.6128357879818385, w1=-1.4937425323939002\n",
      "Logistic regression iter. 27/29: loss=-5.541803724124996, w0=-1.666331299460272, w1=-1.543336662464716\n",
      "Logistic regression iter. 28/29: loss=-5.397313884490942, w0=-1.7198262727794902, w1=-1.592930462337104\n",
      "Logistic regression iter. 29/29: loss=-5.3324030764806345, w0=-1.773320858124471, w1=-1.6425240267604702\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-21102.701633982368, w0=-0.15478615999999998, w1=-0.1487058316943292\n",
      "Logistic regression iter. 1/29: loss=-10926.522928266648, w0=-0.24709076711063355, w1=-0.23313119991911213\n",
      "Logistic regression iter. 2/29: loss=-6337.942425804588, w0=-0.32416795419134287, w1=-0.3033004222158915\n",
      "Logistic regression iter. 3/29: loss=-3877.341684316149, w0=-0.39408158651095415, w1=-0.36708704169999085\n",
      "Logistic regression iter. 4/29: loss=-2445.3391042510784, w0=-0.4600807948053984, w1=-0.42750921490318283\n",
      "Logistic regression iter. 5/29: loss=-1572.5455761272917, w0=-0.5237826134140924, w1=-0.48601340304517776\n",
      "Logistic regression iter. 6/29: loss=-1025.0834043857915, w0=-0.5860800811583088, w1=-0.5433740232991078\n",
      "Logistic regression iter. 7/29: loss=-675.0548784158267, w0=-0.6474968489604269, w1=-0.6000331785912943\n",
      "Logistic regression iter. 8/29: loss=-448.2219505924797, w0=-0.7083516710616342, w1=-0.6562536640445118\n",
      "Logistic regression iter. 9/29: loss=-299.7507158598181, w0=-0.7688434422212734, w1=-0.7121959638678017\n",
      "Logistic regression iter. 10/29: loss=-201.8182875573412, w0=-0.8290984290213256, w1=-0.7679599613875576\n",
      "Logistic regression iter. 11/29: loss=-136.82227891871085, w0=-0.8891978109537668, w1=-0.8236087001725151\n",
      "Logistic regression iter. 12/29: loss=-93.46774836725336, w0=-0.9491942896175144, w1=-0.8791824046418112\n",
      "Logistic regression iter. 13/29: loss=-64.42862965759653, w0=-1.0091223466942194, w1=-0.9347069641977362\n",
      "Logistic regression iter. 14/29: loss=-44.91257120247302, w0=-1.0690046902235393, w1=-0.9901991634029061\n",
      "Logistic regression iter. 15/29: loss=-31.762779969525305, w0=-1.1288563588107374, w1=-1.0456699521663702\n",
      "Logistic regression iter. 16/29: loss=-22.887579085948104, w0=-1.188687361478374, w1=-1.1011265133821155\n",
      "Logistic regression iter. 17/29: loss=-16.893943635792187, w0=-1.2485043897161219, w1=-1.1565735821783103\n",
      "Logistic regression iter. 18/29: loss=-12.849923856726189, w0=-1.3083119355204873, w1=-1.2120142941187295\n",
      "Logistic regression iter. 19/29: loss=-10.129469849332413, w0=-1.3681130258910041, w1=-1.2674507341575112\n",
      "Logistic regression iter. 20/29: loss=-8.310435353624259, w0=-1.4279097079099405, w1=-1.3228842939962755\n",
      "Logistic regression iter. 21/29: loss=-7.107167844409183, w0=-1.48770337062865, w1=-1.3783159059350818\n",
      "Logistic regression iter. 22/29: loss=-6.325740208039358, w0=-1.5474949595952534, w1=-1.4337461966326694\n",
      "Logistic regression iter. 23/29: loss=-5.8340886822808535, w0=-1.607285120408843, w1=-1.4891755886501774\n",
      "Logistic regression iter. 24/29: loss=-5.542013772463874, w0=-1.6670742951435644, w1=-1.5446043677844532\n",
      "Logistic regression iter. 25/29: loss=-5.387736718239727, w0=-1.726862787345912, w1=-1.6000327278867208\n",
      "Logistic regression iter. 26/29: loss=-5.328831224981081, w0=-1.7866508059951298, w1=-1.655460800801757\n",
      "Logistic regression iter. 27/29: loss=-5.336086486549594, w0=-1.846438495330334, w1=-1.7108886764349898\n",
      "Logistic regression iter. 28/29: loss=-5.389341100603913, w0=-1.9062259551497156, w1=-1.7663164162456895\n",
      "Logistic regression iter. 29/29: loss=-5.474646592526407, w0=-1.9660132546655158, w1=-1.8217440623473529\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-21102.701633982368, w0=-0.15478615999999998, w1=-0.1487058316943292\n",
      "Logistic regression iter. 1/29: loss=-10926.522928266648, w0=-0.24709076711063355, w1=-0.23313119991911213\n",
      "Logistic regression iter. 2/29: loss=-6337.942425804588, w0=-0.32416795419134287, w1=-0.3033004222158915\n",
      "Logistic regression iter. 3/29: loss=-3877.341684316149, w0=-0.39408158651095415, w1=-0.36708704169999085\n",
      "Logistic regression iter. 4/29: loss=-2445.3391042510784, w0=-0.4600807948053984, w1=-0.42750921490318283\n",
      "Logistic regression iter. 5/29: loss=-1572.5455761272917, w0=-0.5237826134140924, w1=-0.48601340304517776\n",
      "Logistic regression iter. 6/29: loss=-1025.0834043857915, w0=-0.5860800811583088, w1=-0.5433740232991078\n",
      "Logistic regression iter. 7/29: loss=-675.0548784158267, w0=-0.6474968489604269, w1=-0.6000331785912943\n",
      "Logistic regression iter. 8/29: loss=-448.2219505924797, w0=-0.7083516710616342, w1=-0.6562536640445118\n",
      "Logistic regression iter. 9/29: loss=-299.7507158598181, w0=-0.7688434422212734, w1=-0.7121959638678017\n",
      "Logistic regression iter. 10/29: loss=-201.8182875573412, w0=-0.8290984290213256, w1=-0.7679599613875576\n",
      "Logistic regression iter. 11/29: loss=-136.82227891871085, w0=-0.8891978109537668, w1=-0.8236087001725151\n",
      "Logistic regression iter. 12/29: loss=-93.46774836725336, w0=-0.9491942896175144, w1=-0.8791824046418112\n",
      "Logistic regression iter. 13/29: loss=-64.42862965759653, w0=-1.0091223466942194, w1=-0.9347069641977362\n",
      "Logistic regression iter. 14/29: loss=-44.91257120247302, w0=-1.0690046902235393, w1=-0.9901991634029061\n",
      "Logistic regression iter. 15/29: loss=-31.762779969525305, w0=-1.1288563588107374, w1=-1.0456699521663702\n",
      "Logistic regression iter. 16/29: loss=-22.887579085948104, w0=-1.188687361478374, w1=-1.1011265133821155\n",
      "Logistic regression iter. 17/29: loss=-16.893943635792187, w0=-1.2485043897161219, w1=-1.1565735821783103\n",
      "Logistic regression iter. 18/29: loss=-12.849923856726189, w0=-1.3083119355204873, w1=-1.2120142941187295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 19/29: loss=-10.129469849332413, w0=-1.3681130258910041, w1=-1.2674507341575112\n",
      "Logistic regression iter. 20/29: loss=-8.310435353624259, w0=-1.4279097079099405, w1=-1.3228842939962755\n",
      "Logistic regression iter. 21/29: loss=-7.107167844409183, w0=-1.48770337062865, w1=-1.3783159059350818\n",
      "Logistic regression iter. 22/29: loss=-6.325740208039358, w0=-1.5474949595952534, w1=-1.4337461966326694\n",
      "Logistic regression iter. 23/29: loss=-5.8340886822808535, w0=-1.607285120408843, w1=-1.4891755886501774\n",
      "Logistic regression iter. 24/29: loss=-5.542013772463874, w0=-1.6670742951435644, w1=-1.5446043677844532\n",
      "Logistic regression iter. 25/29: loss=-5.387736718239727, w0=-1.726862787345912, w1=-1.6000327278867208\n",
      "Logistic regression iter. 26/29: loss=-5.328831224981081, w0=-1.7866508059951298, w1=-1.655460800801757\n",
      "Logistic regression iter. 27/29: loss=-5.336086486549594, w0=-1.846438495330334, w1=-1.7108886764349898\n",
      "Logistic regression iter. 28/29: loss=-5.389341100603913, w0=-1.9062259551497156, w1=-1.7663164162456895\n",
      "Logistic regression iter. 29/29: loss=-5.474646592526407, w0=-1.9660132546655158, w1=-1.8217440623473529\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-21102.701633982368, w0=-0.15478615999999998, w1=-0.1487058316943292\n",
      "Logistic regression iter. 1/29: loss=-10926.522928266648, w0=-0.24709076711063355, w1=-0.23313119991911213\n",
      "Logistic regression iter. 2/29: loss=-6337.942425804588, w0=-0.32416795419134287, w1=-0.3033004222158915\n",
      "Logistic regression iter. 3/29: loss=-3877.341684316149, w0=-0.39408158651095415, w1=-0.36708704169999085\n",
      "Logistic regression iter. 4/29: loss=-2445.3391042510784, w0=-0.4600807948053984, w1=-0.42750921490318283\n",
      "Logistic regression iter. 5/29: loss=-1572.5455761272917, w0=-0.5237826134140924, w1=-0.48601340304517776\n",
      "Logistic regression iter. 6/29: loss=-1025.0834043857915, w0=-0.5860800811583088, w1=-0.5433740232991078\n",
      "Logistic regression iter. 7/29: loss=-675.0548784158267, w0=-0.6474968489604269, w1=-0.6000331785912943\n",
      "Logistic regression iter. 8/29: loss=-448.2219505924797, w0=-0.7083516710616342, w1=-0.6562536640445118\n",
      "Logistic regression iter. 9/29: loss=-299.7507158598181, w0=-0.7688434422212734, w1=-0.7121959638678017\n",
      "Logistic regression iter. 10/29: loss=-201.8182875573412, w0=-0.8290984290213256, w1=-0.7679599613875576\n",
      "Logistic regression iter. 11/29: loss=-136.82227891871085, w0=-0.8891978109537668, w1=-0.8236087001725151\n",
      "Logistic regression iter. 12/29: loss=-93.46774836725336, w0=-0.9491942896175144, w1=-0.8791824046418112\n",
      "Logistic regression iter. 13/29: loss=-64.42862965759653, w0=-1.0091223466942194, w1=-0.9347069641977362\n",
      "Logistic regression iter. 14/29: loss=-44.91257120247302, w0=-1.0690046902235393, w1=-0.9901991634029061\n",
      "Logistic regression iter. 15/29: loss=-31.762779969525305, w0=-1.1288563588107374, w1=-1.0456699521663702\n",
      "Logistic regression iter. 16/29: loss=-22.887579085948104, w0=-1.188687361478374, w1=-1.1011265133821155\n",
      "Logistic regression iter. 17/29: loss=-16.893943635792187, w0=-1.2485043897161219, w1=-1.1565735821783103\n",
      "Logistic regression iter. 18/29: loss=-12.849923856726189, w0=-1.3083119355204873, w1=-1.2120142941187295\n",
      "Logistic regression iter. 19/29: loss=-10.129469849332413, w0=-1.3681130258910041, w1=-1.2674507341575112\n",
      "Logistic regression iter. 20/29: loss=-8.310435353624259, w0=-1.4279097079099405, w1=-1.3228842939962755\n",
      "Logistic regression iter. 21/29: loss=-7.107167844409183, w0=-1.48770337062865, w1=-1.3783159059350818\n",
      "Logistic regression iter. 22/29: loss=-6.325740208039358, w0=-1.5474949595952534, w1=-1.4337461966326694\n",
      "Logistic regression iter. 23/29: loss=-5.8340886822808535, w0=-1.607285120408843, w1=-1.4891755886501774\n",
      "Logistic regression iter. 24/29: loss=-5.542013772463874, w0=-1.6670742951435644, w1=-1.5446043677844532\n",
      "Logistic regression iter. 25/29: loss=-5.387736718239727, w0=-1.726862787345912, w1=-1.6000327278867208\n",
      "Logistic regression iter. 26/29: loss=-5.328831224981081, w0=-1.7866508059951298, w1=-1.655460800801757\n",
      "Logistic regression iter. 27/29: loss=-5.336086486549594, w0=-1.846438495330334, w1=-1.7108886764349898\n",
      "Logistic regression iter. 28/29: loss=-5.389341100603913, w0=-1.9062259551497156, w1=-1.7663164162456895\n",
      "Logistic regression iter. 29/29: loss=-5.474646592526407, w0=-1.9660132546655158, w1=-1.8217440623473529\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-21102.701633982368, w0=-0.15478615999999998, w1=-0.1487058316943292\n",
      "Logistic regression iter. 1/29: loss=-10926.522928266648, w0=-0.24709076711063355, w1=-0.23313119991911213\n",
      "Logistic regression iter. 2/29: loss=-6337.942425804588, w0=-0.32416795419134287, w1=-0.3033004222158915\n",
      "Logistic regression iter. 3/29: loss=-3877.341684316149, w0=-0.39408158651095415, w1=-0.36708704169999085\n",
      "Logistic regression iter. 4/29: loss=-2445.3391042510784, w0=-0.4600807948053984, w1=-0.42750921490318283\n",
      "Logistic regression iter. 5/29: loss=-1572.5455761272917, w0=-0.5237826134140924, w1=-0.48601340304517776\n",
      "Logistic regression iter. 6/29: loss=-1025.0834043857915, w0=-0.5860800811583088, w1=-0.5433740232991078\n",
      "Logistic regression iter. 7/29: loss=-675.0548784158267, w0=-0.6474968489604269, w1=-0.6000331785912943\n",
      "Logistic regression iter. 8/29: loss=-448.2219505924797, w0=-0.7083516710616342, w1=-0.6562536640445118\n",
      "Logistic regression iter. 9/29: loss=-299.7507158598181, w0=-0.7688434422212734, w1=-0.7121959638678017\n",
      "Logistic regression iter. 10/29: loss=-201.8182875573412, w0=-0.8290984290213256, w1=-0.7679599613875576\n",
      "Logistic regression iter. 11/29: loss=-136.82227891871085, w0=-0.8891978109537668, w1=-0.8236087001725151\n",
      "Logistic regression iter. 12/29: loss=-93.46774836725336, w0=-0.9491942896175144, w1=-0.8791824046418112\n",
      "Logistic regression iter. 13/29: loss=-64.42862965759653, w0=-1.0091223466942194, w1=-0.9347069641977362\n",
      "Logistic regression iter. 14/29: loss=-44.91257120247302, w0=-1.0690046902235393, w1=-0.9901991634029061\n",
      "Logistic regression iter. 15/29: loss=-31.762779969525305, w0=-1.1288563588107374, w1=-1.0456699521663702\n",
      "Logistic regression iter. 16/29: loss=-22.887579085948104, w0=-1.188687361478374, w1=-1.1011265133821155\n",
      "Logistic regression iter. 17/29: loss=-16.893943635792187, w0=-1.2485043897161219, w1=-1.1565735821783103\n",
      "Logistic regression iter. 18/29: loss=-12.849923856726189, w0=-1.3083119355204873, w1=-1.2120142941187295\n",
      "Logistic regression iter. 19/29: loss=-10.129469849332413, w0=-1.3681130258910041, w1=-1.2674507341575112\n",
      "Logistic regression iter. 20/29: loss=-8.310435353624259, w0=-1.4279097079099405, w1=-1.3228842939962755\n",
      "Logistic regression iter. 21/29: loss=-7.107167844409183, w0=-1.48770337062865, w1=-1.3783159059350818\n",
      "Logistic regression iter. 22/29: loss=-6.325740208039358, w0=-1.5474949595952534, w1=-1.4337461966326694\n",
      "Logistic regression iter. 23/29: loss=-5.8340886822808535, w0=-1.607285120408843, w1=-1.4891755886501774\n",
      "Logistic regression iter. 24/29: loss=-5.542013772463874, w0=-1.6670742951435644, w1=-1.5446043677844532\n",
      "Logistic regression iter. 25/29: loss=-5.387736718239727, w0=-1.726862787345912, w1=-1.6000327278867208\n",
      "Logistic regression iter. 26/29: loss=-5.328831224981081, w0=-1.7866508059951298, w1=-1.655460800801757\n",
      "Logistic regression iter. 27/29: loss=-5.336086486549594, w0=-1.846438495330334, w1=-1.7108886764349898\n",
      "Logistic regression iter. 28/29: loss=-5.389341100603913, w0=-1.9062259551497156, w1=-1.7663164162456895\n",
      "Logistic regression iter. 29/29: loss=-5.474646592526407, w0=-1.9660132546655158, w1=-1.8217440623473529\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-18591.321236193213, w0=-0.17107943999999997, w1=-0.16435907713583753\n",
      "Logistic regression iter. 1/29: loss=-9285.590518079529, w0=-0.26897334702586434, w1=-0.2537234769120717\n",
      "Logistic regression iter. 2/29: loss=-5200.461346596306, w0=-0.3513279326976703, w1=-0.3287180728184058\n",
      "Logistic regression iter. 3/29: loss=-3070.951404262926, w0=-0.42659387516595926, w1=-0.3974783079104447\n",
      "Logistic regression iter. 4/29: loss=-1868.7218339939604, w0=-0.4981050851134647, w1=-0.46305481741899535\n",
      "Logistic regression iter. 5/29: loss=-1159.2130231294718, w0=-0.567483325820479, w1=-0.5268756750506524\n",
      "Logistic regression iter. 6/29: loss=-728.8991213492773, w0=-0.6356013027580465, w1=-0.589685406472782\n",
      "Logistic regression iter. 7/29: loss=-463.1432323807303, w0=-0.7029561515568065, w1=-0.6518966789173897\n",
      "Logistic regression iter. 8/29: loss=-296.89948415381815, w0=-0.7698411544210917, w1=-0.7137470824904354\n",
      "Logistic regression iter. 9/29: loss=-191.90641982090426, w0=-0.8364333615194005, w1=-0.7753769061889995\n",
      "Logistic regression iter. 10/29: loss=-125.10135221768377, w0=-0.9028413913044904, w1=-0.836870475786507\n",
      "Logistic regression iter. 11/29: loss=-82.34068749768491, w0=-0.9691326815145911, w1=-0.8982791575451468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 12/29: loss=-54.839081263938276, w0=-1.0353494961295016, w1=-0.9596345697012842\n",
      "Logistic regression iter. 13/29: loss=-37.08524566596423, w0=-1.1015185270456345, w1=-1.0209563424944477\n",
      "Logistic regression iter. 14/29: loss=-25.59406098174685, w0=-1.167656742704957, w1=-1.082256752727068\n",
      "Logistic regression iter. 15/29: loss=-18.14684811224334, w0=-1.2337749927599655, w1=-1.1435435275559125\n",
      "Logistic regression iter. 16/29: loss=-13.322935010084487, w0=-1.2998802507053626, w1=-1.2048215583881028\n",
      "Logistic regression iter. 17/29: loss=-10.207925796471628, w0=-1.3659770200855132, w1=-1.2660939576504497\n",
      "Logistic regression iter. 18/29: loss=-8.210524305220542, w0=-1.4320682221961645, w1=-1.3273627152489953\n",
      "Logistic regression iter. 19/29: loss=-6.946720234129442, w0=-1.498155759816874, w1=-1.3886291090417775\n",
      "Logistic regression iter. 20/29: loss=-6.166123696470087, w0=-1.564240877139261, w1=-1.4498939630166334\n",
      "Logistic regression iter. 21/29: loss=-5.704857412757698, w0=-1.630324390708274, w1=-1.511157810552655\n",
      "Logistic regression iter. 22/29: loss=-5.455285440169489, w0=-1.6964068382808644, w1=-1.5724209981715653\n",
      "Logistic regression iter. 23/29: loss=-5.346473937820677, w0=-1.7624885751877883, w1=-1.6336837517739224\n",
      "Logistic regression iter. 24/29: loss=-5.331526695128558, w0=-1.8285698369638907, w1=-1.6949462191066471\n",
      "Logistic regression iter. 25/29: loss=-5.379344581231685, w0=-1.894650780209901, w1=-1.75620849710086\n",
      "Logistic regression iter. 26/29: loss=-5.469243848187911, w0=-1.960731509348377, w1=-1.8174706495369883\n",
      "Logistic regression iter. 27/29: loss=-5.587429230298691, w0=-2.0268120942036543, w1=-1.8787327185005076\n",
      "Logistic regression iter. 28/29: loss=-5.724674919202707, w0=-2.0928925815907373, w1=-1.9399947318361945\n",
      "Logistic regression iter. 29/29: loss=-5.874794917990621, w0=-2.1589730029788656, w1=-2.0012567080142767\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-18591.321236193213, w0=-0.17107943999999997, w1=-0.16435907713583753\n",
      "Logistic regression iter. 1/29: loss=-9285.590518079529, w0=-0.26897334702586434, w1=-0.2537234769120717\n",
      "Logistic regression iter. 2/29: loss=-5200.461346596306, w0=-0.3513279326976703, w1=-0.3287180728184058\n",
      "Logistic regression iter. 3/29: loss=-3070.951404262926, w0=-0.42659387516595926, w1=-0.3974783079104447\n",
      "Logistic regression iter. 4/29: loss=-1868.7218339939604, w0=-0.4981050851134647, w1=-0.46305481741899535\n",
      "Logistic regression iter. 5/29: loss=-1159.2130231294718, w0=-0.567483325820479, w1=-0.5268756750506524\n",
      "Logistic regression iter. 6/29: loss=-728.8991213492773, w0=-0.6356013027580465, w1=-0.589685406472782\n",
      "Logistic regression iter. 7/29: loss=-463.1432323807303, w0=-0.7029561515568065, w1=-0.6518966789173897\n",
      "Logistic regression iter. 8/29: loss=-296.89948415381815, w0=-0.7698411544210917, w1=-0.7137470824904354\n",
      "Logistic regression iter. 9/29: loss=-191.90641982090426, w0=-0.8364333615194005, w1=-0.7753769061889995\n",
      "Logistic regression iter. 10/29: loss=-125.10135221768377, w0=-0.9028413913044904, w1=-0.836870475786507\n",
      "Logistic regression iter. 11/29: loss=-82.34068749768491, w0=-0.9691326815145911, w1=-0.8982791575451468\n",
      "Logistic regression iter. 12/29: loss=-54.839081263938276, w0=-1.0353494961295016, w1=-0.9596345697012842\n",
      "Logistic regression iter. 13/29: loss=-37.08524566596423, w0=-1.1015185270456345, w1=-1.0209563424944477\n",
      "Logistic regression iter. 14/29: loss=-25.59406098174685, w0=-1.167656742704957, w1=-1.082256752727068\n",
      "Logistic regression iter. 15/29: loss=-18.14684811224334, w0=-1.2337749927599655, w1=-1.1435435275559125\n",
      "Logistic regression iter. 16/29: loss=-13.322935010084487, w0=-1.2998802507053626, w1=-1.2048215583881028\n",
      "Logistic regression iter. 17/29: loss=-10.207925796471628, w0=-1.3659770200855132, w1=-1.2660939576504497\n",
      "Logistic regression iter. 18/29: loss=-8.210524305220542, w0=-1.4320682221961645, w1=-1.3273627152489953\n",
      "Logistic regression iter. 19/29: loss=-6.946720234129442, w0=-1.498155759816874, w1=-1.3886291090417775\n",
      "Logistic regression iter. 20/29: loss=-6.166123696470087, w0=-1.564240877139261, w1=-1.4498939630166334\n",
      "Logistic regression iter. 21/29: loss=-5.704857412757698, w0=-1.630324390708274, w1=-1.511157810552655\n",
      "Logistic regression iter. 22/29: loss=-5.455285440169489, w0=-1.6964068382808644, w1=-1.5724209981715653\n",
      "Logistic regression iter. 23/29: loss=-5.346473937820677, w0=-1.7624885751877883, w1=-1.6336837517739224\n",
      "Logistic regression iter. 24/29: loss=-5.331526695128558, w0=-1.8285698369638907, w1=-1.6949462191066471\n",
      "Logistic regression iter. 25/29: loss=-5.379344581231685, w0=-1.894650780209901, w1=-1.75620849710086\n",
      "Logistic regression iter. 26/29: loss=-5.469243848187911, w0=-1.960731509348377, w1=-1.8174706495369883\n",
      "Logistic regression iter. 27/29: loss=-5.587429230298691, w0=-2.0268120942036543, w1=-1.8787327185005076\n",
      "Logistic regression iter. 28/29: loss=-5.724674919202707, w0=-2.0928925815907373, w1=-1.9399947318361945\n",
      "Logistic regression iter. 29/29: loss=-5.874794917990621, w0=-2.1589730029788656, w1=-2.0012567080142767\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-18591.321236193213, w0=-0.17107943999999997, w1=-0.16435907713583753\n",
      "Logistic regression iter. 1/29: loss=-9285.590518079529, w0=-0.26897334702586434, w1=-0.2537234769120717\n",
      "Logistic regression iter. 2/29: loss=-5200.461346596306, w0=-0.3513279326976703, w1=-0.3287180728184058\n",
      "Logistic regression iter. 3/29: loss=-3070.951404262926, w0=-0.42659387516595926, w1=-0.3974783079104447\n",
      "Logistic regression iter. 4/29: loss=-1868.7218339939604, w0=-0.4981050851134647, w1=-0.46305481741899535\n",
      "Logistic regression iter. 5/29: loss=-1159.2130231294718, w0=-0.567483325820479, w1=-0.5268756750506524\n",
      "Logistic regression iter. 6/29: loss=-728.8991213492773, w0=-0.6356013027580465, w1=-0.589685406472782\n",
      "Logistic regression iter. 7/29: loss=-463.1432323807303, w0=-0.7029561515568065, w1=-0.6518966789173897\n",
      "Logistic regression iter. 8/29: loss=-296.89948415381815, w0=-0.7698411544210917, w1=-0.7137470824904354\n",
      "Logistic regression iter. 9/29: loss=-191.90641982090426, w0=-0.8364333615194005, w1=-0.7753769061889995\n",
      "Logistic regression iter. 10/29: loss=-125.10135221768377, w0=-0.9028413913044904, w1=-0.836870475786507\n",
      "Logistic regression iter. 11/29: loss=-82.34068749768491, w0=-0.9691326815145911, w1=-0.8982791575451468\n",
      "Logistic regression iter. 12/29: loss=-54.839081263938276, w0=-1.0353494961295016, w1=-0.9596345697012842\n",
      "Logistic regression iter. 13/29: loss=-37.08524566596423, w0=-1.1015185270456345, w1=-1.0209563424944477\n",
      "Logistic regression iter. 14/29: loss=-25.59406098174685, w0=-1.167656742704957, w1=-1.082256752727068\n",
      "Logistic regression iter. 15/29: loss=-18.14684811224334, w0=-1.2337749927599655, w1=-1.1435435275559125\n",
      "Logistic regression iter. 16/29: loss=-13.322935010084487, w0=-1.2998802507053626, w1=-1.2048215583881028\n",
      "Logistic regression iter. 17/29: loss=-10.207925796471628, w0=-1.3659770200855132, w1=-1.2660939576504497\n",
      "Logistic regression iter. 18/29: loss=-8.210524305220542, w0=-1.4320682221961645, w1=-1.3273627152489953\n",
      "Logistic regression iter. 19/29: loss=-6.946720234129442, w0=-1.498155759816874, w1=-1.3886291090417775\n",
      "Logistic regression iter. 20/29: loss=-6.166123696470087, w0=-1.564240877139261, w1=-1.4498939630166334\n",
      "Logistic regression iter. 21/29: loss=-5.704857412757698, w0=-1.630324390708274, w1=-1.511157810552655\n",
      "Logistic regression iter. 22/29: loss=-5.455285440169489, w0=-1.6964068382808644, w1=-1.5724209981715653\n",
      "Logistic regression iter. 23/29: loss=-5.346473937820677, w0=-1.7624885751877883, w1=-1.6336837517739224\n",
      "Logistic regression iter. 24/29: loss=-5.331526695128558, w0=-1.8285698369638907, w1=-1.6949462191066471\n",
      "Logistic regression iter. 25/29: loss=-5.379344581231685, w0=-1.894650780209901, w1=-1.75620849710086\n",
      "Logistic regression iter. 26/29: loss=-5.469243848187911, w0=-1.960731509348377, w1=-1.8174706495369883\n",
      "Logistic regression iter. 27/29: loss=-5.587429230298691, w0=-2.0268120942036543, w1=-1.8787327185005076\n",
      "Logistic regression iter. 28/29: loss=-5.724674919202707, w0=-2.0928925815907373, w1=-1.9399947318361945\n",
      "Logistic regression iter. 29/29: loss=-5.874794917990621, w0=-2.1589730029788656, w1=-2.0012567080142767\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-18591.321236193213, w0=-0.17107943999999997, w1=-0.16435907713583753\n",
      "Logistic regression iter. 1/29: loss=-9285.590518079529, w0=-0.26897334702586434, w1=-0.2537234769120717\n",
      "Logistic regression iter. 2/29: loss=-5200.461346596306, w0=-0.3513279326976703, w1=-0.3287180728184058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 3/29: loss=-3070.951404262926, w0=-0.42659387516595926, w1=-0.3974783079104447\n",
      "Logistic regression iter. 4/29: loss=-1868.7218339939604, w0=-0.4981050851134647, w1=-0.46305481741899535\n",
      "Logistic regression iter. 5/29: loss=-1159.2130231294718, w0=-0.567483325820479, w1=-0.5268756750506524\n",
      "Logistic regression iter. 6/29: loss=-728.8991213492773, w0=-0.6356013027580465, w1=-0.589685406472782\n",
      "Logistic regression iter. 7/29: loss=-463.1432323807303, w0=-0.7029561515568065, w1=-0.6518966789173897\n",
      "Logistic regression iter. 8/29: loss=-296.89948415381815, w0=-0.7698411544210917, w1=-0.7137470824904354\n",
      "Logistic regression iter. 9/29: loss=-191.90641982090426, w0=-0.8364333615194005, w1=-0.7753769061889995\n",
      "Logistic regression iter. 10/29: loss=-125.10135221768377, w0=-0.9028413913044904, w1=-0.836870475786507\n",
      "Logistic regression iter. 11/29: loss=-82.34068749768491, w0=-0.9691326815145911, w1=-0.8982791575451468\n",
      "Logistic regression iter. 12/29: loss=-54.839081263938276, w0=-1.0353494961295016, w1=-0.9596345697012842\n",
      "Logistic regression iter. 13/29: loss=-37.08524566596423, w0=-1.1015185270456345, w1=-1.0209563424944477\n",
      "Logistic regression iter. 14/29: loss=-25.59406098174685, w0=-1.167656742704957, w1=-1.082256752727068\n",
      "Logistic regression iter. 15/29: loss=-18.14684811224334, w0=-1.2337749927599655, w1=-1.1435435275559125\n",
      "Logistic regression iter. 16/29: loss=-13.322935010084487, w0=-1.2998802507053626, w1=-1.2048215583881028\n",
      "Logistic regression iter. 17/29: loss=-10.207925796471628, w0=-1.3659770200855132, w1=-1.2660939576504497\n",
      "Logistic regression iter. 18/29: loss=-8.210524305220542, w0=-1.4320682221961645, w1=-1.3273627152489953\n",
      "Logistic regression iter. 19/29: loss=-6.946720234129442, w0=-1.498155759816874, w1=-1.3886291090417775\n",
      "Logistic regression iter. 20/29: loss=-6.166123696470087, w0=-1.564240877139261, w1=-1.4498939630166334\n",
      "Logistic regression iter. 21/29: loss=-5.704857412757698, w0=-1.630324390708274, w1=-1.511157810552655\n",
      "Logistic regression iter. 22/29: loss=-5.455285440169489, w0=-1.6964068382808644, w1=-1.5724209981715653\n",
      "Logistic regression iter. 23/29: loss=-5.346473937820677, w0=-1.7624885751877883, w1=-1.6336837517739224\n",
      "Logistic regression iter. 24/29: loss=-5.331526695128558, w0=-1.8285698369638907, w1=-1.6949462191066471\n",
      "Logistic regression iter. 25/29: loss=-5.379344581231685, w0=-1.894650780209901, w1=-1.75620849710086\n",
      "Logistic regression iter. 26/29: loss=-5.469243848187911, w0=-1.960731509348377, w1=-1.8174706495369883\n",
      "Logistic regression iter. 27/29: loss=-5.587429230298691, w0=-2.0268120942036543, w1=-1.8787327185005076\n",
      "Logistic regression iter. 28/29: loss=-5.724674919202707, w0=-2.0928925815907373, w1=-1.9399947318361945\n",
      "Logistic regression iter. 29/29: loss=-5.874794917990621, w0=-2.1589730029788656, w1=-2.0012567080142767\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-16390.26866662165, w0=-0.18737271999999996, w1=-0.1800123225773459\n",
      "Logistic regression iter. 1/29: loss=-7910.954556183523, w0=-0.29058158172547277, w1=-0.27409798988281575\n",
      "Logistic regression iter. 2/29: loss=-4280.292199023439, w0=-0.3781600722407753, w1=-0.35389876642857065\n",
      "Logistic regression iter. 3/29: loss=-2440.22696398765, w0=-0.4588099096275718, w1=-0.4276798999885534\n",
      "Logistic regression iter. 4/29: loss=-1432.8781001915297, w0=-0.5359009750005781, w1=-0.4984816574925238\n",
      "Logistic regression iter. 5/29: loss=-857.5282540993115, w0=-0.6110355590573103, w1=-0.5676943588688736\n",
      "Logistic regression iter. 6/29: loss=-520.2840673800262, w0=-0.6850531071776198, w1=-0.6360232902305556\n",
      "Logistic regression iter. 7/29: loss=-319.17101287802006, w0=-0.7584176916463043, w1=-0.7038476096443487\n",
      "Logistic regression iter. 8/29: loss=-197.7580172554904, w0=-0.8313943977541998, w1=-0.7713785607826651\n",
      "Logistic regression iter. 9/29: loss=-123.78057319356542, w0=-0.9041379402041734, w1=-0.83873666264457\n",
      "Logistic regression iter. 10/29: loss=-78.38030435621607, w0=-0.9767400018446362, w1=-0.9059918471975783\n",
      "Logistic regression iter. 11/29: loss=-50.360171956081814, w0=-1.049255542428445, w1=-0.9731852176494259\n",
      "Logistic regression iter. 12/29: loss=-32.992983681935115, w0=-1.1217178134392827, w1=-1.0403411828932976\n",
      "Logistic regression iter. 13/29: loss=-22.199013494584502, w0=-1.1941470892947839, w1=-1.1074743635317108\n",
      "Logistic regression iter. 14/29: loss=-15.48481878586807, w0=-1.2665558158299788, w1=-1.1745935827337073\n",
      "Logistic regression iter. 15/29: loss=-11.316250528816834, w0=-1.338951679548405, w1=-1.2417042004520533\n",
      "Logistic regression iter. 16/29: loss=-8.743792655767988, w0=-1.411339453878315, w1=-1.3088094923282017\n",
      "Logistic regression iter. 17/29: loss=-7.1766215088051, w0=-1.4837221183366165, w1=-1.3759114712061835\n",
      "Logistic regression iter. 18/29: loss=-6.2452959872594995, w0=-1.556101541562806, w1=-1.4430113802285485\n",
      "Logistic regression iter. 19/29: loss=-5.717749099967966, w0=-1.6284789007373404, w1=-1.5101099907881534\n",
      "Logistic regression iter. 20/29: loss=-5.4475819412026345, w0=-1.7008549405735804, w1=-1.5772077836601508\n",
      "Logistic regression iter. 21/29: loss=-5.3420660513269995, w0=-1.7732301340785996, w1=-1.6443050597321214\n",
      "Logistic regression iter. 22/29: loss=-5.3422427668141665, w0=-1.8456047828255637, w1=-1.711402008046597\n",
      "Logistic regression iter. 23/29: loss=-5.4104884213646205, w0=-1.917979079783224, w1=-1.7784987478144034\n",
      "Logistic regression iter. 24/29: loss=-5.522710524173932, w0=-1.9903531488541413, w1=-1.8455953544734567\n",
      "Logistic regression iter. 25/29: loss=-5.663430011697354, w0=-2.0627270698577105, w1=-1.912691875920514\n",
      "Logistic regression iter. 26/29: loss=-5.822670084221536, w0=-2.1351008943776346, w1=-1.9797883426623488\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-2.207474655851755, w1=-2.0468847741883094\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-2.279848376018861, w1=-2.113981182985206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\basti\\ML\\The_MadLads\\logistic_regression.py:37: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -np.sum(np.dot(y.T,np.log(y_pred)+ np.dot((1-y).T, np.log(1-y_pred)))) /(len(y_pred))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 29/29: loss=nan, w0=-2.3522220690518, w1=-2.1810775770754662\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-16390.26866662165, w0=-0.18737271999999996, w1=-0.1800123225773459\n",
      "Logistic regression iter. 1/29: loss=-7910.954556183523, w0=-0.29058158172547277, w1=-0.27409798988281575\n",
      "Logistic regression iter. 2/29: loss=-4280.292199023439, w0=-0.3781600722407753, w1=-0.35389876642857065\n",
      "Logistic regression iter. 3/29: loss=-2440.22696398765, w0=-0.4588099096275718, w1=-0.4276798999885534\n",
      "Logistic regression iter. 4/29: loss=-1432.8781001915297, w0=-0.5359009750005781, w1=-0.4984816574925238\n",
      "Logistic regression iter. 5/29: loss=-857.5282540993115, w0=-0.6110355590573103, w1=-0.5676943588688736\n",
      "Logistic regression iter. 6/29: loss=-520.2840673800262, w0=-0.6850531071776198, w1=-0.6360232902305556\n",
      "Logistic regression iter. 7/29: loss=-319.17101287802006, w0=-0.7584176916463043, w1=-0.7038476096443487\n",
      "Logistic regression iter. 8/29: loss=-197.7580172554904, w0=-0.8313943977541998, w1=-0.7713785607826651\n",
      "Logistic regression iter. 9/29: loss=-123.78057319356542, w0=-0.9041379402041734, w1=-0.83873666264457\n",
      "Logistic regression iter. 10/29: loss=-78.38030435621607, w0=-0.9767400018446362, w1=-0.9059918471975783\n",
      "Logistic regression iter. 11/29: loss=-50.360171956081814, w0=-1.049255542428445, w1=-0.9731852176494259\n",
      "Logistic regression iter. 12/29: loss=-32.992983681935115, w0=-1.1217178134392827, w1=-1.0403411828932976\n",
      "Logistic regression iter. 13/29: loss=-22.199013494584502, w0=-1.1941470892947839, w1=-1.1074743635317108\n",
      "Logistic regression iter. 14/29: loss=-15.48481878586807, w0=-1.2665558158299788, w1=-1.1745935827337073\n",
      "Logistic regression iter. 15/29: loss=-11.316250528816834, w0=-1.338951679548405, w1=-1.2417042004520533\n",
      "Logistic regression iter. 16/29: loss=-8.743792655767988, w0=-1.411339453878315, w1=-1.3088094923282017\n",
      "Logistic regression iter. 17/29: loss=-7.1766215088051, w0=-1.4837221183366165, w1=-1.3759114712061835\n",
      "Logistic regression iter. 18/29: loss=-6.2452959872594995, w0=-1.556101541562806, w1=-1.4430113802285485\n",
      "Logistic regression iter. 19/29: loss=-5.717749099967966, w0=-1.6284789007373404, w1=-1.5101099907881534\n",
      "Logistic regression iter. 20/29: loss=-5.4475819412026345, w0=-1.7008549405735804, w1=-1.5772077836601508\n",
      "Logistic regression iter. 21/29: loss=-5.3420660513269995, w0=-1.7732301340785996, w1=-1.6443050597321214\n",
      "Logistic regression iter. 22/29: loss=-5.3422427668141665, w0=-1.8456047828255637, w1=-1.711402008046597\n",
      "Logistic regression iter. 23/29: loss=-5.4104884213646205, w0=-1.917979079783224, w1=-1.7784987478144034\n",
      "Logistic regression iter. 24/29: loss=-5.522710524173932, w0=-1.9903531488541413, w1=-1.8455953544734567\n",
      "Logistic regression iter. 25/29: loss=-5.663430011697354, w0=-2.0627270698577105, w1=-1.912691875920514\n",
      "Logistic regression iter. 26/29: loss=-5.822670084221536, w0=-2.1351008943776346, w1=-1.9797883426623488\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-2.207474655851755, w1=-2.0468847741883094\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-2.279848376018861, w1=-2.113981182985206\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-2.3522220690518, w1=-2.1810775770754662\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-16390.26866662165, w0=-0.18737271999999996, w1=-0.1800123225773459\n",
      "Logistic regression iter. 1/29: loss=-7910.954556183523, w0=-0.29058158172547277, w1=-0.27409798988281575\n",
      "Logistic regression iter. 2/29: loss=-4280.292199023439, w0=-0.3781600722407753, w1=-0.35389876642857065\n",
      "Logistic regression iter. 3/29: loss=-2440.22696398765, w0=-0.4588099096275718, w1=-0.4276798999885534\n",
      "Logistic regression iter. 4/29: loss=-1432.8781001915297, w0=-0.5359009750005781, w1=-0.4984816574925238\n",
      "Logistic regression iter. 5/29: loss=-857.5282540993115, w0=-0.6110355590573103, w1=-0.5676943588688736\n",
      "Logistic regression iter. 6/29: loss=-520.2840673800262, w0=-0.6850531071776198, w1=-0.6360232902305556\n",
      "Logistic regression iter. 7/29: loss=-319.17101287802006, w0=-0.7584176916463043, w1=-0.7038476096443487\n",
      "Logistic regression iter. 8/29: loss=-197.7580172554904, w0=-0.8313943977541998, w1=-0.7713785607826651\n",
      "Logistic regression iter. 9/29: loss=-123.78057319356542, w0=-0.9041379402041734, w1=-0.83873666264457\n",
      "Logistic regression iter. 10/29: loss=-78.38030435621607, w0=-0.9767400018446362, w1=-0.9059918471975783\n",
      "Logistic regression iter. 11/29: loss=-50.360171956081814, w0=-1.049255542428445, w1=-0.9731852176494259\n",
      "Logistic regression iter. 12/29: loss=-32.992983681935115, w0=-1.1217178134392827, w1=-1.0403411828932976\n",
      "Logistic regression iter. 13/29: loss=-22.199013494584502, w0=-1.1941470892947839, w1=-1.1074743635317108\n",
      "Logistic regression iter. 14/29: loss=-15.48481878586807, w0=-1.2665558158299788, w1=-1.1745935827337073\n",
      "Logistic regression iter. 15/29: loss=-11.316250528816834, w0=-1.338951679548405, w1=-1.2417042004520533\n",
      "Logistic regression iter. 16/29: loss=-8.743792655767988, w0=-1.411339453878315, w1=-1.3088094923282017\n",
      "Logistic regression iter. 17/29: loss=-7.1766215088051, w0=-1.4837221183366165, w1=-1.3759114712061835\n",
      "Logistic regression iter. 18/29: loss=-6.2452959872594995, w0=-1.556101541562806, w1=-1.4430113802285485\n",
      "Logistic regression iter. 19/29: loss=-5.717749099967966, w0=-1.6284789007373404, w1=-1.5101099907881534\n",
      "Logistic regression iter. 20/29: loss=-5.4475819412026345, w0=-1.7008549405735804, w1=-1.5772077836601508\n",
      "Logistic regression iter. 21/29: loss=-5.3420660513269995, w0=-1.7732301340785996, w1=-1.6443050597321214\n",
      "Logistic regression iter. 22/29: loss=-5.3422427668141665, w0=-1.8456047828255637, w1=-1.711402008046597\n",
      "Logistic regression iter. 23/29: loss=-5.4104884213646205, w0=-1.917979079783224, w1=-1.7784987478144034\n",
      "Logistic regression iter. 24/29: loss=-5.522710524173932, w0=-1.9903531488541413, w1=-1.8455953544734567\n",
      "Logistic regression iter. 25/29: loss=-5.663430011697354, w0=-2.0627270698577105, w1=-1.912691875920514\n",
      "Logistic regression iter. 26/29: loss=-5.822670084221536, w0=-2.1351008943776346, w1=-1.9797883426623488\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-2.207474655851755, w1=-2.0468847741883094\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-2.279848376018861, w1=-2.113981182985206\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-2.3522220690518, w1=-2.1810775770754662\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-16390.26866662165, w0=-0.18737271999999996, w1=-0.1800123225773459\n",
      "Logistic regression iter. 1/29: loss=-7910.954556183523, w0=-0.29058158172547277, w1=-0.27409798988281575\n",
      "Logistic regression iter. 2/29: loss=-4280.292199023439, w0=-0.3781600722407753, w1=-0.35389876642857065\n",
      "Logistic regression iter. 3/29: loss=-2440.22696398765, w0=-0.4588099096275718, w1=-0.4276798999885534\n",
      "Logistic regression iter. 4/29: loss=-1432.8781001915297, w0=-0.5359009750005781, w1=-0.4984816574925238\n",
      "Logistic regression iter. 5/29: loss=-857.5282540993115, w0=-0.6110355590573103, w1=-0.5676943588688736\n",
      "Logistic regression iter. 6/29: loss=-520.2840673800262, w0=-0.6850531071776198, w1=-0.6360232902305556\n",
      "Logistic regression iter. 7/29: loss=-319.17101287802006, w0=-0.7584176916463043, w1=-0.7038476096443487\n",
      "Logistic regression iter. 8/29: loss=-197.7580172554904, w0=-0.8313943977541998, w1=-0.7713785607826651\n",
      "Logistic regression iter. 9/29: loss=-123.78057319356542, w0=-0.9041379402041734, w1=-0.83873666264457\n",
      "Logistic regression iter. 10/29: loss=-78.38030435621607, w0=-0.9767400018446362, w1=-0.9059918471975783\n",
      "Logistic regression iter. 11/29: loss=-50.360171956081814, w0=-1.049255542428445, w1=-0.9731852176494259\n",
      "Logistic regression iter. 12/29: loss=-32.992983681935115, w0=-1.1217178134392827, w1=-1.0403411828932976\n",
      "Logistic regression iter. 13/29: loss=-22.199013494584502, w0=-1.1941470892947839, w1=-1.1074743635317108\n",
      "Logistic regression iter. 14/29: loss=-15.48481878586807, w0=-1.2665558158299788, w1=-1.1745935827337073\n",
      "Logistic regression iter. 15/29: loss=-11.316250528816834, w0=-1.338951679548405, w1=-1.2417042004520533\n",
      "Logistic regression iter. 16/29: loss=-8.743792655767988, w0=-1.411339453878315, w1=-1.3088094923282017\n",
      "Logistic regression iter. 17/29: loss=-7.1766215088051, w0=-1.4837221183366165, w1=-1.3759114712061835\n",
      "Logistic regression iter. 18/29: loss=-6.2452959872594995, w0=-1.556101541562806, w1=-1.4430113802285485\n",
      "Logistic regression iter. 19/29: loss=-5.717749099967966, w0=-1.6284789007373404, w1=-1.5101099907881534\n",
      "Logistic regression iter. 20/29: loss=-5.4475819412026345, w0=-1.7008549405735804, w1=-1.5772077836601508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 21/29: loss=-5.3420660513269995, w0=-1.7732301340785996, w1=-1.6443050597321214\n",
      "Logistic regression iter. 22/29: loss=-5.3422427668141665, w0=-1.8456047828255637, w1=-1.711402008046597\n",
      "Logistic regression iter. 23/29: loss=-5.4104884213646205, w0=-1.917979079783224, w1=-1.7784987478144034\n",
      "Logistic regression iter. 24/29: loss=-5.522710524173932, w0=-1.9903531488541413, w1=-1.8455953544734567\n",
      "Logistic regression iter. 25/29: loss=-5.663430011697354, w0=-2.0627270698577105, w1=-1.912691875920514\n",
      "Logistic regression iter. 26/29: loss=-5.822670084221536, w0=-2.1351008943776346, w1=-1.9797883426623488\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-2.207474655851755, w1=-2.0468847741883094\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-2.279848376018861, w1=-2.113981182985206\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-2.3522220690518, w1=-2.1810775770754662\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-14460.04270120874, w0=-0.20366599999999996, w1=-0.1956655680188542\n",
      "Logistic regression iter. 1/29: loss=-6753.329376487164, w0=-0.3119915395800564, w1=-0.29432610778206714\n",
      "Logistic regression iter. 2/29: loss=-3531.2816636113034, w0=-0.4047756808974149, w1=-0.3789418453738397\n",
      "Logistic regression iter. 3/29: loss=-1943.8486297982936, w0=-0.49085653377771915, w1=-0.45780065914074664\n",
      "Logistic regression iter. 4/29: loss=-1101.5153416481578, w0=-0.5735984532310725, w1=-0.5338975992751499\n",
      "Logistic regression iter. 5/29: loss=-636.1296147057136, w0=-0.6545643547001021, w1=-0.6085701515338935\n",
      "Logistic regression iter. 6/29: loss=-372.58782520270205, w0=-0.7345506719855734, w1=-0.6824778961956479\n",
      "Logistic regression iter. 7/29: loss=-220.86952845997163, w0=-0.8139842951038705, w1=-0.7559644335366754\n",
      "Logistic regression iter. 8/29: loss=-132.48779873251036, w0=-0.8931011521364149, w1=-0.8292148742295682\n",
      "Logistic regression iter. 9/29: loss=-80.54039295233959, w0=-0.9720343042855982, w1=-0.9023312115376769\n",
      "Logistic regression iter. 10/29: loss=-49.79785957464335, w0=-1.050859899944581, w1=-0.9753705638727882\n",
      "Logistic regression iter. 11/29: loss=-31.512409349561516, w0=-1.1296220123291623, w1=-1.0483653243071331\n",
      "Logistic regression iter. 12/29: loss=-20.60270173250581, w0=-1.2083463878231846, w1=-1.121334053941514\n",
      "Logistic regression iter. 13/29: loss=-14.090112690031418, w0=-1.2870481870024248, w1=-1.1942874820580933\n",
      "Logistic regression iter. 14/29: loss=-10.215033404487993, w0=-1.365736400282461, w1=-1.2672318587781113\n",
      "Logistic regression iter. 15/29: loss=-7.930886757380614, w0=-1.444416393283281, w1=-1.3401708502619187\n",
      "Logistic regression iter. 16/29: loss=-6.611367837220584, w0=-1.5230913871676544, w1=-1.4131066205715934\n",
      "Logistic regression iter. 17/29: loss=-5.879568173755629, w0=-1.6017633262972173, w1=-1.486040454531344\n",
      "Logistic regression iter. 18/29: loss=-5.507482956205243, w0=-1.6804333903437887, w1=-1.5589731190599885\n",
      "Logistic regression iter. 19/29: loss=-5.356511827307345, w0=-1.7591022984821587, w1=-1.6319050742327432\n",
      "Logistic regression iter. 20/29: loss=-5.3420151882618905, w0=-1.8377704911476422, w1=-1.7048365973459962\n",
      "Logistic regression iter. 21/29: loss=-5.412081359069285, w0=-1.9164382392360346, w1=-1.7777678562699848\n",
      "Logistic regression iter. 22/29: loss=-5.53474016857517, w0=-1.9951057100501404, w1=-1.8506989530536833\n",
      "Logistic regression iter. 23/29: loss=-5.6902251263801995, w0=-2.0737730073194034, w1=-1.9236299499774916\n",
      "Logistic regression iter. 24/29: loss=-5.866269085649602, w0=-2.1524401955978267, w1=-1.9965608851930938\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-2.2311073152032654, w1=-2.0694917821544125\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-2.3097743914031534, w1=-2.142422655328622\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-2.3884414400849927, w1=-2.2153535136683704\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-2.4671084712701368, w1=-2.2882843627308795\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-2.5457754912990764, w1=-2.3612152059759084\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-14460.04270120874, w0=-0.20366599999999996, w1=-0.1956655680188542\n",
      "Logistic regression iter. 1/29: loss=-6753.329376487164, w0=-0.3119915395800564, w1=-0.29432610778206714\n",
      "Logistic regression iter. 2/29: loss=-3531.2816636113034, w0=-0.4047756808974149, w1=-0.3789418453738397\n",
      "Logistic regression iter. 3/29: loss=-1943.8486297982936, w0=-0.49085653377771915, w1=-0.45780065914074664\n",
      "Logistic regression iter. 4/29: loss=-1101.5153416481578, w0=-0.5735984532310725, w1=-0.5338975992751499\n",
      "Logistic regression iter. 5/29: loss=-636.1296147057136, w0=-0.6545643547001021, w1=-0.6085701515338935\n",
      "Logistic regression iter. 6/29: loss=-372.58782520270205, w0=-0.7345506719855734, w1=-0.6824778961956479\n",
      "Logistic regression iter. 7/29: loss=-220.86952845997163, w0=-0.8139842951038705, w1=-0.7559644335366754\n",
      "Logistic regression iter. 8/29: loss=-132.48779873251036, w0=-0.8931011521364149, w1=-0.8292148742295682\n",
      "Logistic regression iter. 9/29: loss=-80.54039295233959, w0=-0.9720343042855982, w1=-0.9023312115376769\n",
      "Logistic regression iter. 10/29: loss=-49.79785957464335, w0=-1.050859899944581, w1=-0.9753705638727882\n",
      "Logistic regression iter. 11/29: loss=-31.512409349561516, w0=-1.1296220123291623, w1=-1.0483653243071331\n",
      "Logistic regression iter. 12/29: loss=-20.60270173250581, w0=-1.2083463878231846, w1=-1.121334053941514\n",
      "Logistic regression iter. 13/29: loss=-14.090112690031418, w0=-1.2870481870024248, w1=-1.1942874820580933\n",
      "Logistic regression iter. 14/29: loss=-10.215033404487993, w0=-1.365736400282461, w1=-1.2672318587781113\n",
      "Logistic regression iter. 15/29: loss=-7.930886757380614, w0=-1.444416393283281, w1=-1.3401708502619187\n",
      "Logistic regression iter. 16/29: loss=-6.611367837220584, w0=-1.5230913871676544, w1=-1.4131066205715934\n",
      "Logistic regression iter. 17/29: loss=-5.879568173755629, w0=-1.6017633262972173, w1=-1.486040454531344\n",
      "Logistic regression iter. 18/29: loss=-5.507482956205243, w0=-1.6804333903437887, w1=-1.5589731190599885\n",
      "Logistic regression iter. 19/29: loss=-5.356511827307345, w0=-1.7591022984821587, w1=-1.6319050742327432\n",
      "Logistic regression iter. 20/29: loss=-5.3420151882618905, w0=-1.8377704911476422, w1=-1.7048365973459962\n",
      "Logistic regression iter. 21/29: loss=-5.412081359069285, w0=-1.9164382392360346, w1=-1.7777678562699848\n",
      "Logistic regression iter. 22/29: loss=-5.53474016857517, w0=-1.9951057100501404, w1=-1.8506989530536833\n",
      "Logistic regression iter. 23/29: loss=-5.6902251263801995, w0=-2.0737730073194034, w1=-1.9236299499774916\n",
      "Logistic regression iter. 24/29: loss=-5.866269085649602, w0=-2.1524401955978267, w1=-1.9965608851930938\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-2.2311073152032654, w1=-2.0694917821544125\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-2.3097743914031534, w1=-2.142422655328622\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-2.3884414400849927, w1=-2.2153535136683704\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-2.4671084712701368, w1=-2.2882843627308795\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-2.5457754912990764, w1=-2.3612152059759084\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-14460.04270120874, w0=-0.20366599999999996, w1=-0.1956655680188542\n",
      "Logistic regression iter. 1/29: loss=-6753.329376487164, w0=-0.3119915395800564, w1=-0.29432610778206714\n",
      "Logistic regression iter. 2/29: loss=-3531.2816636113034, w0=-0.4047756808974149, w1=-0.3789418453738397\n",
      "Logistic regression iter. 3/29: loss=-1943.8486297982936, w0=-0.49085653377771915, w1=-0.45780065914074664\n",
      "Logistic regression iter. 4/29: loss=-1101.5153416481578, w0=-0.5735984532310725, w1=-0.5338975992751499\n",
      "Logistic regression iter. 5/29: loss=-636.1296147057136, w0=-0.6545643547001021, w1=-0.6085701515338935\n",
      "Logistic regression iter. 6/29: loss=-372.58782520270205, w0=-0.7345506719855734, w1=-0.6824778961956479\n",
      "Logistic regression iter. 7/29: loss=-220.86952845997163, w0=-0.8139842951038705, w1=-0.7559644335366754\n",
      "Logistic regression iter. 8/29: loss=-132.48779873251036, w0=-0.8931011521364149, w1=-0.8292148742295682\n",
      "Logistic regression iter. 9/29: loss=-80.54039295233959, w0=-0.9720343042855982, w1=-0.9023312115376769\n",
      "Logistic regression iter. 10/29: loss=-49.79785957464335, w0=-1.050859899944581, w1=-0.9753705638727882\n",
      "Logistic regression iter. 11/29: loss=-31.512409349561516, w0=-1.1296220123291623, w1=-1.0483653243071331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 12/29: loss=-20.60270173250581, w0=-1.2083463878231846, w1=-1.121334053941514\n",
      "Logistic regression iter. 13/29: loss=-14.090112690031418, w0=-1.2870481870024248, w1=-1.1942874820580933\n",
      "Logistic regression iter. 14/29: loss=-10.215033404487993, w0=-1.365736400282461, w1=-1.2672318587781113\n",
      "Logistic regression iter. 15/29: loss=-7.930886757380614, w0=-1.444416393283281, w1=-1.3401708502619187\n",
      "Logistic regression iter. 16/29: loss=-6.611367837220584, w0=-1.5230913871676544, w1=-1.4131066205715934\n",
      "Logistic regression iter. 17/29: loss=-5.879568173755629, w0=-1.6017633262972173, w1=-1.486040454531344\n",
      "Logistic regression iter. 18/29: loss=-5.507482956205243, w0=-1.6804333903437887, w1=-1.5589731190599885\n",
      "Logistic regression iter. 19/29: loss=-5.356511827307345, w0=-1.7591022984821587, w1=-1.6319050742327432\n",
      "Logistic regression iter. 20/29: loss=-5.3420151882618905, w0=-1.8377704911476422, w1=-1.7048365973459962\n",
      "Logistic regression iter. 21/29: loss=-5.412081359069285, w0=-1.9164382392360346, w1=-1.7777678562699848\n",
      "Logistic regression iter. 22/29: loss=-5.53474016857517, w0=-1.9951057100501404, w1=-1.8506989530536833\n",
      "Logistic regression iter. 23/29: loss=-5.6902251263801995, w0=-2.0737730073194034, w1=-1.9236299499774916\n",
      "Logistic regression iter. 24/29: loss=-5.866269085649602, w0=-2.1524401955978267, w1=-1.9965608851930938\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-2.2311073152032654, w1=-2.0694917821544125\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-2.3097743914031534, w1=-2.142422655328622\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-2.3884414400849927, w1=-2.2153535136683704\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-2.4671084712701368, w1=-2.2882843627308795\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-2.5457754912990764, w1=-2.3612152059759084\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-14460.04270120874, w0=-0.20366599999999996, w1=-0.1956655680188542\n",
      "Logistic regression iter. 1/29: loss=-6753.329376487164, w0=-0.3119915395800564, w1=-0.29432610778206714\n",
      "Logistic regression iter. 2/29: loss=-3531.2816636113034, w0=-0.4047756808974149, w1=-0.3789418453738397\n",
      "Logistic regression iter. 3/29: loss=-1943.8486297982936, w0=-0.49085653377771915, w1=-0.45780065914074664\n",
      "Logistic regression iter. 4/29: loss=-1101.5153416481578, w0=-0.5735984532310725, w1=-0.5338975992751499\n",
      "Logistic regression iter. 5/29: loss=-636.1296147057136, w0=-0.6545643547001021, w1=-0.6085701515338935\n",
      "Logistic regression iter. 6/29: loss=-372.58782520270205, w0=-0.7345506719855734, w1=-0.6824778961956479\n",
      "Logistic regression iter. 7/29: loss=-220.86952845997163, w0=-0.8139842951038705, w1=-0.7559644335366754\n",
      "Logistic regression iter. 8/29: loss=-132.48779873251036, w0=-0.8931011521364149, w1=-0.8292148742295682\n",
      "Logistic regression iter. 9/29: loss=-80.54039295233959, w0=-0.9720343042855982, w1=-0.9023312115376769\n",
      "Logistic regression iter. 10/29: loss=-49.79785957464335, w0=-1.050859899944581, w1=-0.9753705638727882\n",
      "Logistic regression iter. 11/29: loss=-31.512409349561516, w0=-1.1296220123291623, w1=-1.0483653243071331\n",
      "Logistic regression iter. 12/29: loss=-20.60270173250581, w0=-1.2083463878231846, w1=-1.121334053941514\n",
      "Logistic regression iter. 13/29: loss=-14.090112690031418, w0=-1.2870481870024248, w1=-1.1942874820580933\n",
      "Logistic regression iter. 14/29: loss=-10.215033404487993, w0=-1.365736400282461, w1=-1.2672318587781113\n",
      "Logistic regression iter. 15/29: loss=-7.930886757380614, w0=-1.444416393283281, w1=-1.3401708502619187\n",
      "Logistic regression iter. 16/29: loss=-6.611367837220584, w0=-1.5230913871676544, w1=-1.4131066205715934\n",
      "Logistic regression iter. 17/29: loss=-5.879568173755629, w0=-1.6017633262972173, w1=-1.486040454531344\n",
      "Logistic regression iter. 18/29: loss=-5.507482956205243, w0=-1.6804333903437887, w1=-1.5589731190599885\n",
      "Logistic regression iter. 19/29: loss=-5.356511827307345, w0=-1.7591022984821587, w1=-1.6319050742327432\n",
      "Logistic regression iter. 20/29: loss=-5.3420151882618905, w0=-1.8377704911476422, w1=-1.7048365973459962\n",
      "Logistic regression iter. 21/29: loss=-5.412081359069285, w0=-1.9164382392360346, w1=-1.7777678562699848\n",
      "Logistic regression iter. 22/29: loss=-5.53474016857517, w0=-1.9951057100501404, w1=-1.8506989530536833\n",
      "Logistic regression iter. 23/29: loss=-5.6902251263801995, w0=-2.0737730073194034, w1=-1.9236299499774916\n",
      "Logistic regression iter. 24/29: loss=-5.866269085649602, w0=-2.1524401955978267, w1=-1.9965608851930938\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-2.2311073152032654, w1=-2.0694917821544125\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-2.3097743914031534, w1=-2.142422655328622\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-2.3884414400849927, w1=-2.2153535136683704\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-2.4671084712701368, w1=-2.2882843627308795\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-2.5457754912990764, w1=-2.3612152059759084\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-12766.214674206638, w0=-0.21995927999999995, w1=-0.21131881346036255\n",
      "Logistic regression iter. 1/29: loss=-5774.357488281637, w0=-0.33326505075990454, w1=-0.3144641173871404\n",
      "Logistic regression iter. 2/29: loss=-2918.654562757246, w0=-0.43126260552463025, w1=-0.40392326386289423\n",
      "Logistic regression iter. 3/29: loss=-1551.3723475729346, w0=-0.522830978907419, w1=-0.4879211877346159\n",
      "Logistic regression iter. 4/29: loss=-848.4896166649548, w0=-0.6112941135208176, w1=-0.5693797677142638\n",
      "Logistic regression iter. 5/29: loss=-472.9905150307592, w0=-0.698159608974787, w1=-0.6495723306148136\n",
      "Logistic regression iter. 6/29: loss=-267.61671839768525, w0=-0.7841739985074485, w1=-0.7291087541927705\n",
      "Logistic regression iter. 7/29: loss=-153.49901528109947, w0=-0.8697249235825775, w1=-0.808296653350381\n",
      "Logistic regression iter. 8/29: loss=-89.35931449913632, w0=-0.9550195470864566, w1=-0.8872961890200936\n",
      "Logistic regression iter. 9/29: loss=-52.99996925605204, w0=-1.0401707402213933, w1=-0.9661925606500499\n",
      "Logistic regression iter. 10/29: loss=-32.25983968180621, w0=-1.1252408796740994, w1=-1.045031814837953\n",
      "Logistic regression iter. 11/29: loss=-20.384054030187432, w0=-1.210264827430446, w1=-1.1238391515055501\n",
      "Logistic regression iter. 12/29: loss=-13.579607633590944, w0=-1.2952622527267512, w1=-1.2026285065934386\n",
      "Logistic regression iter. 13/29: loss=-9.697237935068596, w0=-1.3802443450478679, w1=-1.281407656856029\n",
      "Logistic regression iter. 14/29: loss=-7.509459622209538, w0=-1.4652175172640753, w1=-1.3601809769671993\n",
      "Logistic regression iter. 15/29: loss=-6.3102818757680135, w0=-1.5501854696740922, w1=-1.4389509457319647\n",
      "Logistic regression iter. 16/29: loss=-5.691040714647917, w0=-1.6351503508090266, w1=-1.5177189770535904\n",
      "Logistic regression iter. 17/29: loss=-5.413775005135009, w0=-1.7201134154941664, w1=-1.5964858823691077\n",
      "Logistic regression iter. 18/29: loss=-5.339154578781616, w0=-1.8050754006227872, w1=-1.6752521300150955\n",
      "Logistic regression iter. 19/29: loss=-5.385174817044494, w0=-1.8900367411742416, w1=-1.7540179917387075\n",
      "Logistic regression iter. 20/29: loss=-5.503333664582887, w0=-1.974997695168981, w1=-1.8327836260041106\n",
      "Logistic regression iter. 21/29: loss=-5.664810675878867, w0=-2.059958416370254, w1=-1.9115491256497807\n",
      "Logistic regression iter. 22/29: loss=-5.852404514957652, w0=-2.144918996815815, w1=-1.9903145453069622\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-2.229879491828662, w1=-2.0690799172581538\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-2.3148399347968036, w1=-2.1478452606553007\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-2.399800345947921, w1=-2.226610586903359\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-2.4847607375819845, w1=-2.305375902818369\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-2.569721117204864, w1=-2.3841412124878345\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-2.654681489412542, w1=-2.462906518371027\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-2.739641857028477, w1=-2.5416718219522334\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-12766.214674206638, w0=-0.21995927999999995, w1=-0.21131881346036255\n",
      "Logistic regression iter. 1/29: loss=-5774.357488281637, w0=-0.33326505075990454, w1=-0.3144641173871404\n",
      "Logistic regression iter. 2/29: loss=-2918.654562757246, w0=-0.43126260552463025, w1=-0.40392326386289423\n",
      "Logistic regression iter. 3/29: loss=-1551.3723475729346, w0=-0.522830978907419, w1=-0.4879211877346159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 4/29: loss=-848.4896166649548, w0=-0.6112941135208176, w1=-0.5693797677142638\n",
      "Logistic regression iter. 5/29: loss=-472.9905150307592, w0=-0.698159608974787, w1=-0.6495723306148136\n",
      "Logistic regression iter. 6/29: loss=-267.61671839768525, w0=-0.7841739985074485, w1=-0.7291087541927705\n",
      "Logistic regression iter. 7/29: loss=-153.49901528109947, w0=-0.8697249235825775, w1=-0.808296653350381\n",
      "Logistic regression iter. 8/29: loss=-89.35931449913632, w0=-0.9550195470864566, w1=-0.8872961890200936\n",
      "Logistic regression iter. 9/29: loss=-52.99996925605204, w0=-1.0401707402213933, w1=-0.9661925606500499\n",
      "Logistic regression iter. 10/29: loss=-32.25983968180621, w0=-1.1252408796740994, w1=-1.045031814837953\n",
      "Logistic regression iter. 11/29: loss=-20.384054030187432, w0=-1.210264827430446, w1=-1.1238391515055501\n",
      "Logistic regression iter. 12/29: loss=-13.579607633590944, w0=-1.2952622527267512, w1=-1.2026285065934386\n",
      "Logistic regression iter. 13/29: loss=-9.697237935068596, w0=-1.3802443450478679, w1=-1.281407656856029\n",
      "Logistic regression iter. 14/29: loss=-7.509459622209538, w0=-1.4652175172640753, w1=-1.3601809769671993\n",
      "Logistic regression iter. 15/29: loss=-6.3102818757680135, w0=-1.5501854696740922, w1=-1.4389509457319647\n",
      "Logistic regression iter. 16/29: loss=-5.691040714647917, w0=-1.6351503508090266, w1=-1.5177189770535904\n",
      "Logistic regression iter. 17/29: loss=-5.413775005135009, w0=-1.7201134154941664, w1=-1.5964858823691077\n",
      "Logistic regression iter. 18/29: loss=-5.339154578781616, w0=-1.8050754006227872, w1=-1.6752521300150955\n",
      "Logistic regression iter. 19/29: loss=-5.385174817044494, w0=-1.8900367411742416, w1=-1.7540179917387075\n",
      "Logistic regression iter. 20/29: loss=-5.503333664582887, w0=-1.974997695168981, w1=-1.8327836260041106\n",
      "Logistic regression iter. 21/29: loss=-5.664810675878867, w0=-2.059958416370254, w1=-1.9115491256497807\n",
      "Logistic regression iter. 22/29: loss=-5.852404514957652, w0=-2.144918996815815, w1=-1.9903145453069622\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-2.229879491828662, w1=-2.0690799172581538\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-2.3148399347968036, w1=-2.1478452606553007\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-2.399800345947921, w1=-2.226610586903359\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-2.4847607375819845, w1=-2.305375902818369\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-2.569721117204864, w1=-2.3841412124878345\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-2.654681489412542, w1=-2.462906518371027\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-2.739641857028477, w1=-2.5416718219522334\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-12766.214674206638, w0=-0.21995927999999995, w1=-0.21131881346036255\n",
      "Logistic regression iter. 1/29: loss=-5774.357488281637, w0=-0.33326505075990454, w1=-0.3144641173871404\n",
      "Logistic regression iter. 2/29: loss=-2918.654562757246, w0=-0.43126260552463025, w1=-0.40392326386289423\n",
      "Logistic regression iter. 3/29: loss=-1551.3723475729346, w0=-0.522830978907419, w1=-0.4879211877346159\n",
      "Logistic regression iter. 4/29: loss=-848.4896166649548, w0=-0.6112941135208176, w1=-0.5693797677142638\n",
      "Logistic regression iter. 5/29: loss=-472.9905150307592, w0=-0.698159608974787, w1=-0.6495723306148136\n",
      "Logistic regression iter. 6/29: loss=-267.61671839768525, w0=-0.7841739985074485, w1=-0.7291087541927705\n",
      "Logistic regression iter. 7/29: loss=-153.49901528109947, w0=-0.8697249235825775, w1=-0.808296653350381\n",
      "Logistic regression iter. 8/29: loss=-89.35931449913632, w0=-0.9550195470864566, w1=-0.8872961890200936\n",
      "Logistic regression iter. 9/29: loss=-52.99996925605204, w0=-1.0401707402213933, w1=-0.9661925606500499\n",
      "Logistic regression iter. 10/29: loss=-32.25983968180621, w0=-1.1252408796740994, w1=-1.045031814837953\n",
      "Logistic regression iter. 11/29: loss=-20.384054030187432, w0=-1.210264827430446, w1=-1.1238391515055501\n",
      "Logistic regression iter. 12/29: loss=-13.579607633590944, w0=-1.2952622527267512, w1=-1.2026285065934386\n",
      "Logistic regression iter. 13/29: loss=-9.697237935068596, w0=-1.3802443450478679, w1=-1.281407656856029\n",
      "Logistic regression iter. 14/29: loss=-7.509459622209538, w0=-1.4652175172640753, w1=-1.3601809769671993\n",
      "Logistic regression iter. 15/29: loss=-6.3102818757680135, w0=-1.5501854696740922, w1=-1.4389509457319647\n",
      "Logistic regression iter. 16/29: loss=-5.691040714647917, w0=-1.6351503508090266, w1=-1.5177189770535904\n",
      "Logistic regression iter. 17/29: loss=-5.413775005135009, w0=-1.7201134154941664, w1=-1.5964858823691077\n",
      "Logistic regression iter. 18/29: loss=-5.339154578781616, w0=-1.8050754006227872, w1=-1.6752521300150955\n",
      "Logistic regression iter. 19/29: loss=-5.385174817044494, w0=-1.8900367411742416, w1=-1.7540179917387075\n",
      "Logistic regression iter. 20/29: loss=-5.503333664582887, w0=-1.974997695168981, w1=-1.8327836260041106\n",
      "Logistic regression iter. 21/29: loss=-5.664810675878867, w0=-2.059958416370254, w1=-1.9115491256497807\n",
      "Logistic regression iter. 22/29: loss=-5.852404514957652, w0=-2.144918996815815, w1=-1.9903145453069622\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-2.229879491828662, w1=-2.0690799172581538\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-2.3148399347968036, w1=-2.1478452606553007\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-2.399800345947921, w1=-2.226610586903359\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-2.4847607375819845, w1=-2.305375902818369\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-2.569721117204864, w1=-2.3841412124878345\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-2.654681489412542, w1=-2.462906518371027\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-2.739641857028477, w1=-2.5416718219522334\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-12766.214674206638, w0=-0.21995927999999995, w1=-0.21131881346036255\n",
      "Logistic regression iter. 1/29: loss=-5774.357488281637, w0=-0.33326505075990454, w1=-0.3144641173871404\n",
      "Logistic regression iter. 2/29: loss=-2918.654562757246, w0=-0.43126260552463025, w1=-0.40392326386289423\n",
      "Logistic regression iter. 3/29: loss=-1551.3723475729346, w0=-0.522830978907419, w1=-0.4879211877346159\n",
      "Logistic regression iter. 4/29: loss=-848.4896166649548, w0=-0.6112941135208176, w1=-0.5693797677142638\n",
      "Logistic regression iter. 5/29: loss=-472.9905150307592, w0=-0.698159608974787, w1=-0.6495723306148136\n",
      "Logistic regression iter. 6/29: loss=-267.61671839768525, w0=-0.7841739985074485, w1=-0.7291087541927705\n",
      "Logistic regression iter. 7/29: loss=-153.49901528109947, w0=-0.8697249235825775, w1=-0.808296653350381\n",
      "Logistic regression iter. 8/29: loss=-89.35931449913632, w0=-0.9550195470864566, w1=-0.8872961890200936\n",
      "Logistic regression iter. 9/29: loss=-52.99996925605204, w0=-1.0401707402213933, w1=-0.9661925606500499\n",
      "Logistic regression iter. 10/29: loss=-32.25983968180621, w0=-1.1252408796740994, w1=-1.045031814837953\n",
      "Logistic regression iter. 11/29: loss=-20.384054030187432, w0=-1.210264827430446, w1=-1.1238391515055501\n",
      "Logistic regression iter. 12/29: loss=-13.579607633590944, w0=-1.2952622527267512, w1=-1.2026285065934386\n",
      "Logistic regression iter. 13/29: loss=-9.697237935068596, w0=-1.3802443450478679, w1=-1.281407656856029\n",
      "Logistic regression iter. 14/29: loss=-7.509459622209538, w0=-1.4652175172640753, w1=-1.3601809769671993\n",
      "Logistic regression iter. 15/29: loss=-6.3102818757680135, w0=-1.5501854696740922, w1=-1.4389509457319647\n",
      "Logistic regression iter. 16/29: loss=-5.691040714647917, w0=-1.6351503508090266, w1=-1.5177189770535904\n",
      "Logistic regression iter. 17/29: loss=-5.413775005135009, w0=-1.7201134154941664, w1=-1.5964858823691077\n",
      "Logistic regression iter. 18/29: loss=-5.339154578781616, w0=-1.8050754006227872, w1=-1.6752521300150955\n",
      "Logistic regression iter. 19/29: loss=-5.385174817044494, w0=-1.8900367411742416, w1=-1.7540179917387075\n",
      "Logistic regression iter. 20/29: loss=-5.503333664582887, w0=-1.974997695168981, w1=-1.8327836260041106\n",
      "Logistic regression iter. 21/29: loss=-5.664810675878867, w0=-2.059958416370254, w1=-1.9115491256497807\n",
      "Logistic regression iter. 22/29: loss=-5.852404514957652, w0=-2.144918996815815, w1=-1.9903145453069622\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-2.229879491828662, w1=-2.0690799172581538\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-2.3148399347968036, w1=-2.1478452606553007\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-2.399800345947921, w1=-2.226610586903359\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-2.4847607375819845, w1=-2.305375902818369\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-2.569721117204864, w1=-2.3841412124878345\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-2.654681489412542, w1=-2.462906518371027\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-2.739641857028477, w1=-2.5416718219522334\n",
      "4\n",
      "4,5\n",
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-11278.810369566876, w0=-0.23625255999999997, w1=-0.2269720589018709\n",
      "Logistic regression iter. 1/29: loss=-4943.700916102144, w0=-0.3544518624730982, w1=-0.33455587877906223\n",
      "Logistic regression iter. 2/29: loss=-2415.718797728031, w0=-0.45768957269758576, w1=-0.4289003894341652\n",
      "Logistic regression iter. 3/29: loss=-1239.956498452501, w0=-0.5548069749770339, w1=-0.5181001932678994\n",
      "Logistic regression iter. 4/29: loss=-654.6547226601384, w0=-0.6490586710635345, w1=-0.6049819905592284\n",
      "Logistic regression iter. 5/29: loss=-352.4159971873329, w0=-0.7418846288747851, w1=-0.6907469040945006\n",
      "Logistic regression iter. 6/29: loss=-192.79416144517373, w0=-0.8339771107078972, w1=-0.7759531832401274\n",
      "Logistic regression iter. 7/29: loss=-107.19592648205393, w0=-0.9256841080878234, w1=-0.860873272063531\n",
      "Logistic regression iter. 8/29: loss=-60.784480793624915, w0=-1.0171853817417293, w1=-0.9456442031740153\n",
      "Logistic regression iter. 9/29: loss=-35.41869071125949, w0=-1.1085755348207558, w1=-1.0303363454199774\n",
      "Logistic regression iter. 10/29: loss=-21.484869558072248, w0=-1.1999050573276813, w1=-1.1149864033970898\n",
      "Logistic regression iter. 11/29: loss=-13.820267881968899, w0=-1.2912012043893308, w1=-1.1996137651374956\n",
      "Logistic regression iter. 12/29: loss=-9.622357043294912, w0=-1.382478832645698, w1=-1.2842287817052742\n",
      "Logistic regression iter. 13/29: loss=-7.35577971427349, w0=-1.4737461105523637, w1=-1.3688370313537412\n",
      "Logistic regression iter. 14/29: loss=-6.172539543450109, w0=-1.5650075645171222, w1=-1.4534415455123366\n",
      "Logistic regression iter. 15/29: loss=-5.60082653740094, w0=-1.6562657208534126, w1=-1.5380439841475657\n",
      "Logistic regression iter. 16/29: loss=-5.3763566690165465, w0=-1.7475219989987876, w1=-1.622645262597661\n",
      "Logistic regression iter. 17/29: loss=-5.350160267733288, w0=-1.838777201464615, w1=-1.7072458888643351\n",
      "Logistic regression iter. 18/29: loss=-5.437772686653953, w0=-1.930031784632117, w1=-1.7918461465781703\n",
      "Logistic regression iter. 19/29: loss=-5.591037514102013, w0=-2.021286009477894, w1=-1.876446194986929\n",
      "Logistic regression iter. 20/29: loss=-5.782354948513179, w0=-2.112540026020176, w1=-1.9610461239729533\n",
      "Logistic regression iter. 21/29: loss=-5.995827961741803, w0=-2.203793920923164, w1=-2.0456459845187647\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-2.2950477444887523, w1=-2.1302458056775286\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-2.3863015260449725, w1=-2.214845604078862\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-2.4775552827649303, w1=-2.2994453892812587\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-2.56880902474586, w1=-2.3840451668007434\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-2.6600627579480904, w1=-2.468644939832613\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-2.7513164859033266, w1=-2.5532447102345457\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-2.8425702107118695, w1=-2.637844479090309\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-2.9338239336271577, w1=-2.722444247034277\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-11278.810369566876, w0=-0.23625255999999997, w1=-0.2269720589018709\n",
      "Logistic regression iter. 1/29: loss=-4943.700916102144, w0=-0.3544518624730982, w1=-0.33455587877906223\n",
      "Logistic regression iter. 2/29: loss=-2415.718797728031, w0=-0.45768957269758576, w1=-0.4289003894341652\n",
      "Logistic regression iter. 3/29: loss=-1239.956498452501, w0=-0.5548069749770339, w1=-0.5181001932678994\n",
      "Logistic regression iter. 4/29: loss=-654.6547226601384, w0=-0.6490586710635345, w1=-0.6049819905592284\n",
      "Logistic regression iter. 5/29: loss=-352.4159971873329, w0=-0.7418846288747851, w1=-0.6907469040945006\n",
      "Logistic regression iter. 6/29: loss=-192.79416144517373, w0=-0.8339771107078972, w1=-0.7759531832401274\n",
      "Logistic regression iter. 7/29: loss=-107.19592648205393, w0=-0.9256841080878234, w1=-0.860873272063531\n",
      "Logistic regression iter. 8/29: loss=-60.784480793624915, w0=-1.0171853817417293, w1=-0.9456442031740153\n",
      "Logistic regression iter. 9/29: loss=-35.41869071125949, w0=-1.1085755348207558, w1=-1.0303363454199774\n",
      "Logistic regression iter. 10/29: loss=-21.484869558072248, w0=-1.1999050573276813, w1=-1.1149864033970898\n",
      "Logistic regression iter. 11/29: loss=-13.820267881968899, w0=-1.2912012043893308, w1=-1.1996137651374956\n",
      "Logistic regression iter. 12/29: loss=-9.622357043294912, w0=-1.382478832645698, w1=-1.2842287817052742\n",
      "Logistic regression iter. 13/29: loss=-7.35577971427349, w0=-1.4737461105523637, w1=-1.3688370313537412\n",
      "Logistic regression iter. 14/29: loss=-6.172539543450109, w0=-1.5650075645171222, w1=-1.4534415455123366\n",
      "Logistic regression iter. 15/29: loss=-5.60082653740094, w0=-1.6562657208534126, w1=-1.5380439841475657\n",
      "Logistic regression iter. 16/29: loss=-5.3763566690165465, w0=-1.7475219989987876, w1=-1.622645262597661\n",
      "Logistic regression iter. 17/29: loss=-5.350160267733288, w0=-1.838777201464615, w1=-1.7072458888643351\n",
      "Logistic regression iter. 18/29: loss=-5.437772686653953, w0=-1.930031784632117, w1=-1.7918461465781703\n",
      "Logistic regression iter. 19/29: loss=-5.591037514102013, w0=-2.021286009477894, w1=-1.876446194986929\n",
      "Logistic regression iter. 20/29: loss=-5.782354948513179, w0=-2.112540026020176, w1=-1.9610461239729533\n",
      "Logistic regression iter. 21/29: loss=-5.995827961741803, w0=-2.203793920923164, w1=-2.0456459845187647\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-2.2950477444887523, w1=-2.1302458056775286\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-2.3863015260449725, w1=-2.214845604078862\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-2.4775552827649303, w1=-2.2994453892812587\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-2.56880902474586, w1=-2.3840451668007434\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-2.6600627579480904, w1=-2.468644939832613\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-2.7513164859033266, w1=-2.5532447102345457\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-2.8425702107118695, w1=-2.637844479090309\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-2.9338239336271577, w1=-2.722444247034277\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-11278.810369566876, w0=-0.23625255999999997, w1=-0.2269720589018709\n",
      "Logistic regression iter. 1/29: loss=-4943.700916102144, w0=-0.3544518624730982, w1=-0.33455587877906223\n",
      "Logistic regression iter. 2/29: loss=-2415.718797728031, w0=-0.45768957269758576, w1=-0.4289003894341652\n",
      "Logistic regression iter. 3/29: loss=-1239.956498452501, w0=-0.5548069749770339, w1=-0.5181001932678994\n",
      "Logistic regression iter. 4/29: loss=-654.6547226601384, w0=-0.6490586710635345, w1=-0.6049819905592284\n",
      "Logistic regression iter. 5/29: loss=-352.4159971873329, w0=-0.7418846288747851, w1=-0.6907469040945006\n",
      "Logistic regression iter. 6/29: loss=-192.79416144517373, w0=-0.8339771107078972, w1=-0.7759531832401274\n",
      "Logistic regression iter. 7/29: loss=-107.19592648205393, w0=-0.9256841080878234, w1=-0.860873272063531\n",
      "Logistic regression iter. 8/29: loss=-60.784480793624915, w0=-1.0171853817417293, w1=-0.9456442031740153\n",
      "Logistic regression iter. 9/29: loss=-35.41869071125949, w0=-1.1085755348207558, w1=-1.0303363454199774\n",
      "Logistic regression iter. 10/29: loss=-21.484869558072248, w0=-1.1999050573276813, w1=-1.1149864033970898\n",
      "Logistic regression iter. 11/29: loss=-13.820267881968899, w0=-1.2912012043893308, w1=-1.1996137651374956\n",
      "Logistic regression iter. 12/29: loss=-9.622357043294912, w0=-1.382478832645698, w1=-1.2842287817052742\n",
      "Logistic regression iter. 13/29: loss=-7.35577971427349, w0=-1.4737461105523637, w1=-1.3688370313537412\n",
      "Logistic regression iter. 14/29: loss=-6.172539543450109, w0=-1.5650075645171222, w1=-1.4534415455123366\n",
      "Logistic regression iter. 15/29: loss=-5.60082653740094, w0=-1.6562657208534126, w1=-1.5380439841475657\n",
      "Logistic regression iter. 16/29: loss=-5.3763566690165465, w0=-1.7475219989987876, w1=-1.622645262597661\n",
      "Logistic regression iter. 17/29: loss=-5.350160267733288, w0=-1.838777201464615, w1=-1.7072458888643351\n",
      "Logistic regression iter. 18/29: loss=-5.437772686653953, w0=-1.930031784632117, w1=-1.7918461465781703\n",
      "Logistic regression iter. 19/29: loss=-5.591037514102013, w0=-2.021286009477894, w1=-1.876446194986929\n",
      "Logistic regression iter. 20/29: loss=-5.782354948513179, w0=-2.112540026020176, w1=-1.9610461239729533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 21/29: loss=-5.995827961741803, w0=-2.203793920923164, w1=-2.0456459845187647\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-2.2950477444887523, w1=-2.1302458056775286\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-2.3863015260449725, w1=-2.214845604078862\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-2.4775552827649303, w1=-2.2994453892812587\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-2.56880902474586, w1=-2.3840451668007434\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-2.6600627579480904, w1=-2.468644939832613\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-2.7513164859033266, w1=-2.5532447102345457\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-2.8425702107118695, w1=-2.637844479090309\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-2.9338239336271577, w1=-2.722444247034277\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-11278.810369566876, w0=-0.23625255999999997, w1=-0.2269720589018709\n",
      "Logistic regression iter. 1/29: loss=-4943.700916102144, w0=-0.3544518624730982, w1=-0.33455587877906223\n",
      "Logistic regression iter. 2/29: loss=-2415.718797728031, w0=-0.45768957269758576, w1=-0.4289003894341652\n",
      "Logistic regression iter. 3/29: loss=-1239.956498452501, w0=-0.5548069749770339, w1=-0.5181001932678994\n",
      "Logistic regression iter. 4/29: loss=-654.6547226601384, w0=-0.6490586710635345, w1=-0.6049819905592284\n",
      "Logistic regression iter. 5/29: loss=-352.4159971873329, w0=-0.7418846288747851, w1=-0.6907469040945006\n",
      "Logistic regression iter. 6/29: loss=-192.79416144517373, w0=-0.8339771107078972, w1=-0.7759531832401274\n",
      "Logistic regression iter. 7/29: loss=-107.19592648205393, w0=-0.9256841080878234, w1=-0.860873272063531\n",
      "Logistic regression iter. 8/29: loss=-60.784480793624915, w0=-1.0171853817417293, w1=-0.9456442031740153\n",
      "Logistic regression iter. 9/29: loss=-35.41869071125949, w0=-1.1085755348207558, w1=-1.0303363454199774\n",
      "Logistic regression iter. 10/29: loss=-21.484869558072248, w0=-1.1999050573276813, w1=-1.1149864033970898\n",
      "Logistic regression iter. 11/29: loss=-13.820267881968899, w0=-1.2912012043893308, w1=-1.1996137651374956\n",
      "Logistic regression iter. 12/29: loss=-9.622357043294912, w0=-1.382478832645698, w1=-1.2842287817052742\n",
      "Logistic regression iter. 13/29: loss=-7.35577971427349, w0=-1.4737461105523637, w1=-1.3688370313537412\n",
      "Logistic regression iter. 14/29: loss=-6.172539543450109, w0=-1.5650075645171222, w1=-1.4534415455123366\n",
      "Logistic regression iter. 15/29: loss=-5.60082653740094, w0=-1.6562657208534126, w1=-1.5380439841475657\n",
      "Logistic regression iter. 16/29: loss=-5.3763566690165465, w0=-1.7475219989987876, w1=-1.622645262597661\n",
      "Logistic regression iter. 17/29: loss=-5.350160267733288, w0=-1.838777201464615, w1=-1.7072458888643351\n",
      "Logistic regression iter. 18/29: loss=-5.437772686653953, w0=-1.930031784632117, w1=-1.7918461465781703\n",
      "Logistic regression iter. 19/29: loss=-5.591037514102013, w0=-2.021286009477894, w1=-1.876446194986929\n",
      "Logistic regression iter. 20/29: loss=-5.782354948513179, w0=-2.112540026020176, w1=-1.9610461239729533\n",
      "Logistic regression iter. 21/29: loss=-5.995827961741803, w0=-2.203793920923164, w1=-2.0456459845187647\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-2.2950477444887523, w1=-2.1302458056775286\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-2.3863015260449725, w1=-2.214845604078862\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-2.4775552827649303, w1=-2.2994453892812587\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-2.56880902474586, w1=-2.3840451668007434\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-2.6600627579480904, w1=-2.468644939832613\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-2.7513164859033266, w1=-2.5532447102345457\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-2.8425702107118695, w1=-2.637844479090309\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-2.9338239336271577, w1=-2.722444247034277\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-9971.747075838624, w0=-0.2525458399999999, w1=-0.2426253043433792\n",
      "Logistic regression iter. 1/29: loss=-4237.020453717056, w0=-0.3755915758790641, w1=-0.3546351127013803\n",
      "Logistic regression iter. 2/29: loss=-2001.6613729004764, w0=-0.48410971597495456, w1=-0.453915820546443\n",
      "Logistic regression iter. 3/29: loss=-992.2077551126999, w0=-0.5868395186858311, w1=-0.5483793589009966\n",
      "Logistic regression iter. 4/29: loss=-505.80609079189753, w0=-0.6869426769170119, w1=-0.6407403781498429\n",
      "Logistic regression iter. 5/29: loss=-263.09846306058415, w0=-0.7857824861352561, w1=-0.7321225882030317\n",
      "Logistic regression iter. 6/29: loss=-139.3441034424781, w0=-0.8839947211020508, w1=-0.823032371735417\n",
      "Logistic regression iter. 7/29: loss=-75.30514823147699, w0=-0.9818886107623442, w1=-0.9137087165691956\n",
      "Logistic regression iter. 8/29: loss=-41.817535056684115, w0=-1.0796185164442802, w1=-1.0042677154280044\n",
      "Logistic regression iter. 9/29: loss=-24.183569721312082, w0=-1.1772629040031601, w1=-1.094766915415968\n",
      "Logistic regression iter. 10/29: loss=-14.871113988897385, w0=-1.2748622226203652, w1=-1.1852352900740448\n",
      "Logistic regression iter. 11/29: loss=-9.969500630969748, w0=-1.3724375676507579, w1=-1.2756876146480403\n",
      "Logistic regression iter. 12/29: loss=-7.426195893493868, w0=-1.4700000526202115, w1=-1.3661315071121602\n",
      "Logistic regression iter. 13/29: loss=-6.153721651768049, w0=-1.5675555855900396, w1=-1.456570933547409\n",
      "Logistic regression iter. 14/29: loss=-5.571028732176317, w0=-1.665107333354861, w1=-1.5470079769111682\n",
      "Logistic regression iter. 15/29: loss=-5.365258790879517, w0=-1.7626570063229974, w1=-1.6374437399093191\n",
      "Logistic regression iter. 16/29: loss=-5.366754382447835, w0=-1.8602055348610624, w1=-1.727878810600516\n",
      "Logistic regression iter. 17/29: loss=-5.482900603923894, w0=-1.957753428399972, w1=-1.8183135047221985\n",
      "Logistic regression iter. 18/29: loss=-5.66282350459802, w0=-2.05530096762581, w1=-1.9087479928711315\n",
      "Logistic regression iter. 19/29: loss=-5.878412884215882, w0=-2.152848308102303, w1=-1.9991823677676601\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-2.250395536527923, w1=-2.0896166800851796\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-2.3479427014770957, w1=-2.1800509576620954\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-2.4454898303016273, w1=-2.2704852158672653\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-2.5430369384770444, w1=-2.3609194632249735\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-2.640584034799515, w1=-2.4513537044840943\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-2.738131124290673, w1=-2.5417879423013523\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-2.8356782098293856, w1=-2.6322221781689947\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-2.9332252930727587, w1=-2.7226564129284077\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-3.0307723749783455, w1=-2.8130906470557377\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-3.1283194561015337, w1=-2.903524880821386\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-9971.747075838624, w0=-0.2525458399999999, w1=-0.2426253043433792\n",
      "Logistic regression iter. 1/29: loss=-4237.020453717056, w0=-0.3755915758790641, w1=-0.3546351127013803\n",
      "Logistic regression iter. 2/29: loss=-2001.6613729004764, w0=-0.48410971597495456, w1=-0.453915820546443\n",
      "Logistic regression iter. 3/29: loss=-992.2077551126999, w0=-0.5868395186858311, w1=-0.5483793589009966\n",
      "Logistic regression iter. 4/29: loss=-505.80609079189753, w0=-0.6869426769170119, w1=-0.6407403781498429\n",
      "Logistic regression iter. 5/29: loss=-263.09846306058415, w0=-0.7857824861352561, w1=-0.7321225882030317\n",
      "Logistic regression iter. 6/29: loss=-139.3441034424781, w0=-0.8839947211020508, w1=-0.823032371735417\n",
      "Logistic regression iter. 7/29: loss=-75.30514823147699, w0=-0.9818886107623442, w1=-0.9137087165691956\n",
      "Logistic regression iter. 8/29: loss=-41.817535056684115, w0=-1.0796185164442802, w1=-1.0042677154280044\n",
      "Logistic regression iter. 9/29: loss=-24.183569721312082, w0=-1.1772629040031601, w1=-1.094766915415968\n",
      "Logistic regression iter. 10/29: loss=-14.871113988897385, w0=-1.2748622226203652, w1=-1.1852352900740448\n",
      "Logistic regression iter. 11/29: loss=-9.969500630969748, w0=-1.3724375676507579, w1=-1.2756876146480403\n",
      "Logistic regression iter. 12/29: loss=-7.426195893493868, w0=-1.4700000526202115, w1=-1.3661315071121602\n",
      "Logistic regression iter. 13/29: loss=-6.153721651768049, w0=-1.5675555855900396, w1=-1.456570933547409\n",
      "Logistic regression iter. 14/29: loss=-5.571028732176317, w0=-1.665107333354861, w1=-1.5470079769111682\n",
      "Logistic regression iter. 15/29: loss=-5.365258790879517, w0=-1.7626570063229974, w1=-1.6374437399093191\n",
      "Logistic regression iter. 16/29: loss=-5.366754382447835, w0=-1.8602055348610624, w1=-1.727878810600516\n",
      "Logistic regression iter. 17/29: loss=-5.482900603923894, w0=-1.957753428399972, w1=-1.8183135047221985\n",
      "Logistic regression iter. 18/29: loss=-5.66282350459802, w0=-2.05530096762581, w1=-1.9087479928711315\n",
      "Logistic regression iter. 19/29: loss=-5.878412884215882, w0=-2.152848308102303, w1=-1.9991823677676601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 20/29: loss=nan, w0=-2.250395536527923, w1=-2.0896166800851796\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-2.3479427014770957, w1=-2.1800509576620954\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-2.4454898303016273, w1=-2.2704852158672653\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-2.5430369384770444, w1=-2.3609194632249735\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-2.640584034799515, w1=-2.4513537044840943\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-2.738131124290673, w1=-2.5417879423013523\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-2.8356782098293856, w1=-2.6322221781689947\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-2.9332252930727587, w1=-2.7226564129284077\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-3.0307723749783455, w1=-2.8130906470557377\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-3.1283194561015337, w1=-2.903524880821386\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-9971.747075838624, w0=-0.2525458399999999, w1=-0.2426253043433792\n",
      "Logistic regression iter. 1/29: loss=-4237.020453717056, w0=-0.3755915758790641, w1=-0.3546351127013803\n",
      "Logistic regression iter. 2/29: loss=-2001.6613729004764, w0=-0.48410971597495456, w1=-0.453915820546443\n",
      "Logistic regression iter. 3/29: loss=-992.2077551126999, w0=-0.5868395186858311, w1=-0.5483793589009966\n",
      "Logistic regression iter. 4/29: loss=-505.80609079189753, w0=-0.6869426769170119, w1=-0.6407403781498429\n",
      "Logistic regression iter. 5/29: loss=-263.09846306058415, w0=-0.7857824861352561, w1=-0.7321225882030317\n",
      "Logistic regression iter. 6/29: loss=-139.3441034424781, w0=-0.8839947211020508, w1=-0.823032371735417\n",
      "Logistic regression iter. 7/29: loss=-75.30514823147699, w0=-0.9818886107623442, w1=-0.9137087165691956\n",
      "Logistic regression iter. 8/29: loss=-41.817535056684115, w0=-1.0796185164442802, w1=-1.0042677154280044\n",
      "Logistic regression iter. 9/29: loss=-24.183569721312082, w0=-1.1772629040031601, w1=-1.094766915415968\n",
      "Logistic regression iter. 10/29: loss=-14.871113988897385, w0=-1.2748622226203652, w1=-1.1852352900740448\n",
      "Logistic regression iter. 11/29: loss=-9.969500630969748, w0=-1.3724375676507579, w1=-1.2756876146480403\n",
      "Logistic regression iter. 12/29: loss=-7.426195893493868, w0=-1.4700000526202115, w1=-1.3661315071121602\n",
      "Logistic regression iter. 13/29: loss=-6.153721651768049, w0=-1.5675555855900396, w1=-1.456570933547409\n",
      "Logistic regression iter. 14/29: loss=-5.571028732176317, w0=-1.665107333354861, w1=-1.5470079769111682\n",
      "Logistic regression iter. 15/29: loss=-5.365258790879517, w0=-1.7626570063229974, w1=-1.6374437399093191\n",
      "Logistic regression iter. 16/29: loss=-5.366754382447835, w0=-1.8602055348610624, w1=-1.727878810600516\n",
      "Logistic regression iter. 17/29: loss=-5.482900603923894, w0=-1.957753428399972, w1=-1.8183135047221985\n",
      "Logistic regression iter. 18/29: loss=-5.66282350459802, w0=-2.05530096762581, w1=-1.9087479928711315\n",
      "Logistic regression iter. 19/29: loss=-5.878412884215882, w0=-2.152848308102303, w1=-1.9991823677676601\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-2.250395536527923, w1=-2.0896166800851796\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-2.3479427014770957, w1=-2.1800509576620954\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-2.4454898303016273, w1=-2.2704852158672653\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-2.5430369384770444, w1=-2.3609194632249735\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-2.640584034799515, w1=-2.4513537044840943\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-2.738131124290673, w1=-2.5417879423013523\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-2.8356782098293856, w1=-2.6322221781689947\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-2.9332252930727587, w1=-2.7226564129284077\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-3.0307723749783455, w1=-2.8130906470557377\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-3.1283194561015337, w1=-2.903524880821386\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-9971.747075838624, w0=-0.2525458399999999, w1=-0.2426253043433792\n",
      "Logistic regression iter. 1/29: loss=-4237.020453717056, w0=-0.3755915758790641, w1=-0.3546351127013803\n",
      "Logistic regression iter. 2/29: loss=-2001.6613729004764, w0=-0.48410971597495456, w1=-0.453915820546443\n",
      "Logistic regression iter. 3/29: loss=-992.2077551126999, w0=-0.5868395186858311, w1=-0.5483793589009966\n",
      "Logistic regression iter. 4/29: loss=-505.80609079189753, w0=-0.6869426769170119, w1=-0.6407403781498429\n",
      "Logistic regression iter. 5/29: loss=-263.09846306058415, w0=-0.7857824861352561, w1=-0.7321225882030317\n",
      "Logistic regression iter. 6/29: loss=-139.3441034424781, w0=-0.8839947211020508, w1=-0.823032371735417\n",
      "Logistic regression iter. 7/29: loss=-75.30514823147699, w0=-0.9818886107623442, w1=-0.9137087165691956\n",
      "Logistic regression iter. 8/29: loss=-41.817535056684115, w0=-1.0796185164442802, w1=-1.0042677154280044\n",
      "Logistic regression iter. 9/29: loss=-24.183569721312082, w0=-1.1772629040031601, w1=-1.094766915415968\n",
      "Logistic regression iter. 10/29: loss=-14.871113988897385, w0=-1.2748622226203652, w1=-1.1852352900740448\n",
      "Logistic regression iter. 11/29: loss=-9.969500630969748, w0=-1.3724375676507579, w1=-1.2756876146480403\n",
      "Logistic regression iter. 12/29: loss=-7.426195893493868, w0=-1.4700000526202115, w1=-1.3661315071121602\n",
      "Logistic regression iter. 13/29: loss=-6.153721651768049, w0=-1.5675555855900396, w1=-1.456570933547409\n",
      "Logistic regression iter. 14/29: loss=-5.571028732176317, w0=-1.665107333354861, w1=-1.5470079769111682\n",
      "Logistic regression iter. 15/29: loss=-5.365258790879517, w0=-1.7626570063229974, w1=-1.6374437399093191\n",
      "Logistic regression iter. 16/29: loss=-5.366754382447835, w0=-1.8602055348610624, w1=-1.727878810600516\n",
      "Logistic regression iter. 17/29: loss=-5.482900603923894, w0=-1.957753428399972, w1=-1.8183135047221985\n",
      "Logistic regression iter. 18/29: loss=-5.66282350459802, w0=-2.05530096762581, w1=-1.9087479928711315\n",
      "Logistic regression iter. 19/29: loss=-5.878412884215882, w0=-2.152848308102303, w1=-1.9991823677676601\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-2.250395536527923, w1=-2.0896166800851796\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-2.3479427014770957, w1=-2.1800509576620954\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-2.4454898303016273, w1=-2.2704852158672653\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-2.5430369384770444, w1=-2.3609194632249735\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-2.640584034799515, w1=-2.4513537044840943\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-2.738131124290673, w1=-2.5417879423013523\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-2.8356782098293856, w1=-2.6322221781689947\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-2.9332252930727587, w1=-2.7226564129284077\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-3.0307723749783455, w1=-2.8130906470557377\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-3.1283194561015337, w1=-2.903524880821386\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-8822.33083758387, w0=-0.26883911999999993, w1=-0.25827854978488757\n",
      "Logistic regression iter. 1/29: loss=-3634.547355893433, w0=-0.396715345127002, w1=-0.3747273342917142\n",
      "Logistic regression iter. 2/29: loss=-1660.0380858212866, w0=-0.5105634724380481, w1=-0.4790004485842057\n",
      "Logistic regression iter. 3/29: loss=-794.7226068663599, w0=-0.6189686699532209, w1=-0.5787871485116947\n",
      "Logistic regression iter. 4/29: loss=-391.2983728874542, w0=-0.7249809644465735, w1=-0.6766775930372051\n",
      "Logistic regression iter. 5/29: loss=-196.8224469517355, w0=-0.8298808494672621, w1=-0.7737152864971111\n",
      "Logistic regression iter. 6/29: loss=-101.09876266272693, w0=-0.9342471792326441, w1=-0.8703558340988387\n",
      "Logistic regression iter. 7/29: loss=-53.308057565260775, w0=-1.0383522504153633, w1=-0.9668070858507997\n",
      "Logistic regression iter. 8/29: loss=-29.21609797103405, w0=-1.1423273952996558, w1=-1.063166539818201\n",
      "Logistic regression iter. 9/29: loss=-17.007774400253147, w0=-1.2462371014167963, w1=-1.1594808510837171\n",
      "Logistic regression iter. 10/29: loss=-10.82834356202718, w0=-1.3501134852594106, w1=-1.2557726972829593\n",
      "Logistic regression iter. 11/29: loss=-7.738632643298827, w0=-1.4539727341337787, w1=-1.352053246305924\n",
      "Logistic regression iter. 12/29: loss=-6.246759829029776, w0=-1.5578230930743229, w1=-1.4483280606761693\n",
      "Logistic regression iter. 13/29: loss=-5.588060787851263, w0=-1.6616688017731343, w1=-1.5445999390982859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 14/29: loss=-5.36714244877936, w0=-1.7655120593735958, w1=-1.6408703026087401\n",
      "Logistic regression iter. 15/29: loss=-5.377895407761565, w0=-1.8693540157947126, w1=-1.7371398787755488\n",
      "Logistic regression iter. 16/29: loss=-5.5120702684400165, w0=-1.9731952768463477, w1=-1.8334090429817875\n",
      "Logistic regression iter. 17/29: loss=-5.712413642129917, w0=-2.077036163934603, w1=-1.9296779902847956\n",
      "Logistic regression iter. 18/29: loss=-5.948443512672497, w0=-2.180876848707641, w1=-2.0259468227133324\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-2.2847174234078373, w1=-2.1222155939666214\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-2.3885579378988324, w1=-2.218484332471768\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-2.4923984192869026, w1=-2.314753053359815\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-2.5962388823859803, w1=-2.4110217647262804\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-2.7000793353333234, w1=-2.507290470923689\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-2.803919782620465, w1=-2.6035591743030353\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-2.9077602267381497, w1=-2.6998278761398113\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-3.011600669073765, w1=-2.7960965771289343\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-3.115441110403419, w1=-2.8923652776505357\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-3.219281551163054, w1=-2.9886339779133575\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-3.3231219915985095, w1=-3.0849026780324498\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-8822.33083758387, w0=-0.26883911999999993, w1=-0.25827854978488757\n",
      "Logistic regression iter. 1/29: loss=-3634.547355893433, w0=-0.396715345127002, w1=-0.3747273342917142\n",
      "Logistic regression iter. 2/29: loss=-1660.0380858212866, w0=-0.5105634724380481, w1=-0.4790004485842057\n",
      "Logistic regression iter. 3/29: loss=-794.7226068663599, w0=-0.6189686699532209, w1=-0.5787871485116947\n",
      "Logistic regression iter. 4/29: loss=-391.2983728874542, w0=-0.7249809644465735, w1=-0.6766775930372051\n",
      "Logistic regression iter. 5/29: loss=-196.8224469517355, w0=-0.8298808494672621, w1=-0.7737152864971111\n",
      "Logistic regression iter. 6/29: loss=-101.09876266272693, w0=-0.9342471792326441, w1=-0.8703558340988387\n",
      "Logistic regression iter. 7/29: loss=-53.308057565260775, w0=-1.0383522504153633, w1=-0.9668070858507997\n",
      "Logistic regression iter. 8/29: loss=-29.21609797103405, w0=-1.1423273952996558, w1=-1.063166539818201\n",
      "Logistic regression iter. 9/29: loss=-17.007774400253147, w0=-1.2462371014167963, w1=-1.1594808510837171\n",
      "Logistic regression iter. 10/29: loss=-10.82834356202718, w0=-1.3501134852594106, w1=-1.2557726972829593\n",
      "Logistic regression iter. 11/29: loss=-7.738632643298827, w0=-1.4539727341337787, w1=-1.352053246305924\n",
      "Logistic regression iter. 12/29: loss=-6.246759829029776, w0=-1.5578230930743229, w1=-1.4483280606761693\n",
      "Logistic regression iter. 13/29: loss=-5.588060787851263, w0=-1.6616688017731343, w1=-1.5445999390982859\n",
      "Logistic regression iter. 14/29: loss=-5.36714244877936, w0=-1.7655120593735958, w1=-1.6408703026087401\n",
      "Logistic regression iter. 15/29: loss=-5.377895407761565, w0=-1.8693540157947126, w1=-1.7371398787755488\n",
      "Logistic regression iter. 16/29: loss=-5.5120702684400165, w0=-1.9731952768463477, w1=-1.8334090429817875\n",
      "Logistic regression iter. 17/29: loss=-5.712413642129917, w0=-2.077036163934603, w1=-1.9296779902847956\n",
      "Logistic regression iter. 18/29: loss=-5.948443512672497, w0=-2.180876848707641, w1=-2.0259468227133324\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-2.2847174234078373, w1=-2.1222155939666214\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-2.3885579378988324, w1=-2.218484332471768\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-2.4923984192869026, w1=-2.314753053359815\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-2.5962388823859803, w1=-2.4110217647262804\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-2.7000793353333234, w1=-2.507290470923689\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-2.803919782620465, w1=-2.6035591743030353\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-2.9077602267381497, w1=-2.6998278761398113\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-3.011600669073765, w1=-2.7960965771289343\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-3.115441110403419, w1=-2.8923652776505357\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-3.219281551163054, w1=-2.9886339779133575\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-3.3231219915985095, w1=-3.0849026780324498\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-8822.33083758387, w0=-0.26883911999999993, w1=-0.25827854978488757\n",
      "Logistic regression iter. 1/29: loss=-3634.547355893433, w0=-0.396715345127002, w1=-0.3747273342917142\n",
      "Logistic regression iter. 2/29: loss=-1660.0380858212866, w0=-0.5105634724380481, w1=-0.4790004485842057\n",
      "Logistic regression iter. 3/29: loss=-794.7226068663599, w0=-0.6189686699532209, w1=-0.5787871485116947\n",
      "Logistic regression iter. 4/29: loss=-391.2983728874542, w0=-0.7249809644465735, w1=-0.6766775930372051\n",
      "Logistic regression iter. 5/29: loss=-196.8224469517355, w0=-0.8298808494672621, w1=-0.7737152864971111\n",
      "Logistic regression iter. 6/29: loss=-101.09876266272693, w0=-0.9342471792326441, w1=-0.8703558340988387\n",
      "Logistic regression iter. 7/29: loss=-53.308057565260775, w0=-1.0383522504153633, w1=-0.9668070858507997\n",
      "Logistic regression iter. 8/29: loss=-29.21609797103405, w0=-1.1423273952996558, w1=-1.063166539818201\n",
      "Logistic regression iter. 9/29: loss=-17.007774400253147, w0=-1.2462371014167963, w1=-1.1594808510837171\n",
      "Logistic regression iter. 10/29: loss=-10.82834356202718, w0=-1.3501134852594106, w1=-1.2557726972829593\n",
      "Logistic regression iter. 11/29: loss=-7.738632643298827, w0=-1.4539727341337787, w1=-1.352053246305924\n",
      "Logistic regression iter. 12/29: loss=-6.246759829029776, w0=-1.5578230930743229, w1=-1.4483280606761693\n",
      "Logistic regression iter. 13/29: loss=-5.588060787851263, w0=-1.6616688017731343, w1=-1.5445999390982859\n",
      "Logistic regression iter. 14/29: loss=-5.36714244877936, w0=-1.7655120593735958, w1=-1.6408703026087401\n",
      "Logistic regression iter. 15/29: loss=-5.377895407761565, w0=-1.8693540157947126, w1=-1.7371398787755488\n",
      "Logistic regression iter. 16/29: loss=-5.5120702684400165, w0=-1.9731952768463477, w1=-1.8334090429817875\n",
      "Logistic regression iter. 17/29: loss=-5.712413642129917, w0=-2.077036163934603, w1=-1.9296779902847956\n",
      "Logistic regression iter. 18/29: loss=-5.948443512672497, w0=-2.180876848707641, w1=-2.0259468227133324\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-2.2847174234078373, w1=-2.1222155939666214\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-2.3885579378988324, w1=-2.218484332471768\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-2.4923984192869026, w1=-2.314753053359815\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-2.5962388823859803, w1=-2.4110217647262804\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-2.7000793353333234, w1=-2.507290470923689\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-2.803919782620465, w1=-2.6035591743030353\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-2.9077602267381497, w1=-2.6998278761398113\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-3.011600669073765, w1=-2.7960965771289343\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-3.115441110403419, w1=-2.8923652776505357\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-3.219281551163054, w1=-2.9886339779133575\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-3.3231219915985095, w1=-3.0849026780324498\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-8822.33083758387, w0=-0.26883911999999993, w1=-0.25827854978488757\n",
      "Logistic regression iter. 1/29: loss=-3634.547355893433, w0=-0.396715345127002, w1=-0.3747273342917142\n",
      "Logistic regression iter. 2/29: loss=-1660.0380858212866, w0=-0.5105634724380481, w1=-0.4790004485842057\n",
      "Logistic regression iter. 3/29: loss=-794.7226068663599, w0=-0.6189686699532209, w1=-0.5787871485116947\n",
      "Logistic regression iter. 4/29: loss=-391.2983728874542, w0=-0.7249809644465735, w1=-0.6766775930372051\n",
      "Logistic regression iter. 5/29: loss=-196.8224469517355, w0=-0.8298808494672621, w1=-0.7737152864971111\n",
      "Logistic regression iter. 6/29: loss=-101.09876266272693, w0=-0.9342471792326441, w1=-0.8703558340988387\n",
      "Logistic regression iter. 7/29: loss=-53.308057565260775, w0=-1.0383522504153633, w1=-0.9668070858507997\n",
      "Logistic regression iter. 8/29: loss=-29.21609797103405, w0=-1.1423273952996558, w1=-1.063166539818201\n",
      "Logistic regression iter. 9/29: loss=-17.007774400253147, w0=-1.2462371014167963, w1=-1.1594808510837171\n",
      "Logistic regression iter. 10/29: loss=-10.82834356202718, w0=-1.3501134852594106, w1=-1.2557726972829593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 11/29: loss=-7.738632643298827, w0=-1.4539727341337787, w1=-1.352053246305924\n",
      "Logistic regression iter. 12/29: loss=-6.246759829029776, w0=-1.5578230930743229, w1=-1.4483280606761693\n",
      "Logistic regression iter. 13/29: loss=-5.588060787851263, w0=-1.6616688017731343, w1=-1.5445999390982859\n",
      "Logistic regression iter. 14/29: loss=-5.36714244877936, w0=-1.7655120593735958, w1=-1.6408703026087401\n",
      "Logistic regression iter. 15/29: loss=-5.377895407761565, w0=-1.8693540157947126, w1=-1.7371398787755488\n",
      "Logistic regression iter. 16/29: loss=-5.5120702684400165, w0=-1.9731952768463477, w1=-1.8334090429817875\n",
      "Logistic regression iter. 17/29: loss=-5.712413642129917, w0=-2.077036163934603, w1=-1.9296779902847956\n",
      "Logistic regression iter. 18/29: loss=-5.948443512672497, w0=-2.180876848707641, w1=-2.0259468227133324\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-2.2847174234078373, w1=-2.1222155939666214\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-2.3885579378988324, w1=-2.218484332471768\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-2.4923984192869026, w1=-2.314753053359815\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-2.5962388823859803, w1=-2.4110217647262804\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-2.7000793353333234, w1=-2.507290470923689\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-2.803919782620465, w1=-2.6035591743030353\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-2.9077602267381497, w1=-2.6998278761398113\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-3.011600669073765, w1=-2.7960965771289343\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-3.115441110403419, w1=-2.8923652776505357\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-3.219281551163054, w1=-2.9886339779133575\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-3.3231219915985095, w1=-3.0849026780324498\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-7810.8129017606625, w0=-0.28513239999999995, w1=-0.27393179522639594\n",
      "Logistic regression iter. 1/29: loss=-3120.055870918313, w0=-0.41784734324622186, w1=-0.3948514664975455\n",
      "Logistic regression iter. 2/29: loss=-1377.7145047584238, w0=-0.5370809761535066, w1=-0.5041759260265046\n",
      "Logistic regression iter. 3/29: loss=-637.0738816622791, w0=-0.6512226150065049, w1=-0.6093418118117602\n",
      "Logistic regression iter. 4/29: loss=-303.09136347046353, w0=-0.7631961620107355, w1=-0.7128061585593484\n",
      "Logistic regression iter. 5/29: loss=-147.58096100812452, w0=-0.8741957175545965, w1=-0.8155314906525767\n",
      "Logistic regression iter. 6/29: loss=-73.69998836748185, w0=-0.9847442064086255, w1=-0.9179247242146358\n",
      "Logistic regression iter. 7/29: loss=-38.12173697091942, w0=-1.0950794633713454, w1=-1.0201652478193508\n",
      "Logistic regression iter. 8/29: loss=-20.84464654374136, w0=-1.2053123177875587, w1=-1.1223343128131757\n",
      "Logistic regression iter. 9/29: loss=-12.437118343335278, w0=-1.315495344743267, w1=-1.224469456856907\n",
      "Logistic regression iter. 10/29: loss=-8.379911445645948, w0=-1.4256538468943736, w1=-1.3265882990670337\n",
      "Logistic regression iter. 11/29: loss=-6.478991320310846, w0=-1.5358001531233518, w1=-1.4286992209741438\n",
      "Logistic regression iter. 12/29: loss=-5.657048227085067, w0=-1.6459403371438133, w1=-1.5308062568099237\n",
      "Logistic regression iter. 13/29: loss=-5.379730795644207, w0=-1.7560774210962244, w1=-1.632911368837176\n",
      "Logistic regression iter. 14/29: loss=-5.379547258831523, w0=-1.8662129225658737, w1=-1.7350155206305262\n",
      "Logistic regression iter. 15/29: loss=-5.521451523290729, w0=-1.9763476101130746, w1=-1.8371191894924173\n",
      "Logistic regression iter. 16/29: loss=-5.736722973030699, w0=-2.086481876057531, w1=-1.9392226137561615\n",
      "Logistic regression iter. 17/29: loss=-5.990131895443604, w0=-2.196615922153637, w1=-2.0413259133172788\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-2.306749852882027, w1=-2.143429148908916\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-2.416883722706615, w1=-2.245532351495133\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-2.52701756019607, w1=-2.347635536958711\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-2.637151380424861, w1=-2.4497387134933146\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-2.7472851913920717, w1=-2.55184188534881\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-2.857418997365172, w1=-2.6539450547408117\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-2.967552800632543, w1=-2.756048222830031\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-3.0776866024273377, w1=-2.8581513902273583\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-3.1878204034172026, w1=-2.9602545572557317\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-3.297954203965243, w1=-3.0623577240865907\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-3.408088004269794, w1=-3.1644608908113154\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-3.51822180443964, w1=-3.2665640574788037\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-7810.8129017606625, w0=-0.28513239999999995, w1=-0.27393179522639594\n",
      "Logistic regression iter. 1/29: loss=-3120.055870918313, w0=-0.41784734324622186, w1=-0.3948514664975455\n",
      "Logistic regression iter. 2/29: loss=-1377.7145047584238, w0=-0.5370809761535066, w1=-0.5041759260265046\n",
      "Logistic regression iter. 3/29: loss=-637.0738816622791, w0=-0.6512226150065049, w1=-0.6093418118117602\n",
      "Logistic regression iter. 4/29: loss=-303.09136347046353, w0=-0.7631961620107355, w1=-0.7128061585593484\n",
      "Logistic regression iter. 5/29: loss=-147.58096100812452, w0=-0.8741957175545965, w1=-0.8155314906525767\n",
      "Logistic regression iter. 6/29: loss=-73.69998836748185, w0=-0.9847442064086255, w1=-0.9179247242146358\n",
      "Logistic regression iter. 7/29: loss=-38.12173697091942, w0=-1.0950794633713454, w1=-1.0201652478193508\n",
      "Logistic regression iter. 8/29: loss=-20.84464654374136, w0=-1.2053123177875587, w1=-1.1223343128131757\n",
      "Logistic regression iter. 9/29: loss=-12.437118343335278, w0=-1.315495344743267, w1=-1.224469456856907\n",
      "Logistic regression iter. 10/29: loss=-8.379911445645948, w0=-1.4256538468943736, w1=-1.3265882990670337\n",
      "Logistic regression iter. 11/29: loss=-6.478991320310846, w0=-1.5358001531233518, w1=-1.4286992209741438\n",
      "Logistic regression iter. 12/29: loss=-5.657048227085067, w0=-1.6459403371438133, w1=-1.5308062568099237\n",
      "Logistic regression iter. 13/29: loss=-5.379730795644207, w0=-1.7560774210962244, w1=-1.632911368837176\n",
      "Logistic regression iter. 14/29: loss=-5.379547258831523, w0=-1.8662129225658737, w1=-1.7350155206305262\n",
      "Logistic regression iter. 15/29: loss=-5.521451523290729, w0=-1.9763476101130746, w1=-1.8371191894924173\n",
      "Logistic regression iter. 16/29: loss=-5.736722973030699, w0=-2.086481876057531, w1=-1.9392226137561615\n",
      "Logistic regression iter. 17/29: loss=-5.990131895443604, w0=-2.196615922153637, w1=-2.0413259133172788\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-2.306749852882027, w1=-2.143429148908916\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-2.416883722706615, w1=-2.245532351495133\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-2.52701756019607, w1=-2.347635536958711\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-2.637151380424861, w1=-2.4497387134933146\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-2.7472851913920717, w1=-2.55184188534881\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-2.857418997365172, w1=-2.6539450547408117\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-2.967552800632543, w1=-2.756048222830031\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-3.0776866024273377, w1=-2.8581513902273583\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-3.1878204034172026, w1=-2.9602545572557317\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-3.297954203965243, w1=-3.0623577240865907\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-3.408088004269794, w1=-3.1644608908113154\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-3.51822180443964, w1=-3.2665640574788037\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-7810.8129017606625, w0=-0.28513239999999995, w1=-0.27393179522639594\n",
      "Logistic regression iter. 1/29: loss=-3120.055870918313, w0=-0.41784734324622186, w1=-0.3948514664975455\n",
      "Logistic regression iter. 2/29: loss=-1377.7145047584238, w0=-0.5370809761535066, w1=-0.5041759260265046\n",
      "Logistic regression iter. 3/29: loss=-637.0738816622791, w0=-0.6512226150065049, w1=-0.6093418118117602\n",
      "Logistic regression iter. 4/29: loss=-303.09136347046353, w0=-0.7631961620107355, w1=-0.7128061585593484\n",
      "Logistic regression iter. 5/29: loss=-147.58096100812452, w0=-0.8741957175545965, w1=-0.8155314906525767\n",
      "Logistic regression iter. 6/29: loss=-73.69998836748185, w0=-0.9847442064086255, w1=-0.9179247242146358\n",
      "Logistic regression iter. 7/29: loss=-38.12173697091942, w0=-1.0950794633713454, w1=-1.0201652478193508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 8/29: loss=-20.84464654374136, w0=-1.2053123177875587, w1=-1.1223343128131757\n",
      "Logistic regression iter. 9/29: loss=-12.437118343335278, w0=-1.315495344743267, w1=-1.224469456856907\n",
      "Logistic regression iter. 10/29: loss=-8.379911445645948, w0=-1.4256538468943736, w1=-1.3265882990670337\n",
      "Logistic regression iter. 11/29: loss=-6.478991320310846, w0=-1.5358001531233518, w1=-1.4286992209741438\n",
      "Logistic regression iter. 12/29: loss=-5.657048227085067, w0=-1.6459403371438133, w1=-1.5308062568099237\n",
      "Logistic regression iter. 13/29: loss=-5.379730795644207, w0=-1.7560774210962244, w1=-1.632911368837176\n",
      "Logistic regression iter. 14/29: loss=-5.379547258831523, w0=-1.8662129225658737, w1=-1.7350155206305262\n",
      "Logistic regression iter. 15/29: loss=-5.521451523290729, w0=-1.9763476101130746, w1=-1.8371191894924173\n",
      "Logistic regression iter. 16/29: loss=-5.736722973030699, w0=-2.086481876057531, w1=-1.9392226137561615\n",
      "Logistic regression iter. 17/29: loss=-5.990131895443604, w0=-2.196615922153637, w1=-2.0413259133172788\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-2.306749852882027, w1=-2.143429148908916\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-2.416883722706615, w1=-2.245532351495133\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-2.52701756019607, w1=-2.347635536958711\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-2.637151380424861, w1=-2.4497387134933146\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-2.7472851913920717, w1=-2.55184188534881\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-2.857418997365172, w1=-2.6539450547408117\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-2.967552800632543, w1=-2.756048222830031\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-3.0776866024273377, w1=-2.8581513902273583\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-3.1878204034172026, w1=-2.9602545572557317\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-3.297954203965243, w1=-3.0623577240865907\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-3.408088004269794, w1=-3.1644608908113154\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-3.51822180443964, w1=-3.2665640574788037\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-7810.8129017606625, w0=-0.28513239999999995, w1=-0.27393179522639594\n",
      "Logistic regression iter. 1/29: loss=-3120.055870918313, w0=-0.41784734324622186, w1=-0.3948514664975455\n",
      "Logistic regression iter. 2/29: loss=-1377.7145047584238, w0=-0.5370809761535066, w1=-0.5041759260265046\n",
      "Logistic regression iter. 3/29: loss=-637.0738816622791, w0=-0.6512226150065049, w1=-0.6093418118117602\n",
      "Logistic regression iter. 4/29: loss=-303.09136347046353, w0=-0.7631961620107355, w1=-0.7128061585593484\n",
      "Logistic regression iter. 5/29: loss=-147.58096100812452, w0=-0.8741957175545965, w1=-0.8155314906525767\n",
      "Logistic regression iter. 6/29: loss=-73.69998836748185, w0=-0.9847442064086255, w1=-0.9179247242146358\n",
      "Logistic regression iter. 7/29: loss=-38.12173697091942, w0=-1.0950794633713454, w1=-1.0201652478193508\n",
      "Logistic regression iter. 8/29: loss=-20.84464654374136, w0=-1.2053123177875587, w1=-1.1223343128131757\n",
      "Logistic regression iter. 9/29: loss=-12.437118343335278, w0=-1.315495344743267, w1=-1.224469456856907\n",
      "Logistic regression iter. 10/29: loss=-8.379911445645948, w0=-1.4256538468943736, w1=-1.3265882990670337\n",
      "Logistic regression iter. 11/29: loss=-6.478991320310846, w0=-1.5358001531233518, w1=-1.4286992209741438\n",
      "Logistic regression iter. 12/29: loss=-5.657048227085067, w0=-1.6459403371438133, w1=-1.5308062568099237\n",
      "Logistic regression iter. 13/29: loss=-5.379730795644207, w0=-1.7560774210962244, w1=-1.632911368837176\n",
      "Logistic regression iter. 14/29: loss=-5.379547258831523, w0=-1.8662129225658737, w1=-1.7350155206305262\n",
      "Logistic regression iter. 15/29: loss=-5.521451523290729, w0=-1.9763476101130746, w1=-1.8371191894924173\n",
      "Logistic regression iter. 16/29: loss=-5.736722973030699, w0=-2.086481876057531, w1=-1.9392226137561615\n",
      "Logistic regression iter. 17/29: loss=-5.990131895443604, w0=-2.196615922153637, w1=-2.0413259133172788\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-2.306749852882027, w1=-2.143429148908916\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-2.416883722706615, w1=-2.245532351495133\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-2.52701756019607, w1=-2.347635536958711\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-2.637151380424861, w1=-2.4497387134933146\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-2.7472851913920717, w1=-2.55184188534881\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-2.857418997365172, w1=-2.6539450547408117\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-2.967552800632543, w1=-2.756048222830031\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-3.0776866024273377, w1=-2.8581513902273583\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-3.1878204034172026, w1=-2.9602545572557317\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-3.297954203965243, w1=-3.0623577240865907\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-3.408088004269794, w1=-3.1644608908113154\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-3.51822180443964, w1=-3.2665640574788037\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-6920.001399872075, w0=-0.3014256799999999, w1=-0.28958504066790425\n",
      "Logistic regression iter. 1/29: loss=-2680.1108598448736, w0=-0.43900601190504374, w1=-0.4150211732716246\n",
      "Logistic regression iter. 2/29: loss=-1144.1068896604643, w0=-0.5636840417300781, w1=-0.5294566600938765\n",
      "Logistic regression iter. 3/29: loss=-511.0895519599747, w0=-0.683620156213416, w1=-0.6400537711584244\n",
      "Logistic regression iter. 4/29: loss=-235.0764713953569, w0=-0.801601491656942, w1=-0.7491310391585396\n",
      "Logistic regression iter. 5/29: loss=-110.96073204711189, w0=-0.9187343286860417, w1=-0.857570879860228\n",
      "Logistic regression iter. 6/29: loss=-54.05563792588502, w0=-1.035487742781365, w1=-0.9657343177913977\n",
      "Logistic regression iter. 7/29: loss=-27.634802603708795, w0=-1.1520679637398086, w1=-1.0737751188954217\n",
      "Logistic regression iter. 8/29: loss=-15.29136653763268, w0=-1.2685678438200343, w1=-1.1817605327275338\n",
      "Logistic regression iter. 9/29: loss=-9.543270837720776, w0=-1.3850299445852787, w1=-1.2897205597160084\n",
      "Logistic regression iter. 10/29: loss=-6.923495154369112, w0=-1.5014740656839825, w1=-1.3976688007782205\n",
      "Logistic regression iter. 11/29: loss=-5.803899230608021, w0=-1.6179095368628948, w1=-1.5056115076356924\n",
      "Logistic regression iter. 12/29: loss=-5.410904184844787, w0=-1.734340805147679, w1=-1.6135515890647194\n",
      "Logistic regression iter. 13/29: loss=-5.3730713570989765, w0=-1.850770012499117, w1=-1.7214904132926458\n",
      "Logistic regression iter. 14/29: loss=-5.510325934309853, w0=-1.9671982005997353, w1=-1.8294286303111287\n",
      "Logistic regression iter. 15/29: loss=-5.7345851292472565, w0=-2.083625880588306, w1=-1.9373665517195775\n",
      "Logistic regression iter. 16/29: loss=-6.002403818644809, w0=-2.200053305372428, w1=-2.045304328149919\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-2.3164806010702383, w1=-2.153242032986549\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-2.432907831037395, w1=-2.2611797022396773\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-2.5493350273223374, w1=-2.369117353699468\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-2.6657622062436186, w1=-2.4770549962107076\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-2.782189376162365, w1=-2.584992634197122\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-2.8986165413880514, w1=-2.692930269883765\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-3.0150437041544524, w1=-2.800867904395799\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-3.131470865625691, w1=-2.9088055383050944\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-3.2478980264115838, w1=-3.016743171903718\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-3.3643251868331605, w1=-3.1246808053415256\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-3.480752347060224, w1=-3.2326184386957473\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-3.597179507182997, w1=-3.340556072006353\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-3.7136066672496284, w1=-3.4484937052941134\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-6920.001399872075, w0=-0.3014256799999999, w1=-0.28958504066790425\n",
      "Logistic regression iter. 1/29: loss=-2680.1108598448736, w0=-0.43900601190504374, w1=-0.4150211732716246\n",
      "Logistic regression iter. 2/29: loss=-1144.1068896604643, w0=-0.5636840417300781, w1=-0.5294566600938765\n",
      "Logistic regression iter. 3/29: loss=-511.0895519599747, w0=-0.683620156213416, w1=-0.6400537711584244\n",
      "Logistic regression iter. 4/29: loss=-235.0764713953569, w0=-0.801601491656942, w1=-0.7491310391585396\n",
      "Logistic regression iter. 5/29: loss=-110.96073204711189, w0=-0.9187343286860417, w1=-0.857570879860228\n",
      "Logistic regression iter. 6/29: loss=-54.05563792588502, w0=-1.035487742781365, w1=-0.9657343177913977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 7/29: loss=-27.634802603708795, w0=-1.1520679637398086, w1=-1.0737751188954217\n",
      "Logistic regression iter. 8/29: loss=-15.29136653763268, w0=-1.2685678438200343, w1=-1.1817605327275338\n",
      "Logistic regression iter. 9/29: loss=-9.543270837720776, w0=-1.3850299445852787, w1=-1.2897205597160084\n",
      "Logistic regression iter. 10/29: loss=-6.923495154369112, w0=-1.5014740656839825, w1=-1.3976688007782205\n",
      "Logistic regression iter. 11/29: loss=-5.803899230608021, w0=-1.6179095368628948, w1=-1.5056115076356924\n",
      "Logistic regression iter. 12/29: loss=-5.410904184844787, w0=-1.734340805147679, w1=-1.6135515890647194\n",
      "Logistic regression iter. 13/29: loss=-5.3730713570989765, w0=-1.850770012499117, w1=-1.7214904132926458\n",
      "Logistic regression iter. 14/29: loss=-5.510325934309853, w0=-1.9671982005997353, w1=-1.8294286303111287\n",
      "Logistic regression iter. 15/29: loss=-5.7345851292472565, w0=-2.083625880588306, w1=-1.9373665517195775\n",
      "Logistic regression iter. 16/29: loss=-6.002403818644809, w0=-2.200053305372428, w1=-2.045304328149919\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-2.3164806010702383, w1=-2.153242032986549\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-2.432907831037395, w1=-2.2611797022396773\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-2.5493350273223374, w1=-2.369117353699468\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-2.6657622062436186, w1=-2.4770549962107076\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-2.782189376162365, w1=-2.584992634197122\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-2.8986165413880514, w1=-2.692930269883765\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-3.0150437041544524, w1=-2.800867904395799\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-3.131470865625691, w1=-2.9088055383050944\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-3.2478980264115838, w1=-3.016743171903718\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-3.3643251868331605, w1=-3.1246808053415256\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-3.480752347060224, w1=-3.2326184386957473\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-3.597179507182997, w1=-3.340556072006353\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-3.7136066672496284, w1=-3.4484937052941134\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-6920.001399872075, w0=-0.3014256799999999, w1=-0.28958504066790425\n",
      "Logistic regression iter. 1/29: loss=-2680.1108598448736, w0=-0.43900601190504374, w1=-0.4150211732716246\n",
      "Logistic regression iter. 2/29: loss=-1144.1068896604643, w0=-0.5636840417300781, w1=-0.5294566600938765\n",
      "Logistic regression iter. 3/29: loss=-511.0895519599747, w0=-0.683620156213416, w1=-0.6400537711584244\n",
      "Logistic regression iter. 4/29: loss=-235.0764713953569, w0=-0.801601491656942, w1=-0.7491310391585396\n",
      "Logistic regression iter. 5/29: loss=-110.96073204711189, w0=-0.9187343286860417, w1=-0.857570879860228\n",
      "Logistic regression iter. 6/29: loss=-54.05563792588502, w0=-1.035487742781365, w1=-0.9657343177913977\n",
      "Logistic regression iter. 7/29: loss=-27.634802603708795, w0=-1.1520679637398086, w1=-1.0737751188954217\n",
      "Logistic regression iter. 8/29: loss=-15.29136653763268, w0=-1.2685678438200343, w1=-1.1817605327275338\n",
      "Logistic regression iter. 9/29: loss=-9.543270837720776, w0=-1.3850299445852787, w1=-1.2897205597160084\n",
      "Logistic regression iter. 10/29: loss=-6.923495154369112, w0=-1.5014740656839825, w1=-1.3976688007782205\n",
      "Logistic regression iter. 11/29: loss=-5.803899230608021, w0=-1.6179095368628948, w1=-1.5056115076356924\n",
      "Logistic regression iter. 12/29: loss=-5.410904184844787, w0=-1.734340805147679, w1=-1.6135515890647194\n",
      "Logistic regression iter. 13/29: loss=-5.3730713570989765, w0=-1.850770012499117, w1=-1.7214904132926458\n",
      "Logistic regression iter. 14/29: loss=-5.510325934309853, w0=-1.9671982005997353, w1=-1.8294286303111287\n",
      "Logistic regression iter. 15/29: loss=-5.7345851292472565, w0=-2.083625880588306, w1=-1.9373665517195775\n",
      "Logistic regression iter. 16/29: loss=-6.002403818644809, w0=-2.200053305372428, w1=-2.045304328149919\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-2.3164806010702383, w1=-2.153242032986549\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-2.432907831037395, w1=-2.2611797022396773\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-2.5493350273223374, w1=-2.369117353699468\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-2.6657622062436186, w1=-2.4770549962107076\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-2.782189376162365, w1=-2.584992634197122\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-2.8986165413880514, w1=-2.692930269883765\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-3.0150437041544524, w1=-2.800867904395799\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-3.131470865625691, w1=-2.9088055383050944\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-3.2478980264115838, w1=-3.016743171903718\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-3.3643251868331605, w1=-3.1246808053415256\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-3.480752347060224, w1=-3.2326184386957473\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-3.597179507182997, w1=-3.340556072006353\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-3.7136066672496284, w1=-3.4484937052941134\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-6920.001399872075, w0=-0.3014256799999999, w1=-0.28958504066790425\n",
      "Logistic regression iter. 1/29: loss=-2680.1108598448736, w0=-0.43900601190504374, w1=-0.4150211732716246\n",
      "Logistic regression iter. 2/29: loss=-1144.1068896604643, w0=-0.5636840417300781, w1=-0.5294566600938765\n",
      "Logistic regression iter. 3/29: loss=-511.0895519599747, w0=-0.683620156213416, w1=-0.6400537711584244\n",
      "Logistic regression iter. 4/29: loss=-235.0764713953569, w0=-0.801601491656942, w1=-0.7491310391585396\n",
      "Logistic regression iter. 5/29: loss=-110.96073204711189, w0=-0.9187343286860417, w1=-0.857570879860228\n",
      "Logistic regression iter. 6/29: loss=-54.05563792588502, w0=-1.035487742781365, w1=-0.9657343177913977\n",
      "Logistic regression iter. 7/29: loss=-27.634802603708795, w0=-1.1520679637398086, w1=-1.0737751188954217\n",
      "Logistic regression iter. 8/29: loss=-15.29136653763268, w0=-1.2685678438200343, w1=-1.1817605327275338\n",
      "Logistic regression iter. 9/29: loss=-9.543270837720776, w0=-1.3850299445852787, w1=-1.2897205597160084\n",
      "Logistic regression iter. 10/29: loss=-6.923495154369112, w0=-1.5014740656839825, w1=-1.3976688007782205\n",
      "Logistic regression iter. 11/29: loss=-5.803899230608021, w0=-1.6179095368628948, w1=-1.5056115076356924\n",
      "Logistic regression iter. 12/29: loss=-5.410904184844787, w0=-1.734340805147679, w1=-1.6135515890647194\n",
      "Logistic regression iter. 13/29: loss=-5.3730713570989765, w0=-1.850770012499117, w1=-1.7214904132926458\n",
      "Logistic regression iter. 14/29: loss=-5.510325934309853, w0=-1.9671982005997353, w1=-1.8294286303111287\n",
      "Logistic regression iter. 15/29: loss=-5.7345851292472565, w0=-2.083625880588306, w1=-1.9373665517195775\n",
      "Logistic regression iter. 16/29: loss=-6.002403818644809, w0=-2.200053305372428, w1=-2.045304328149919\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-2.3164806010702383, w1=-2.153242032986549\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-2.432907831037395, w1=-2.2611797022396773\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-2.5493350273223374, w1=-2.369117353699468\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-2.6657622062436186, w1=-2.4770549962107076\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-2.782189376162365, w1=-2.584992634197122\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-2.8986165413880514, w1=-2.692930269883765\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-3.0150437041544524, w1=-2.800867904395799\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-3.131470865625691, w1=-2.9088055383050944\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-3.2478980264115838, w1=-3.016743171903718\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-3.3643251868331605, w1=-3.1246808053415256\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-3.480752347060224, w1=-3.2326184386957473\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-3.597179507182997, w1=-3.340556072006353\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-3.7136066672496284, w1=-3.4484937052941134\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-6134.92305772418, w0=-0.31771895999999994, w1=-0.30523828610941256\n",
      "Logistic regression iter. 1/29: loss=-2303.50697033845, w0=-0.46020511738727377, w1=-0.4352459530304094\n",
      "Logistic regression iter. 2/29: loss=-950.6265843743598, w0=-0.5903878089224046, w1=-0.5548514229571334\n",
      "Logistic regression iter. 3/29: loss=-410.3283862150943, w0=-0.716172741994819, w1=-0.6709275204067079\n",
      "Logistic regression iter. 4/29: loss=-182.59200143406537, w0=-0.8402030063376462, w1=-0.7856516559158658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 5/29: loss=-83.7077034839246, w0=-0.9634974367497455, w1=-0.8998283106865141\n",
      "Logistic regression iter. 6/29: loss=-39.96487921922837, w0=-1.0864741279733543, w1=-1.0137758762833864\n",
      "Logistic regression iter. 7/29: loss=-20.396847566588395, w0=-1.2093107448100107, w1=-1.12762534875524\n",
      "Logistic regression iter. 8/29: loss=-11.619969272187646, w0=-1.3320845802803276, w1=-1.2414320513726262\n",
      "Logistic regression iter. 9/29: loss=-7.731601790385553, w0=-1.4548298762704577, w1=-1.3552198198026686\n",
      "Logistic regression iter. 10/29: loss=-6.08625384556581, w0=-1.5775620347491037, w1=-1.4689990940653432\n",
      "Logistic regression iter. 11/29: loss=-5.48191468937227, w0=-1.7002880765035848, w1=-1.5827745123573462\n",
      "Logistic regression iter. 12/29: loss=-5.365783285183631, w0=-1.8230112404984182, w1=-1.696548161396045\n",
      "Logistic regression iter. 13/29: loss=-5.48082437216145, w0=-1.9457330374582096, w1=-1.8103209906830282\n",
      "Logistic regression iter. 14/29: loss=-5.706313906104006, w0=-2.068454179173305, w1=-1.9240934367329714\n",
      "Logistic regression iter. 15/29: loss=-5.98501767781787, w0=-2.1911750041698945, w1=-2.0378657021272137\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-2.3138956748660524, w1=-2.151637881703022\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-2.4366162698288645, w1=-2.265410020217588\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-2.559336827358514, w1=-2.379182138952863\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-2.6820573662623404, w1=-2.492954248099798\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-2.804777895839666, w1=-2.6067263525707185\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-2.927498420718774, w1=-2.720498454748291\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-3.0502189432175575, w1=-2.834270555795024\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-3.172939464503752, w1=-2.948042656281281\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-3.295659985168991, w1=-3.061814756488391\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-3.418380505514653, w1=-3.17558685655582\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-3.541101025695055, w1=-3.2893589565530412\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-3.6638215457896055, w1=-3.4031310565148227\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-3.7865420658393614, w1=-3.5169031564586413\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-3.9092625858656462, w1=-3.6306752563933187\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-6134.92305772418, w0=-0.31771895999999994, w1=-0.30523828610941256\n",
      "Logistic regression iter. 1/29: loss=-2303.50697033845, w0=-0.46020511738727377, w1=-0.4352459530304094\n",
      "Logistic regression iter. 2/29: loss=-950.6265843743598, w0=-0.5903878089224046, w1=-0.5548514229571334\n",
      "Logistic regression iter. 3/29: loss=-410.3283862150943, w0=-0.716172741994819, w1=-0.6709275204067079\n",
      "Logistic regression iter. 4/29: loss=-182.59200143406537, w0=-0.8402030063376462, w1=-0.7856516559158658\n",
      "Logistic regression iter. 5/29: loss=-83.7077034839246, w0=-0.9634974367497455, w1=-0.8998283106865141\n",
      "Logistic regression iter. 6/29: loss=-39.96487921922837, w0=-1.0864741279733543, w1=-1.0137758762833864\n",
      "Logistic regression iter. 7/29: loss=-20.396847566588395, w0=-1.2093107448100107, w1=-1.12762534875524\n",
      "Logistic regression iter. 8/29: loss=-11.619969272187646, w0=-1.3320845802803276, w1=-1.2414320513726262\n",
      "Logistic regression iter. 9/29: loss=-7.731601790385553, w0=-1.4548298762704577, w1=-1.3552198198026686\n",
      "Logistic regression iter. 10/29: loss=-6.08625384556581, w0=-1.5775620347491037, w1=-1.4689990940653432\n",
      "Logistic regression iter. 11/29: loss=-5.48191468937227, w0=-1.7002880765035848, w1=-1.5827745123573462\n",
      "Logistic regression iter. 12/29: loss=-5.365783285183631, w0=-1.8230112404984182, w1=-1.696548161396045\n",
      "Logistic regression iter. 13/29: loss=-5.48082437216145, w0=-1.9457330374582096, w1=-1.8103209906830282\n",
      "Logistic regression iter. 14/29: loss=-5.706313906104006, w0=-2.068454179173305, w1=-1.9240934367329714\n",
      "Logistic regression iter. 15/29: loss=-5.98501767781787, w0=-2.1911750041698945, w1=-2.0378657021272137\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-2.3138956748660524, w1=-2.151637881703022\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-2.4366162698288645, w1=-2.265410020217588\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-2.559336827358514, w1=-2.379182138952863\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-2.6820573662623404, w1=-2.492954248099798\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-2.804777895839666, w1=-2.6067263525707185\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-2.927498420718774, w1=-2.720498454748291\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-3.0502189432175575, w1=-2.834270555795024\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-3.172939464503752, w1=-2.948042656281281\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-3.295659985168991, w1=-3.061814756488391\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-3.418380505514653, w1=-3.17558685655582\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-3.541101025695055, w1=-3.2893589565530412\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-3.6638215457896055, w1=-3.4031310565148227\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-3.7865420658393614, w1=-3.5169031564586413\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-3.9092625858656462, w1=-3.6306752563933187\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-6134.92305772418, w0=-0.31771895999999994, w1=-0.30523828610941256\n",
      "Logistic regression iter. 1/29: loss=-2303.50697033845, w0=-0.46020511738727377, w1=-0.4352459530304094\n",
      "Logistic regression iter. 2/29: loss=-950.6265843743598, w0=-0.5903878089224046, w1=-0.5548514229571334\n",
      "Logistic regression iter. 3/29: loss=-410.3283862150943, w0=-0.716172741994819, w1=-0.6709275204067079\n",
      "Logistic regression iter. 4/29: loss=-182.59200143406537, w0=-0.8402030063376462, w1=-0.7856516559158658\n",
      "Logistic regression iter. 5/29: loss=-83.7077034839246, w0=-0.9634974367497455, w1=-0.8998283106865141\n",
      "Logistic regression iter. 6/29: loss=-39.96487921922837, w0=-1.0864741279733543, w1=-1.0137758762833864\n",
      "Logistic regression iter. 7/29: loss=-20.396847566588395, w0=-1.2093107448100107, w1=-1.12762534875524\n",
      "Logistic regression iter. 8/29: loss=-11.619969272187646, w0=-1.3320845802803276, w1=-1.2414320513726262\n",
      "Logistic regression iter. 9/29: loss=-7.731601790385553, w0=-1.4548298762704577, w1=-1.3552198198026686\n",
      "Logistic regression iter. 10/29: loss=-6.08625384556581, w0=-1.5775620347491037, w1=-1.4689990940653432\n",
      "Logistic regression iter. 11/29: loss=-5.48191468937227, w0=-1.7002880765035848, w1=-1.5827745123573462\n",
      "Logistic regression iter. 12/29: loss=-5.365783285183631, w0=-1.8230112404984182, w1=-1.696548161396045\n",
      "Logistic regression iter. 13/29: loss=-5.48082437216145, w0=-1.9457330374582096, w1=-1.8103209906830282\n",
      "Logistic regression iter. 14/29: loss=-5.706313906104006, w0=-2.068454179173305, w1=-1.9240934367329714\n",
      "Logistic regression iter. 15/29: loss=-5.98501767781787, w0=-2.1911750041698945, w1=-2.0378657021272137\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-2.3138956748660524, w1=-2.151637881703022\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-2.4366162698288645, w1=-2.265410020217588\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-2.559336827358514, w1=-2.379182138952863\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-2.6820573662623404, w1=-2.492954248099798\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-2.804777895839666, w1=-2.6067263525707185\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-2.927498420718774, w1=-2.720498454748291\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-3.0502189432175575, w1=-2.834270555795024\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-3.172939464503752, w1=-2.948042656281281\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-3.295659985168991, w1=-3.061814756488391\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-3.418380505514653, w1=-3.17558685655582\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-3.541101025695055, w1=-3.2893589565530412\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-3.6638215457896055, w1=-3.4031310565148227\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-3.7865420658393614, w1=-3.5169031564586413\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-3.9092625858656462, w1=-3.6306752563933187\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-6134.92305772418, w0=-0.31771895999999994, w1=-0.30523828610941256\n",
      "Logistic regression iter. 1/29: loss=-2303.50697033845, w0=-0.46020511738727377, w1=-0.4352459530304094\n",
      "Logistic regression iter. 2/29: loss=-950.6265843743598, w0=-0.5903878089224046, w1=-0.5548514229571334\n",
      "Logistic regression iter. 3/29: loss=-410.3283862150943, w0=-0.716172741994819, w1=-0.6709275204067079\n",
      "Logistic regression iter. 4/29: loss=-182.59200143406537, w0=-0.8402030063376462, w1=-0.7856516559158658\n",
      "Logistic regression iter. 5/29: loss=-83.7077034839246, w0=-0.9634974367497455, w1=-0.8998283106865141\n",
      "Logistic regression iter. 6/29: loss=-39.96487921922837, w0=-1.0864741279733543, w1=-1.0137758762833864\n",
      "Logistic regression iter. 7/29: loss=-20.396847566588395, w0=-1.2093107448100107, w1=-1.12762534875524\n",
      "Logistic regression iter. 8/29: loss=-11.619969272187646, w0=-1.3320845802803276, w1=-1.2414320513726262\n",
      "Logistic regression iter. 9/29: loss=-7.731601790385553, w0=-1.4548298762704577, w1=-1.3552198198026686\n",
      "Logistic regression iter. 10/29: loss=-6.08625384556581, w0=-1.5775620347491037, w1=-1.4689990940653432\n",
      "Logistic regression iter. 11/29: loss=-5.48191468937227, w0=-1.7002880765035848, w1=-1.5827745123573462\n",
      "Logistic regression iter. 12/29: loss=-5.365783285183631, w0=-1.8230112404984182, w1=-1.696548161396045\n",
      "Logistic regression iter. 13/29: loss=-5.48082437216145, w0=-1.9457330374582096, w1=-1.8103209906830282\n",
      "Logistic regression iter. 14/29: loss=-5.706313906104006, w0=-2.068454179173305, w1=-1.9240934367329714\n",
      "Logistic regression iter. 15/29: loss=-5.98501767781787, w0=-2.1911750041698945, w1=-2.0378657021272137\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-2.3138956748660524, w1=-2.151637881703022\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-2.4366162698288645, w1=-2.265410020217588\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-2.559336827358514, w1=-2.379182138952863\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-2.6820573662623404, w1=-2.492954248099798\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-2.804777895839666, w1=-2.6067263525707185\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-2.927498420718774, w1=-2.720498454748291\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-3.0502189432175575, w1=-2.834270555795024\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-3.172939464503752, w1=-2.948042656281281\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-3.295659985168991, w1=-3.061814756488391\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-3.418380505514653, w1=-3.17558685655582\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-3.541101025695055, w1=-3.2893589565530412\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-3.6638215457896055, w1=-3.4031310565148227\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-3.7865420658393614, w1=-3.5169031564586413\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-3.9092625858656462, w1=-3.6306752563933187\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-5442.529403664871, w0=-0.3340122399999999, w1=-0.32089153155092087\n",
      "Logistic regression iter. 1/29: loss=-1980.8432204517135, w0=-0.48145463654194814, w1=-0.4555320300829747\n",
      "Logistic regression iter. 2/29: loss=-790.265231871214, w0=-0.6172021043224014, w1=-0.5803646499587781\n",
      "Logistic regression iter. 3/29: loss=-329.69138282158957, w0=-0.7488861211701704, w1=-0.7019631334080425\n",
      "Logistic regression iter. 4/29: loss=-142.06867325900828, w0=-0.8790013771889462, w1=-0.8223634575350286\n",
      "Logistic regression iter. 5/29: loss=-63.416071330807405, w0=-1.0084810906144075, w1=-0.9422953364018921\n",
      "Logistic regression iter. 6/29: loss=-29.85741667462421, w0=-1.137695771991015, w1=-1.0620380433613623\n",
      "Logistic regression iter. 7/29: loss=-15.409046683874223, w0=-1.2667975883889793, w1=-1.1817025642492316\n",
      "Logistic regression iter. 8/29: loss=-9.207864531218659, w0=-1.3958505150640774, w1=-1.3013341660798745\n",
      "Logistic regression iter. 9/29: loss=-6.62009647454028, w0=-1.5248819502646045, w1=-1.4209516883962745\n",
      "Logistic regression iter. 10/29: loss=-5.636627286397425, w0=-1.6539038131700328, w1=-1.5405631054086075\n",
      "Logistic regression iter. 11/29: loss=-5.373992156945821, w0=-1.7829213614396608, w1=-1.6601718422086609\n",
      "Logistic regression iter. 12/29: loss=-5.43879490367065, w0=-1.9119369435104678, w1=-1.7797793892365474\n",
      "Logistic regression iter. 13/29: loss=-5.653846221414494, w0=-2.040951620451139, w1=-1.8993864027212404\n",
      "Logistic regression iter. 14/29: loss=-5.938492954696633, w0=-2.1699658767713887, w1=-2.01899317469855\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-2.2989799358947365, w1=-2.138599836409968\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-2.4279939017986645, w1=-2.2582064473711827\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-2.557007823290503, w1=-2.3778130347987303\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-2.686021723466731, w1=-2.497419611236377\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-2.8150356133408287, w1=-2.6170261825078294\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-2.94404949820265, w1=-2.7366327513355113\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-3.0730633806104395, w1=-2.8562393190003545\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-3.202077261809493, w1=-2.975845886108758\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-3.3310911424097647, w1=-3.0954524529494667\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-3.46010502271179, w1=-3.2150590196607327\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-3.589118902864486, w1=-3.3346655863091015\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-3.71813278294204, w1=-3.454272152926765\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-3.8471466629816025, w1=-3.5738787195293718\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-3.976160543001869, w1=-3.693485286124563\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-4.105174423012293, w1=-3.8130918527160875\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-5442.529403664871, w0=-0.3340122399999999, w1=-0.32089153155092087\n",
      "Logistic regression iter. 1/29: loss=-1980.8432204517135, w0=-0.48145463654194814, w1=-0.4555320300829747\n",
      "Logistic regression iter. 2/29: loss=-790.265231871214, w0=-0.6172021043224014, w1=-0.5803646499587781\n",
      "Logistic regression iter. 3/29: loss=-329.69138282158957, w0=-0.7488861211701704, w1=-0.7019631334080425\n",
      "Logistic regression iter. 4/29: loss=-142.06867325900828, w0=-0.8790013771889462, w1=-0.8223634575350286\n",
      "Logistic regression iter. 5/29: loss=-63.416071330807405, w0=-1.0084810906144075, w1=-0.9422953364018921\n",
      "Logistic regression iter. 6/29: loss=-29.85741667462421, w0=-1.137695771991015, w1=-1.0620380433613623\n",
      "Logistic regression iter. 7/29: loss=-15.409046683874223, w0=-1.2667975883889793, w1=-1.1817025642492316\n",
      "Logistic regression iter. 8/29: loss=-9.207864531218659, w0=-1.3958505150640774, w1=-1.3013341660798745\n",
      "Logistic regression iter. 9/29: loss=-6.62009647454028, w0=-1.5248819502646045, w1=-1.4209516883962745\n",
      "Logistic regression iter. 10/29: loss=-5.636627286397425, w0=-1.6539038131700328, w1=-1.5405631054086075\n",
      "Logistic regression iter. 11/29: loss=-5.373992156945821, w0=-1.7829213614396608, w1=-1.6601718422086609\n",
      "Logistic regression iter. 12/29: loss=-5.43879490367065, w0=-1.9119369435104678, w1=-1.7797793892365474\n",
      "Logistic regression iter. 13/29: loss=-5.653846221414494, w0=-2.040951620451139, w1=-1.8993864027212404\n",
      "Logistic regression iter. 14/29: loss=-5.938492954696633, w0=-2.1699658767713887, w1=-2.01899317469855\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-2.2989799358947365, w1=-2.138599836409968\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-2.4279939017986645, w1=-2.2582064473711827\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-2.557007823290503, w1=-2.3778130347987303\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-2.686021723466731, w1=-2.497419611236377\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-2.8150356133408287, w1=-2.6170261825078294\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-2.94404949820265, w1=-2.7366327513355113\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-3.0730633806104395, w1=-2.8562393190003545\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-3.202077261809493, w1=-2.975845886108758\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-3.3310911424097647, w1=-3.0954524529494667\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-3.46010502271179, w1=-3.2150590196607327\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-3.589118902864486, w1=-3.3346655863091015\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-3.71813278294204, w1=-3.454272152926765\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-3.8471466629816025, w1=-3.5738787195293718\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-3.976160543001869, w1=-3.693485286124563\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-4.105174423012293, w1=-3.8130918527160875\n",
      "4\n",
      "4,5\n",
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-5442.529403664871, w0=-0.3340122399999999, w1=-0.32089153155092087\n",
      "Logistic regression iter. 1/29: loss=-1980.8432204517135, w0=-0.48145463654194814, w1=-0.4555320300829747\n",
      "Logistic regression iter. 2/29: loss=-790.265231871214, w0=-0.6172021043224014, w1=-0.5803646499587781\n",
      "Logistic regression iter. 3/29: loss=-329.69138282158957, w0=-0.7488861211701704, w1=-0.7019631334080425\n",
      "Logistic regression iter. 4/29: loss=-142.06867325900828, w0=-0.8790013771889462, w1=-0.8223634575350286\n",
      "Logistic regression iter. 5/29: loss=-63.416071330807405, w0=-1.0084810906144075, w1=-0.9422953364018921\n",
      "Logistic regression iter. 6/29: loss=-29.85741667462421, w0=-1.137695771991015, w1=-1.0620380433613623\n",
      "Logistic regression iter. 7/29: loss=-15.409046683874223, w0=-1.2667975883889793, w1=-1.1817025642492316\n",
      "Logistic regression iter. 8/29: loss=-9.207864531218659, w0=-1.3958505150640774, w1=-1.3013341660798745\n",
      "Logistic regression iter. 9/29: loss=-6.62009647454028, w0=-1.5248819502646045, w1=-1.4209516883962745\n",
      "Logistic regression iter. 10/29: loss=-5.636627286397425, w0=-1.6539038131700328, w1=-1.5405631054086075\n",
      "Logistic regression iter. 11/29: loss=-5.373992156945821, w0=-1.7829213614396608, w1=-1.6601718422086609\n",
      "Logistic regression iter. 12/29: loss=-5.43879490367065, w0=-1.9119369435104678, w1=-1.7797793892365474\n",
      "Logistic regression iter. 13/29: loss=-5.653846221414494, w0=-2.040951620451139, w1=-1.8993864027212404\n",
      "Logistic regression iter. 14/29: loss=-5.938492954696633, w0=-2.1699658767713887, w1=-2.01899317469855\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-2.2989799358947365, w1=-2.138599836409968\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-2.4279939017986645, w1=-2.2582064473711827\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-2.557007823290503, w1=-2.3778130347987303\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-2.686021723466731, w1=-2.497419611236377\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-2.8150356133408287, w1=-2.6170261825078294\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-2.94404949820265, w1=-2.7366327513355113\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-3.0730633806104395, w1=-2.8562393190003545\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-3.202077261809493, w1=-2.975845886108758\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-3.3310911424097647, w1=-3.0954524529494667\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-3.46010502271179, w1=-3.2150590196607327\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-3.589118902864486, w1=-3.3346655863091015\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-3.71813278294204, w1=-3.454272152926765\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-3.8471466629816025, w1=-3.5738787195293718\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-3.976160543001869, w1=-3.693485286124563\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-4.105174423012293, w1=-3.8130918527160875\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-5442.529403664871, w0=-0.3340122399999999, w1=-0.32089153155092087\n",
      "Logistic regression iter. 1/29: loss=-1980.8432204517135, w0=-0.48145463654194814, w1=-0.4555320300829747\n",
      "Logistic regression iter. 2/29: loss=-790.265231871214, w0=-0.6172021043224014, w1=-0.5803646499587781\n",
      "Logistic regression iter. 3/29: loss=-329.69138282158957, w0=-0.7488861211701704, w1=-0.7019631334080425\n",
      "Logistic regression iter. 4/29: loss=-142.06867325900828, w0=-0.8790013771889462, w1=-0.8223634575350286\n",
      "Logistic regression iter. 5/29: loss=-63.416071330807405, w0=-1.0084810906144075, w1=-0.9422953364018921\n",
      "Logistic regression iter. 6/29: loss=-29.85741667462421, w0=-1.137695771991015, w1=-1.0620380433613623\n",
      "Logistic regression iter. 7/29: loss=-15.409046683874223, w0=-1.2667975883889793, w1=-1.1817025642492316\n",
      "Logistic regression iter. 8/29: loss=-9.207864531218659, w0=-1.3958505150640774, w1=-1.3013341660798745\n",
      "Logistic regression iter. 9/29: loss=-6.62009647454028, w0=-1.5248819502646045, w1=-1.4209516883962745\n",
      "Logistic regression iter. 10/29: loss=-5.636627286397425, w0=-1.6539038131700328, w1=-1.5405631054086075\n",
      "Logistic regression iter. 11/29: loss=-5.373992156945821, w0=-1.7829213614396608, w1=-1.6601718422086609\n",
      "Logistic regression iter. 12/29: loss=-5.43879490367065, w0=-1.9119369435104678, w1=-1.7797793892365474\n",
      "Logistic regression iter. 13/29: loss=-5.653846221414494, w0=-2.040951620451139, w1=-1.8993864027212404\n",
      "Logistic regression iter. 14/29: loss=-5.938492954696633, w0=-2.1699658767713887, w1=-2.01899317469855\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-2.2989799358947365, w1=-2.138599836409968\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-2.4279939017986645, w1=-2.2582064473711827\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-2.557007823290503, w1=-2.3778130347987303\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-2.686021723466731, w1=-2.497419611236377\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-2.8150356133408287, w1=-2.6170261825078294\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-2.94404949820265, w1=-2.7366327513355113\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-3.0730633806104395, w1=-2.8562393190003545\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-3.202077261809493, w1=-2.975845886108758\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-3.3310911424097647, w1=-3.0954524529494667\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-3.46010502271179, w1=-3.2150590196607327\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-3.589118902864486, w1=-3.3346655863091015\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-3.71813278294204, w1=-3.454272152926765\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-3.8471466629816025, w1=-3.5738787195293718\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-3.976160543001869, w1=-3.693485286124563\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-4.105174423012293, w1=-3.8130918527160875\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-4831.442109283106, w0=-0.3503055199999999, w1=-0.33654477699242924\n",
      "Logistic regression iter. 1/29: loss=-1704.194809457701, w0=-0.5027614957482179, w1=-0.47588307761181026\n",
      "Logistic regression iter. 2/29: loss=-657.2794177864679, w0=-0.6441325656919975, w1=-0.605997483042328\n",
      "Logistic regression iter. 3/29: loss=-265.1290294310063, w0=-0.7817616873584842, w1=-0.7331574578038054\n",
      "Logistic regression iter. 4/29: loss=-110.76697166574824, w0=-0.9179933162624747, w1=-0.8592591385522721\n",
      "Logistic regression iter. 5/29: loss=-48.30354386829164, w0=-1.0536780200500582, w1=-0.9849613601074465\n",
      "Logistic regression iter. 6/29: loss=-22.6106525357543, w0=-1.1891424316439263, w1=-1.110507884770171\n",
      "Logistic regression iter. 7/29: loss=-11.982122812508798, w0=-1.324516201267149, w1=-1.2359922827231924\n",
      "Logistic regression iter. 8/29: loss=-7.640086707321595, w0=-1.4598520137103055, w1=-1.3614514154825452\n",
      "Logistic regression iter. 9/29: loss=-5.962674762682067, w0=-1.5951716859944551, w1=-1.4869001055831848\n",
      "Logistic regression iter. 10/29: loss=-5.430359118244968, w0=-1.730484400380224, w1=-1.6123444174778316\n",
      "Logistic regression iter. 11/29: loss=-5.396303880860107, w0=-1.8657940776003477, w1=-1.7377868701672452\n",
      "Logistic regression iter. 12/29: loss=-5.581546115186857, w0=-2.0011024137846807, w1=-1.8632285241642292\n",
      "Logistic regression iter. 13/29: loss=-5.86433402106158, w0=-2.1364101515250673, w1=-1.9886698314109452\n",
      "Logistic regression iter. 14/29: loss=-6.190941659084491, w0=-2.2717176195542144, w1=-2.1141109866483916\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-2.407024964899112, w1=-2.239552074646541\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-2.54233225395195, w1=-2.3649931326528995\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-2.677639516963943, w1=-2.490434177176804\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-2.8129467678360203, w1=-2.615875215595328\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-2.9482540130071078, w1=-2.74131625122999\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-3.0835612554823464, w1=-2.866757285587012\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-3.218868496674365, w1=-2.992198319354043\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-3.3541757372517336, w1=-3.1176393528470263\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-3.4894829775329295, w1=-3.243080386212005\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-3.624790217670599, w1=-3.368521419516875\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-3.760097457738337, w1=-3.4939624527933755\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-3.8954046977718253, w1=-3.619403486056421\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-4.030711937788456, w1=-3.7448445193130557\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-4.166019177796751, w1=-3.8702855525666218\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-4.301326417800905, w1=-3.995726585818713\n",
      "4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-4831.442109283106, w0=-0.3503055199999999, w1=-0.33654477699242924\n",
      "Logistic regression iter. 1/29: loss=-1704.194809457701, w0=-0.5027614957482179, w1=-0.47588307761181026\n",
      "Logistic regression iter. 2/29: loss=-657.2794177864679, w0=-0.6441325656919975, w1=-0.605997483042328\n",
      "Logistic regression iter. 3/29: loss=-265.1290294310063, w0=-0.7817616873584842, w1=-0.7331574578038054\n",
      "Logistic regression iter. 4/29: loss=-110.76697166574824, w0=-0.9179933162624747, w1=-0.8592591385522721\n",
      "Logistic regression iter. 5/29: loss=-48.30354386829164, w0=-1.0536780200500582, w1=-0.9849613601074465\n",
      "Logistic regression iter. 6/29: loss=-22.6106525357543, w0=-1.1891424316439263, w1=-1.110507884770171\n",
      "Logistic regression iter. 7/29: loss=-11.982122812508798, w0=-1.324516201267149, w1=-1.2359922827231924\n",
      "Logistic regression iter. 8/29: loss=-7.640086707321595, w0=-1.4598520137103055, w1=-1.3614514154825452\n",
      "Logistic regression iter. 9/29: loss=-5.962674762682067, w0=-1.5951716859944551, w1=-1.4869001055831848\n",
      "Logistic regression iter. 10/29: loss=-5.430359118244968, w0=-1.730484400380224, w1=-1.6123444174778316\n",
      "Logistic regression iter. 11/29: loss=-5.396303880860107, w0=-1.8657940776003477, w1=-1.7377868701672452\n",
      "Logistic regression iter. 12/29: loss=-5.581546115186857, w0=-2.0011024137846807, w1=-1.8632285241642292\n",
      "Logistic regression iter. 13/29: loss=-5.86433402106158, w0=-2.1364101515250673, w1=-1.9886698314109452\n",
      "Logistic regression iter. 14/29: loss=-6.190941659084491, w0=-2.2717176195542144, w1=-2.1141109866483916\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-2.407024964899112, w1=-2.239552074646541\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-2.54233225395195, w1=-2.3649931326528995\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-2.677639516963943, w1=-2.490434177176804\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-2.8129467678360203, w1=-2.615875215595328\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-2.9482540130071078, w1=-2.74131625122999\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-3.0835612554823464, w1=-2.866757285587012\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-3.218868496674365, w1=-2.992198319354043\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-3.3541757372517336, w1=-3.1176393528470263\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-3.4894829775329295, w1=-3.243080386212005\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-3.624790217670599, w1=-3.368521419516875\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-3.760097457738337, w1=-3.4939624527933755\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-3.8954046977718253, w1=-3.619403486056421\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-4.030711937788456, w1=-3.7448445193130557\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-4.166019177796751, w1=-3.8702855525666218\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-4.301326417800905, w1=-3.995726585818713\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-4831.442109283106, w0=-0.3503055199999999, w1=-0.33654477699242924\n",
      "Logistic regression iter. 1/29: loss=-1704.194809457701, w0=-0.5027614957482179, w1=-0.47588307761181026\n",
      "Logistic regression iter. 2/29: loss=-657.2794177864679, w0=-0.6441325656919975, w1=-0.605997483042328\n",
      "Logistic regression iter. 3/29: loss=-265.1290294310063, w0=-0.7817616873584842, w1=-0.7331574578038054\n",
      "Logistic regression iter. 4/29: loss=-110.76697166574824, w0=-0.9179933162624747, w1=-0.8592591385522721\n",
      "Logistic regression iter. 5/29: loss=-48.30354386829164, w0=-1.0536780200500582, w1=-0.9849613601074465\n",
      "Logistic regression iter. 6/29: loss=-22.6106525357543, w0=-1.1891424316439263, w1=-1.110507884770171\n",
      "Logistic regression iter. 7/29: loss=-11.982122812508798, w0=-1.324516201267149, w1=-1.2359922827231924\n",
      "Logistic regression iter. 8/29: loss=-7.640086707321595, w0=-1.4598520137103055, w1=-1.3614514154825452\n",
      "Logistic regression iter. 9/29: loss=-5.962674762682067, w0=-1.5951716859944551, w1=-1.4869001055831848\n",
      "Logistic regression iter. 10/29: loss=-5.430359118244968, w0=-1.730484400380224, w1=-1.6123444174778316\n",
      "Logistic regression iter. 11/29: loss=-5.396303880860107, w0=-1.8657940776003477, w1=-1.7377868701672452\n",
      "Logistic regression iter. 12/29: loss=-5.581546115186857, w0=-2.0011024137846807, w1=-1.8632285241642292\n",
      "Logistic regression iter. 13/29: loss=-5.86433402106158, w0=-2.1364101515250673, w1=-1.9886698314109452\n",
      "Logistic regression iter. 14/29: loss=-6.190941659084491, w0=-2.2717176195542144, w1=-2.1141109866483916\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-2.407024964899112, w1=-2.239552074646541\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-2.54233225395195, w1=-2.3649931326528995\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-2.677639516963943, w1=-2.490434177176804\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-2.8129467678360203, w1=-2.615875215595328\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-2.9482540130071078, w1=-2.74131625122999\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-3.0835612554823464, w1=-2.866757285587012\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-3.218868496674365, w1=-2.992198319354043\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-3.3541757372517336, w1=-3.1176393528470263\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-3.4894829775329295, w1=-3.243080386212005\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-3.624790217670599, w1=-3.368521419516875\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-3.760097457738337, w1=-3.4939624527933755\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-3.8954046977718253, w1=-3.619403486056421\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-4.030711937788456, w1=-3.7448445193130557\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-4.166019177796751, w1=-3.8702855525666218\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-4.301326417800905, w1=-3.995726585818713\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-4831.442109283106, w0=-0.3503055199999999, w1=-0.33654477699242924\n",
      "Logistic regression iter. 1/29: loss=-1704.194809457701, w0=-0.5027614957482179, w1=-0.47588307761181026\n",
      "Logistic regression iter. 2/29: loss=-657.2794177864679, w0=-0.6441325656919975, w1=-0.605997483042328\n",
      "Logistic regression iter. 3/29: loss=-265.1290294310063, w0=-0.7817616873584842, w1=-0.7331574578038054\n",
      "Logistic regression iter. 4/29: loss=-110.76697166574824, w0=-0.9179933162624747, w1=-0.8592591385522721\n",
      "Logistic regression iter. 5/29: loss=-48.30354386829164, w0=-1.0536780200500582, w1=-0.9849613601074465\n",
      "Logistic regression iter. 6/29: loss=-22.6106525357543, w0=-1.1891424316439263, w1=-1.110507884770171\n",
      "Logistic regression iter. 7/29: loss=-11.982122812508798, w0=-1.324516201267149, w1=-1.2359922827231924\n",
      "Logistic regression iter. 8/29: loss=-7.640086707321595, w0=-1.4598520137103055, w1=-1.3614514154825452\n",
      "Logistic regression iter. 9/29: loss=-5.962674762682067, w0=-1.5951716859944551, w1=-1.4869001055831848\n",
      "Logistic regression iter. 10/29: loss=-5.430359118244968, w0=-1.730484400380224, w1=-1.6123444174778316\n",
      "Logistic regression iter. 11/29: loss=-5.396303880860107, w0=-1.8657940776003477, w1=-1.7377868701672452\n",
      "Logistic regression iter. 12/29: loss=-5.581546115186857, w0=-2.0011024137846807, w1=-1.8632285241642292\n",
      "Logistic regression iter. 13/29: loss=-5.86433402106158, w0=-2.1364101515250673, w1=-1.9886698314109452\n",
      "Logistic regression iter. 14/29: loss=-6.190941659084491, w0=-2.2717176195542144, w1=-2.1141109866483916\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-2.407024964899112, w1=-2.239552074646541\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-2.54233225395195, w1=-2.3649931326528995\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-2.677639516963943, w1=-2.490434177176804\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-2.8129467678360203, w1=-2.615875215595328\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-2.9482540130071078, w1=-2.74131625122999\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-3.0835612554823464, w1=-2.866757285587012\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-3.218868496674365, w1=-2.992198319354043\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-3.3541757372517336, w1=-3.1176393528470263\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-3.4894829775329295, w1=-3.243080386212005\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-3.624790217670599, w1=-3.368521419516875\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-3.760097457738337, w1=-3.4939624527933755\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-3.8954046977718253, w1=-3.619403486056421\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-4.030711937788456, w1=-3.7448445193130557\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-4.166019177796751, w1=-3.8702855525666218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 29/29: loss=nan, w0=-4.301326417800905, w1=-3.995726585818713\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-4291.7324932321535, w0=-0.36659879999999995, w1=-0.3521980224339376\n",
      "Logistic regression iter. 1/29: loss=-1466.8558768720384, w0=-0.5241301841637549, w1=-0.49630080129807114\n",
      "Logistic regression iter. 2/29: loss=-546.946926690037, w0=-0.6711815667476522, w1=-0.6317486058253767\n",
      "Logistic regression iter. 3/29: loss=-213.41765733364005, w0=-0.8147975662922988, w1=-0.7645050542533637\n",
      "Logistic regression iter. 4/29: loss=-86.58050009827389, w0=-0.9571727024205381, w1=-0.8963295767394698\n",
      "Logistic regression iter. 5/29: loss=-37.047691602170644, w0=-1.0990787083251345, w1=-1.027814501976157\n",
      "Logistic regression iter. 6/29: loss=-17.420764532192354, w0=-1.240802179650174, w1=-1.1591716549611217\n",
      "Logistic regression iter. 7/29: loss=-9.639555427619326, w0=-1.382453066448611, w1=-1.2904795757255196\n",
      "Logistic regression iter. 8/29: loss=-6.639471380394017, w0=-1.5240745609684445, w1=-1.4217681534282034\n",
      "Logistic regression iter. 9/29: loss=-5.600428746098675, w0=-1.6656839623219122, w1=-1.5530490030122777\n",
      "Logistic regression iter. 10/29: loss=-5.377322711997231, w0=-1.8072883163356273, w1=-1.684326719035577\n",
      "Logistic regression iter. 11/29: loss=-5.498143830654852, w0=-1.9488905360823292, w1=-1.8156031475449825\n",
      "Logistic regression iter. 12/29: loss=-5.765675780352633, w0=-2.090491842466471, w1=-1.9468790406596677\n",
      "Logistic regression iter. 13/29: loss=-6.096487716912603, w0=-2.2320927536116857, w1=-2.078154708685561\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-2.3736934919484476, w1=-2.2094302811220796\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-2.5152941539952196, w1=-2.34070581258404\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-2.6568947820552373, w1=-2.4719813263292165\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-2.798495394844477, w1=-2.603256832351598\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-2.9400960007167702, w1=-2.7345323349818997\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-3.0816966034320195, w1=-2.8658078361115935\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-3.2232972046958692, w1=-2.997083336572933\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-3.364897805287868, w1=-3.1283588367346806\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-3.5064984055668327, w1=-3.259634336761315\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-3.6480990056990397, w1=-3.39090983672666\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-3.789699605762036, w1=-3.5221853366640494\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-3.9313002057922084, w1=-3.6534608365886196\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-4.07290080580673, w1=-3.784736336507282\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-4.214501405813753, w1=-3.9160118364232086\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-4.356102005817163, w1=-4.047287336337862\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-4.497702605818826, w1=-4.1785628362519205\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-4291.7324932321535, w0=-0.36659879999999995, w1=-0.3521980224339376\n",
      "Logistic regression iter. 1/29: loss=-1466.8558768720384, w0=-0.5241301841637549, w1=-0.49630080129807114\n",
      "Logistic regression iter. 2/29: loss=-546.946926690037, w0=-0.6711815667476522, w1=-0.6317486058253767\n",
      "Logistic regression iter. 3/29: loss=-213.41765733364005, w0=-0.8147975662922988, w1=-0.7645050542533637\n",
      "Logistic regression iter. 4/29: loss=-86.58050009827389, w0=-0.9571727024205381, w1=-0.8963295767394698\n",
      "Logistic regression iter. 5/29: loss=-37.047691602170644, w0=-1.0990787083251345, w1=-1.027814501976157\n",
      "Logistic regression iter. 6/29: loss=-17.420764532192354, w0=-1.240802179650174, w1=-1.1591716549611217\n",
      "Logistic regression iter. 7/29: loss=-9.639555427619326, w0=-1.382453066448611, w1=-1.2904795757255196\n",
      "Logistic regression iter. 8/29: loss=-6.639471380394017, w0=-1.5240745609684445, w1=-1.4217681534282034\n",
      "Logistic regression iter. 9/29: loss=-5.600428746098675, w0=-1.6656839623219122, w1=-1.5530490030122777\n",
      "Logistic regression iter. 10/29: loss=-5.377322711997231, w0=-1.8072883163356273, w1=-1.684326719035577\n",
      "Logistic regression iter. 11/29: loss=-5.498143830654852, w0=-1.9488905360823292, w1=-1.8156031475449825\n",
      "Logistic regression iter. 12/29: loss=-5.765675780352633, w0=-2.090491842466471, w1=-1.9468790406596677\n",
      "Logistic regression iter. 13/29: loss=-6.096487716912603, w0=-2.2320927536116857, w1=-2.078154708685561\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-2.3736934919484476, w1=-2.2094302811220796\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-2.5152941539952196, w1=-2.34070581258404\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-2.6568947820552373, w1=-2.4719813263292165\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-2.798495394844477, w1=-2.603256832351598\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-2.9400960007167702, w1=-2.7345323349818997\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-3.0816966034320195, w1=-2.8658078361115935\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-3.2232972046958692, w1=-2.997083336572933\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-3.364897805287868, w1=-3.1283588367346806\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-3.5064984055668327, w1=-3.259634336761315\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-3.6480990056990397, w1=-3.39090983672666\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-3.789699605762036, w1=-3.5221853366640494\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-3.9313002057922084, w1=-3.6534608365886196\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-4.07290080580673, w1=-3.784736336507282\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-4.214501405813753, w1=-3.9160118364232086\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-4.356102005817163, w1=-4.047287336337862\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-4.497702605818826, w1=-4.1785628362519205\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-4291.7324932321535, w0=-0.36659879999999995, w1=-0.3521980224339376\n",
      "Logistic regression iter. 1/29: loss=-1466.8558768720384, w0=-0.5241301841637549, w1=-0.49630080129807114\n",
      "Logistic regression iter. 2/29: loss=-546.946926690037, w0=-0.6711815667476522, w1=-0.6317486058253767\n",
      "Logistic regression iter. 3/29: loss=-213.41765733364005, w0=-0.8147975662922988, w1=-0.7645050542533637\n",
      "Logistic regression iter. 4/29: loss=-86.58050009827389, w0=-0.9571727024205381, w1=-0.8963295767394698\n",
      "Logistic regression iter. 5/29: loss=-37.047691602170644, w0=-1.0990787083251345, w1=-1.027814501976157\n",
      "Logistic regression iter. 6/29: loss=-17.420764532192354, w0=-1.240802179650174, w1=-1.1591716549611217\n",
      "Logistic regression iter. 7/29: loss=-9.639555427619326, w0=-1.382453066448611, w1=-1.2904795757255196\n",
      "Logistic regression iter. 8/29: loss=-6.639471380394017, w0=-1.5240745609684445, w1=-1.4217681534282034\n",
      "Logistic regression iter. 9/29: loss=-5.600428746098675, w0=-1.6656839623219122, w1=-1.5530490030122777\n",
      "Logistic regression iter. 10/29: loss=-5.377322711997231, w0=-1.8072883163356273, w1=-1.684326719035577\n",
      "Logistic regression iter. 11/29: loss=-5.498143830654852, w0=-1.9488905360823292, w1=-1.8156031475449825\n",
      "Logistic regression iter. 12/29: loss=-5.765675780352633, w0=-2.090491842466471, w1=-1.9468790406596677\n",
      "Logistic regression iter. 13/29: loss=-6.096487716912603, w0=-2.2320927536116857, w1=-2.078154708685561\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-2.3736934919484476, w1=-2.2094302811220796\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-2.5152941539952196, w1=-2.34070581258404\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-2.6568947820552373, w1=-2.4719813263292165\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-2.798495394844477, w1=-2.603256832351598\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-2.9400960007167702, w1=-2.7345323349818997\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-3.0816966034320195, w1=-2.8658078361115935\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-3.2232972046958692, w1=-2.997083336572933\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-3.364897805287868, w1=-3.1283588367346806\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-3.5064984055668327, w1=-3.259634336761315\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-3.6480990056990397, w1=-3.39090983672666\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-3.789699605762036, w1=-3.5221853366640494\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-3.9313002057922084, w1=-3.6534608365886196\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-4.07290080580673, w1=-3.784736336507282\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-4.214501405813753, w1=-3.9160118364232086\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-4.356102005817163, w1=-4.047287336337862\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-4.497702605818826, w1=-4.1785628362519205\n",
      "4\n",
      "4,5\n",
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-4291.7324932321535, w0=-0.36659879999999995, w1=-0.3521980224339376\n",
      "Logistic regression iter. 1/29: loss=-1466.8558768720384, w0=-0.5241301841637549, w1=-0.49630080129807114\n",
      "Logistic regression iter. 2/29: loss=-546.946926690037, w0=-0.6711815667476522, w1=-0.6317486058253767\n",
      "Logistic regression iter. 3/29: loss=-213.41765733364005, w0=-0.8147975662922988, w1=-0.7645050542533637\n",
      "Logistic regression iter. 4/29: loss=-86.58050009827389, w0=-0.9571727024205381, w1=-0.8963295767394698\n",
      "Logistic regression iter. 5/29: loss=-37.047691602170644, w0=-1.0990787083251345, w1=-1.027814501976157\n",
      "Logistic regression iter. 6/29: loss=-17.420764532192354, w0=-1.240802179650174, w1=-1.1591716549611217\n",
      "Logistic regression iter. 7/29: loss=-9.639555427619326, w0=-1.382453066448611, w1=-1.2904795757255196\n",
      "Logistic regression iter. 8/29: loss=-6.639471380394017, w0=-1.5240745609684445, w1=-1.4217681534282034\n",
      "Logistic regression iter. 9/29: loss=-5.600428746098675, w0=-1.6656839623219122, w1=-1.5530490030122777\n",
      "Logistic regression iter. 10/29: loss=-5.377322711997231, w0=-1.8072883163356273, w1=-1.684326719035577\n",
      "Logistic regression iter. 11/29: loss=-5.498143830654852, w0=-1.9488905360823292, w1=-1.8156031475449825\n",
      "Logistic regression iter. 12/29: loss=-5.765675780352633, w0=-2.090491842466471, w1=-1.9468790406596677\n",
      "Logistic regression iter. 13/29: loss=-6.096487716912603, w0=-2.2320927536116857, w1=-2.078154708685561\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-2.3736934919484476, w1=-2.2094302811220796\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-2.5152941539952196, w1=-2.34070581258404\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-2.6568947820552373, w1=-2.4719813263292165\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-2.798495394844477, w1=-2.603256832351598\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-2.9400960007167702, w1=-2.7345323349818997\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-3.0816966034320195, w1=-2.8658078361115935\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-3.2232972046958692, w1=-2.997083336572933\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-3.364897805287868, w1=-3.1283588367346806\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-3.5064984055668327, w1=-3.259634336761315\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-3.6480990056990397, w1=-3.39090983672666\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-3.789699605762036, w1=-3.5221853366640494\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-3.9313002057922084, w1=-3.6534608365886196\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-4.07290080580673, w1=-3.784736336507282\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-4.214501405813753, w1=-3.9160118364232086\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-4.356102005817163, w1=-4.047287336337862\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-4.497702605818826, w1=-4.1785628362519205\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-3814.730709723048, w0=-0.3828920799999999, w1=-0.3678512678754459\n",
      "Logistic regression iter. 1/29: loss=-1263.134899125624, w0=-0.5455632603045257, w1=-0.5167854083427594\n",
      "Logistic regression iter. 2/29: loss=-455.37560087260425, w0=-0.6983489742185724, w1=-0.6576149083651526\n",
      "Logistic regression iter. 3/29: loss=-171.9866103348028, w0=-0.8479894896676884, w1=-0.7959989297263017\n",
      "Logistic regression iter. 4/29: loss=-67.88755132513812, w0=-0.9965314653815417, w1=-0.9335645471218454\n",
      "Logistic regression iter. 5/29: loss=-28.6661014304831, w0=-1.1446722150762438, w1=-1.0708422434523732\n",
      "Logistic regression iter. 6/29: loss=-13.711408872277696, w0=-1.2926621337577409, w1=-1.208015354287707\n",
      "Logistic regression iter. 7/29: loss=-8.051378727680966, w0=-1.4405940750461088, w1=-1.3451495437515242\n",
      "Logistic regression iter. 8/29: loss=-6.020534113465753, w0=-1.5885033078303084, w1=-1.4822689558264759\n",
      "Logistic regression iter. 9/29: loss=-5.430503934778249, w0=-1.7364034976382625, w1=-1.6193826595156295\n",
      "Logistic regression iter. 10/29: loss=-5.421055398437474, w0=-1.884300032236192, w1=-1.7564941241502006\n",
      "Logistic regression iter. 11/29: loss=-5.64874195625345, w0=-2.032195069145103, w1=-1.8936046984194614\n",
      "Logistic regression iter. 12/29: loss=-5.974499265913611, w0=-2.180089484661301, w1=-2.0307149142076035\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-2.3279836393560385, w1=-2.1678249840156867\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-2.4758776833871177, w1=-2.3049349937540153\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-2.623771679988945, w1=-2.4420449785340086\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-2.7716656560698505, w1=-2.579154952850411\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-2.919559623192678, w1=-2.716264922743003\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.067453586372008, w1=-2.8533748907504766\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-3.2153475478015006, w1=-2.990484857948651\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-3.363241508448682, w1=-3.127594824796937\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-3.5111354687436087, w1=-3.264704791492944\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-3.6590294288788465, w1=-3.401814758122255\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-3.806923388941226, w1=-3.538924724722178\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-3.9548173489701606, w1=-3.676034691309077\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-4.102711308983653, w1=-3.813144657890173\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-4.250605268989976, w1=-3.950254624468669\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-4.398499228992954, w1=-4.087364591045994\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-4.546393188994362, w1=-4.2244745576227904\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-4.69428714899503, w1=-4.361584524199347\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-3814.730709723048, w0=-0.3828920799999999, w1=-0.3678512678754459\n",
      "Logistic regression iter. 1/29: loss=-1263.134899125624, w0=-0.5455632603045257, w1=-0.5167854083427594\n",
      "Logistic regression iter. 2/29: loss=-455.37560087260425, w0=-0.6983489742185724, w1=-0.6576149083651526\n",
      "Logistic regression iter. 3/29: loss=-171.9866103348028, w0=-0.8479894896676884, w1=-0.7959989297263017\n",
      "Logistic regression iter. 4/29: loss=-67.88755132513812, w0=-0.9965314653815417, w1=-0.9335645471218454\n",
      "Logistic regression iter. 5/29: loss=-28.6661014304831, w0=-1.1446722150762438, w1=-1.0708422434523732\n",
      "Logistic regression iter. 6/29: loss=-13.711408872277696, w0=-1.2926621337577409, w1=-1.208015354287707\n",
      "Logistic regression iter. 7/29: loss=-8.051378727680966, w0=-1.4405940750461088, w1=-1.3451495437515242\n",
      "Logistic regression iter. 8/29: loss=-6.020534113465753, w0=-1.5885033078303084, w1=-1.4822689558264759\n",
      "Logistic regression iter. 9/29: loss=-5.430503934778249, w0=-1.7364034976382625, w1=-1.6193826595156295\n",
      "Logistic regression iter. 10/29: loss=-5.421055398437474, w0=-1.884300032236192, w1=-1.7564941241502006\n",
      "Logistic regression iter. 11/29: loss=-5.64874195625345, w0=-2.032195069145103, w1=-1.8936046984194614\n",
      "Logistic regression iter. 12/29: loss=-5.974499265913611, w0=-2.180089484661301, w1=-2.0307149142076035\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-2.3279836393560385, w1=-2.1678249840156867\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-2.4758776833871177, w1=-2.3049349937540153\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-2.623771679988945, w1=-2.4420449785340086\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-2.7716656560698505, w1=-2.579154952850411\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-2.919559623192678, w1=-2.716264922743003\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.067453586372008, w1=-2.8533748907504766\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-3.2153475478015006, w1=-2.990484857948651\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-3.363241508448682, w1=-3.127594824796937\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-3.5111354687436087, w1=-3.264704791492944\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-3.6590294288788465, w1=-3.401814758122255\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-3.806923388941226, w1=-3.538924724722178\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-3.9548173489701606, w1=-3.676034691309077\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-4.102711308983653, w1=-3.813144657890173\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-4.250605268989976, w1=-3.950254624468669\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-4.398499228992954, w1=-4.087364591045994\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-4.546393188994362, w1=-4.2244745576227904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 29/29: loss=nan, w0=-4.69428714899503, w1=-4.361584524199347\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-3814.730709723048, w0=-0.3828920799999999, w1=-0.3678512678754459\n",
      "Logistic regression iter. 1/29: loss=-1263.134899125624, w0=-0.5455632603045257, w1=-0.5167854083427594\n",
      "Logistic regression iter. 2/29: loss=-455.37560087260425, w0=-0.6983489742185724, w1=-0.6576149083651526\n",
      "Logistic regression iter. 3/29: loss=-171.9866103348028, w0=-0.8479894896676884, w1=-0.7959989297263017\n",
      "Logistic regression iter. 4/29: loss=-67.88755132513812, w0=-0.9965314653815417, w1=-0.9335645471218454\n",
      "Logistic regression iter. 5/29: loss=-28.6661014304831, w0=-1.1446722150762438, w1=-1.0708422434523732\n",
      "Logistic regression iter. 6/29: loss=-13.711408872277696, w0=-1.2926621337577409, w1=-1.208015354287707\n",
      "Logistic regression iter. 7/29: loss=-8.051378727680966, w0=-1.4405940750461088, w1=-1.3451495437515242\n",
      "Logistic regression iter. 8/29: loss=-6.020534113465753, w0=-1.5885033078303084, w1=-1.4822689558264759\n",
      "Logistic regression iter. 9/29: loss=-5.430503934778249, w0=-1.7364034976382625, w1=-1.6193826595156295\n",
      "Logistic regression iter. 10/29: loss=-5.421055398437474, w0=-1.884300032236192, w1=-1.7564941241502006\n",
      "Logistic regression iter. 11/29: loss=-5.64874195625345, w0=-2.032195069145103, w1=-1.8936046984194614\n",
      "Logistic regression iter. 12/29: loss=-5.974499265913611, w0=-2.180089484661301, w1=-2.0307149142076035\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-2.3279836393560385, w1=-2.1678249840156867\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-2.4758776833871177, w1=-2.3049349937540153\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-2.623771679988945, w1=-2.4420449785340086\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-2.7716656560698505, w1=-2.579154952850411\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-2.919559623192678, w1=-2.716264922743003\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.067453586372008, w1=-2.8533748907504766\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-3.2153475478015006, w1=-2.990484857948651\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-3.363241508448682, w1=-3.127594824796937\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-3.5111354687436087, w1=-3.264704791492944\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-3.6590294288788465, w1=-3.401814758122255\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-3.806923388941226, w1=-3.538924724722178\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-3.9548173489701606, w1=-3.676034691309077\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-4.102711308983653, w1=-3.813144657890173\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-4.250605268989976, w1=-3.950254624468669\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-4.398499228992954, w1=-4.087364591045994\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-4.546393188994362, w1=-4.2244745576227904\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-4.69428714899503, w1=-4.361584524199347\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-3814.730709723048, w0=-0.3828920799999999, w1=-0.3678512678754459\n",
      "Logistic regression iter. 1/29: loss=-1263.134899125624, w0=-0.5455632603045257, w1=-0.5167854083427594\n",
      "Logistic regression iter. 2/29: loss=-455.37560087260425, w0=-0.6983489742185724, w1=-0.6576149083651526\n",
      "Logistic regression iter. 3/29: loss=-171.9866103348028, w0=-0.8479894896676884, w1=-0.7959989297263017\n",
      "Logistic regression iter. 4/29: loss=-67.88755132513812, w0=-0.9965314653815417, w1=-0.9335645471218454\n",
      "Logistic regression iter. 5/29: loss=-28.6661014304831, w0=-1.1446722150762438, w1=-1.0708422434523732\n",
      "Logistic regression iter. 6/29: loss=-13.711408872277696, w0=-1.2926621337577409, w1=-1.208015354287707\n",
      "Logistic regression iter. 7/29: loss=-8.051378727680966, w0=-1.4405940750461088, w1=-1.3451495437515242\n",
      "Logistic regression iter. 8/29: loss=-6.020534113465753, w0=-1.5885033078303084, w1=-1.4822689558264759\n",
      "Logistic regression iter. 9/29: loss=-5.430503934778249, w0=-1.7364034976382625, w1=-1.6193826595156295\n",
      "Logistic regression iter. 10/29: loss=-5.421055398437474, w0=-1.884300032236192, w1=-1.7564941241502006\n",
      "Logistic regression iter. 11/29: loss=-5.64874195625345, w0=-2.032195069145103, w1=-1.8936046984194614\n",
      "Logistic regression iter. 12/29: loss=-5.974499265913611, w0=-2.180089484661301, w1=-2.0307149142076035\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-2.3279836393560385, w1=-2.1678249840156867\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-2.4758776833871177, w1=-2.3049349937540153\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-2.623771679988945, w1=-2.4420449785340086\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-2.7716656560698505, w1=-2.579154952850411\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-2.919559623192678, w1=-2.716264922743003\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.067453586372008, w1=-2.8533748907504766\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-3.2153475478015006, w1=-2.990484857948651\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-3.363241508448682, w1=-3.127594824796937\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-3.5111354687436087, w1=-3.264704791492944\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-3.6590294288788465, w1=-3.401814758122255\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-3.806923388941226, w1=-3.538924724722178\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-3.9548173489701606, w1=-3.676034691309077\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-4.102711308983653, w1=-3.813144657890173\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-4.250605268989976, w1=-3.950254624468669\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-4.398499228992954, w1=-4.087364591045994\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-4.546393188994362, w1=-4.2244745576227904\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-4.69428714899503, w1=-4.361584524199347\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-3392.8606527044876, w0=-0.39918535999999993, w1=-0.38350451331695423\n",
      "Logistic regression iter. 1/29: loss=-1088.189798886415, w0=-0.5670617686809916, w1=-0.5373359826820135\n",
      "Logistic regression iter. 2/29: loss=-379.35158018983185, w0=-0.7256327641818242, w1=-0.6835920129615626\n",
      "Logistic regression iter. 3/29: loss=-138.783448642865, w0=-0.8813314920721966, w1=-0.8276311045778163\n",
      "Logistic regression iter. 4/29: loss=-53.43837754839128, w0=-1.0360602731975679, w1=-0.9709532588677131\n",
      "Logistic regression iter. 5/29: loss=-22.42823562481772, w0=-1.1904468005595392, w1=-1.114031898025168\n",
      "Logistic regression iter. 6/29: loss=-11.068855439840455, w0=-1.3447089985769958, w1=-1.2570251260548433\n",
      "Logistic regression iter. 7/29: loss=-6.988769565186261, w0=-1.498924989210629, w1=-1.3999876480480302\n",
      "Logistic regression iter. 8/29: loss=-5.658916766654092, w0=-1.6531234694872339, w1=-1.5429389015454604\n",
      "Logistic regression iter. 9/29: loss=-5.386178634648697, w0=-1.8073151990053158, w1=-1.6858859453481803\n",
      "Logistic regression iter. 10/29: loss=-5.52609011870925, w0=-1.961504285185168, w1=-1.8288313914659353\n",
      "Logistic regression iter. 11/29: loss=-5.829366483289242, w0=-2.115692321564894, w1=-1.9717762225686362\n",
      "Logistic regression iter. 12/29: loss=-6.198162967743345, w0=-2.2698799355458035, w1=-2.114720813865382\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-2.4240673775056343, w1=-2.2576653105527753\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-2.5782547486186624, w1=-2.40060976950887\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-2.7324420902454367, w1=-2.5435542132659643\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-2.8866294194786377, w1=-2.6864986508432795\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.0408167434541244, w1=-2.8294430858859743\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.1950040651795697, w1=-2.972387519880566\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-3.349191385934133, w1=-3.1153319534383996\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-3.503378706266484, w1=-3.2582763868129003\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-3.6575660264138645, w1=-3.4012208201099123\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-3.811753346479641, w1=-3.5441652533739574\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-3.9659406665091756, w1=-3.687109686623889\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-4.1201279865225136, w1=-3.830054119867742\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-4.274315306528569, w1=-3.972998553108963\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-4.4285026265313325, w1=-4.1159429863490375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 27/29: loss=nan, w0=-4.582689946532599, w1=-4.258887419588611\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-4.736877266533183, w1=-4.4018318528279625\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-4.891064586533452, w1=-4.544776286067217\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-3392.8606527044876, w0=-0.39918535999999993, w1=-0.38350451331695423\n",
      "Logistic regression iter. 1/29: loss=-1088.189798886415, w0=-0.5670617686809916, w1=-0.5373359826820135\n",
      "Logistic regression iter. 2/29: loss=-379.35158018983185, w0=-0.7256327641818242, w1=-0.6835920129615626\n",
      "Logistic regression iter. 3/29: loss=-138.783448642865, w0=-0.8813314920721966, w1=-0.8276311045778163\n",
      "Logistic regression iter. 4/29: loss=-53.43837754839128, w0=-1.0360602731975679, w1=-0.9709532588677131\n",
      "Logistic regression iter. 5/29: loss=-22.42823562481772, w0=-1.1904468005595392, w1=-1.114031898025168\n",
      "Logistic regression iter. 6/29: loss=-11.068855439840455, w0=-1.3447089985769958, w1=-1.2570251260548433\n",
      "Logistic regression iter. 7/29: loss=-6.988769565186261, w0=-1.498924989210629, w1=-1.3999876480480302\n",
      "Logistic regression iter. 8/29: loss=-5.658916766654092, w0=-1.6531234694872339, w1=-1.5429389015454604\n",
      "Logistic regression iter. 9/29: loss=-5.386178634648697, w0=-1.8073151990053158, w1=-1.6858859453481803\n",
      "Logistic regression iter. 10/29: loss=-5.52609011870925, w0=-1.961504285185168, w1=-1.8288313914659353\n",
      "Logistic regression iter. 11/29: loss=-5.829366483289242, w0=-2.115692321564894, w1=-1.9717762225686362\n",
      "Logistic regression iter. 12/29: loss=-6.198162967743345, w0=-2.2698799355458035, w1=-2.114720813865382\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-2.4240673775056343, w1=-2.2576653105527753\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-2.5782547486186624, w1=-2.40060976950887\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-2.7324420902454367, w1=-2.5435542132659643\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-2.8866294194786377, w1=-2.6864986508432795\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.0408167434541244, w1=-2.8294430858859743\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.1950040651795697, w1=-2.972387519880566\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-3.349191385934133, w1=-3.1153319534383996\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-3.503378706266484, w1=-3.2582763868129003\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-3.6575660264138645, w1=-3.4012208201099123\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-3.811753346479641, w1=-3.5441652533739574\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-3.9659406665091756, w1=-3.687109686623889\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-4.1201279865225136, w1=-3.830054119867742\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-4.274315306528569, w1=-3.972998553108963\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-4.4285026265313325, w1=-4.1159429863490375\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-4.582689946532599, w1=-4.258887419588611\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-4.736877266533183, w1=-4.4018318528279625\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-4.891064586533452, w1=-4.544776286067217\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-3392.8606527044876, w0=-0.39918535999999993, w1=-0.38350451331695423\n",
      "Logistic regression iter. 1/29: loss=-1088.189798886415, w0=-0.5670617686809916, w1=-0.5373359826820135\n",
      "Logistic regression iter. 2/29: loss=-379.35158018983185, w0=-0.7256327641818242, w1=-0.6835920129615626\n",
      "Logistic regression iter. 3/29: loss=-138.783448642865, w0=-0.8813314920721966, w1=-0.8276311045778163\n",
      "Logistic regression iter. 4/29: loss=-53.43837754839128, w0=-1.0360602731975679, w1=-0.9709532588677131\n",
      "Logistic regression iter. 5/29: loss=-22.42823562481772, w0=-1.1904468005595392, w1=-1.114031898025168\n",
      "Logistic regression iter. 6/29: loss=-11.068855439840455, w0=-1.3447089985769958, w1=-1.2570251260548433\n",
      "Logistic regression iter. 7/29: loss=-6.988769565186261, w0=-1.498924989210629, w1=-1.3999876480480302\n",
      "Logistic regression iter. 8/29: loss=-5.658916766654092, w0=-1.6531234694872339, w1=-1.5429389015454604\n",
      "Logistic regression iter. 9/29: loss=-5.386178634648697, w0=-1.8073151990053158, w1=-1.6858859453481803\n",
      "Logistic regression iter. 10/29: loss=-5.52609011870925, w0=-1.961504285185168, w1=-1.8288313914659353\n",
      "Logistic regression iter. 11/29: loss=-5.829366483289242, w0=-2.115692321564894, w1=-1.9717762225686362\n",
      "Logistic regression iter. 12/29: loss=-6.198162967743345, w0=-2.2698799355458035, w1=-2.114720813865382\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-2.4240673775056343, w1=-2.2576653105527753\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-2.5782547486186624, w1=-2.40060976950887\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-2.7324420902454367, w1=-2.5435542132659643\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-2.8866294194786377, w1=-2.6864986508432795\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.0408167434541244, w1=-2.8294430858859743\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.1950040651795697, w1=-2.972387519880566\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-3.349191385934133, w1=-3.1153319534383996\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-3.503378706266484, w1=-3.2582763868129003\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-3.6575660264138645, w1=-3.4012208201099123\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-3.811753346479641, w1=-3.5441652533739574\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-3.9659406665091756, w1=-3.687109686623889\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-4.1201279865225136, w1=-3.830054119867742\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-4.274315306528569, w1=-3.972998553108963\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-4.4285026265313325, w1=-4.1159429863490375\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-4.582689946532599, w1=-4.258887419588611\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-4.736877266533183, w1=-4.4018318528279625\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-4.891064586533452, w1=-4.544776286067217\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-3392.8606527044876, w0=-0.39918535999999993, w1=-0.38350451331695423\n",
      "Logistic regression iter. 1/29: loss=-1088.189798886415, w0=-0.5670617686809916, w1=-0.5373359826820135\n",
      "Logistic regression iter. 2/29: loss=-379.35158018983185, w0=-0.7256327641818242, w1=-0.6835920129615626\n",
      "Logistic regression iter. 3/29: loss=-138.783448642865, w0=-0.8813314920721966, w1=-0.8276311045778163\n",
      "Logistic regression iter. 4/29: loss=-53.43837754839128, w0=-1.0360602731975679, w1=-0.9709532588677131\n",
      "Logistic regression iter. 5/29: loss=-22.42823562481772, w0=-1.1904468005595392, w1=-1.114031898025168\n",
      "Logistic regression iter. 6/29: loss=-11.068855439840455, w0=-1.3447089985769958, w1=-1.2570251260548433\n",
      "Logistic regression iter. 7/29: loss=-6.988769565186261, w0=-1.498924989210629, w1=-1.3999876480480302\n",
      "Logistic regression iter. 8/29: loss=-5.658916766654092, w0=-1.6531234694872339, w1=-1.5429389015454604\n",
      "Logistic regression iter. 9/29: loss=-5.386178634648697, w0=-1.8073151990053158, w1=-1.6858859453481803\n",
      "Logistic regression iter. 10/29: loss=-5.52609011870925, w0=-1.961504285185168, w1=-1.8288313914659353\n",
      "Logistic regression iter. 11/29: loss=-5.829366483289242, w0=-2.115692321564894, w1=-1.9717762225686362\n",
      "Logistic regression iter. 12/29: loss=-6.198162967743345, w0=-2.2698799355458035, w1=-2.114720813865382\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-2.4240673775056343, w1=-2.2576653105527753\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-2.5782547486186624, w1=-2.40060976950887\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-2.7324420902454367, w1=-2.5435542132659643\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-2.8866294194786377, w1=-2.6864986508432795\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.0408167434541244, w1=-2.8294430858859743\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.1950040651795697, w1=-2.972387519880566\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-3.349191385934133, w1=-3.1153319534383996\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-3.503378706266484, w1=-3.2582763868129003\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-3.6575660264138645, w1=-3.4012208201099123\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-3.811753346479641, w1=-3.5441652533739574\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-3.9659406665091756, w1=-3.687109686623889\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-4.1201279865225136, w1=-3.830054119867742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 25/29: loss=nan, w0=-4.274315306528569, w1=-3.972998553108963\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-4.4285026265313325, w1=-4.1159429863490375\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-4.582689946532599, w1=-4.258887419588611\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-4.736877266533183, w1=-4.4018318528279625\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-4.891064586533452, w1=-4.544776286067217\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-3019.497095619333, w0=-0.4154786399999999, w1=-0.39915775875846254\n",
      "Logistic regression iter. 1/29: loss=-937.8935168592527, w0=-0.5886255809766131, w1=-0.5579507837192194\n",
      "Logistic regression iter. 2/29: loss=-316.2175583166667, w0=-0.7530295206791574, w1=-0.7096746868859556\n",
      "Logistic regression iter. 3/29: loss=-112.1680596267255, w0=-0.9148164618292415, w1=-0.8593930460017075\n",
      "Logistic regression iter. 4/29: loss=-42.269177721496305, w0=-1.0757490606899869, w1=-1.0084847524566498\n",
      "Logistic regression iter. 5/29: loss=-17.790403592598935, w0=-1.236390392600894, w1=-1.1573709478891194\n",
      "Logistic regression iter. 6/29: loss=-9.195786350319244, w0=-1.396929461677118, w1=-1.3061875317027334\n",
      "Logistic regression iter. 7/29: loss=-6.292822300619796, w0=-1.5574317750216293, w1=-1.4549799346857608\n",
      "Logistic regression iter. 8/29: loss=-5.471096125851257, w0=-1.7179206089093935, w1=-1.6037637585790456\n",
      "Logistic regression iter. 9/29: loss=-5.424076677208526, w0=-1.8784044103552493, w1=-1.7525444822553413\n",
      "Logistic regression iter. 10/29: loss=-5.670089590924666, w0=-2.038886302414814, w1=-1.901324067206898\n",
      "Logistic regression iter. 11/29: loss=-6.028577333090397, w0=-2.199367459269591, w1=-2.050103227733966\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.3598483291706156, w1=-2.1988822279564477\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-2.520329085657252, w1=-2.3476611668929155\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-2.6808097967917006, w1=-2.4964400821359236\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-2.841290489591652, w1=-2.6452189881236614\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.0017711749032925, w1=-2.7939978904610565\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.162251857126943, w1=-2.942776791345699\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.322732538065596, w1=-3.0915556916472657\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-3.4832132184649356, w1=-3.2403345917129354\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-3.643693898636091, w1=-3.389113491682445\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-3.8041745787099623, w1=-3.5378923916124756\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-3.9646552587420563, w1=-3.686671291526187\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-4.125135938756086, w1=-3.83545019143311\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-4.285616618762257, w1=-3.9842290913371907\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-4.446097298764984, w1=-4.133007991240075\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-4.606577978766197, w1=-4.281786891142453\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-4.767058658766738, w1=-4.430565791044614\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-4.92753933876698, w1=-4.579344690946684\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-5.088020018767089, w1=-4.728123590848714\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-3019.497095619333, w0=-0.4154786399999999, w1=-0.39915775875846254\n",
      "Logistic regression iter. 1/29: loss=-937.8935168592527, w0=-0.5886255809766131, w1=-0.5579507837192194\n",
      "Logistic regression iter. 2/29: loss=-316.2175583166667, w0=-0.7530295206791574, w1=-0.7096746868859556\n",
      "Logistic regression iter. 3/29: loss=-112.1680596267255, w0=-0.9148164618292415, w1=-0.8593930460017075\n",
      "Logistic regression iter. 4/29: loss=-42.269177721496305, w0=-1.0757490606899869, w1=-1.0084847524566498\n",
      "Logistic regression iter. 5/29: loss=-17.790403592598935, w0=-1.236390392600894, w1=-1.1573709478891194\n",
      "Logistic regression iter. 6/29: loss=-9.195786350319244, w0=-1.396929461677118, w1=-1.3061875317027334\n",
      "Logistic regression iter. 7/29: loss=-6.292822300619796, w0=-1.5574317750216293, w1=-1.4549799346857608\n",
      "Logistic regression iter. 8/29: loss=-5.471096125851257, w0=-1.7179206089093935, w1=-1.6037637585790456\n",
      "Logistic regression iter. 9/29: loss=-5.424076677208526, w0=-1.8784044103552493, w1=-1.7525444822553413\n",
      "Logistic regression iter. 10/29: loss=-5.670089590924666, w0=-2.038886302414814, w1=-1.901324067206898\n",
      "Logistic regression iter. 11/29: loss=-6.028577333090397, w0=-2.199367459269591, w1=-2.050103227733966\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.3598483291706156, w1=-2.1988822279564477\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-2.520329085657252, w1=-2.3476611668929155\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-2.6808097967917006, w1=-2.4964400821359236\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-2.841290489591652, w1=-2.6452189881236614\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.0017711749032925, w1=-2.7939978904610565\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.162251857126943, w1=-2.942776791345699\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.322732538065596, w1=-3.0915556916472657\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-3.4832132184649356, w1=-3.2403345917129354\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-3.643693898636091, w1=-3.389113491682445\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-3.8041745787099623, w1=-3.5378923916124756\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-3.9646552587420563, w1=-3.686671291526187\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-4.125135938756086, w1=-3.83545019143311\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-4.285616618762257, w1=-3.9842290913371907\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-4.446097298764984, w1=-4.133007991240075\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-4.606577978766197, w1=-4.281786891142453\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-4.767058658766738, w1=-4.430565791044614\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-4.92753933876698, w1=-4.579344690946684\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-5.088020018767089, w1=-4.728123590848714\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-3019.497095619333, w0=-0.4154786399999999, w1=-0.39915775875846254\n",
      "Logistic regression iter. 1/29: loss=-937.8935168592527, w0=-0.5886255809766131, w1=-0.5579507837192194\n",
      "Logistic regression iter. 2/29: loss=-316.2175583166667, w0=-0.7530295206791574, w1=-0.7096746868859556\n",
      "Logistic regression iter. 3/29: loss=-112.1680596267255, w0=-0.9148164618292415, w1=-0.8593930460017075\n",
      "Logistic regression iter. 4/29: loss=-42.269177721496305, w0=-1.0757490606899869, w1=-1.0084847524566498\n",
      "Logistic regression iter. 5/29: loss=-17.790403592598935, w0=-1.236390392600894, w1=-1.1573709478891194\n",
      "Logistic regression iter. 6/29: loss=-9.195786350319244, w0=-1.396929461677118, w1=-1.3061875317027334\n",
      "Logistic regression iter. 7/29: loss=-6.292822300619796, w0=-1.5574317750216293, w1=-1.4549799346857608\n",
      "Logistic regression iter. 8/29: loss=-5.471096125851257, w0=-1.7179206089093935, w1=-1.6037637585790456\n",
      "Logistic regression iter. 9/29: loss=-5.424076677208526, w0=-1.8784044103552493, w1=-1.7525444822553413\n",
      "Logistic regression iter. 10/29: loss=-5.670089590924666, w0=-2.038886302414814, w1=-1.901324067206898\n",
      "Logistic regression iter. 11/29: loss=-6.028577333090397, w0=-2.199367459269591, w1=-2.050103227733966\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.3598483291706156, w1=-2.1988822279564477\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-2.520329085657252, w1=-2.3476611668929155\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-2.6808097967917006, w1=-2.4964400821359236\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-2.841290489591652, w1=-2.6452189881236614\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.0017711749032925, w1=-2.7939978904610565\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.162251857126943, w1=-2.942776791345699\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.322732538065596, w1=-3.0915556916472657\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-3.4832132184649356, w1=-3.2403345917129354\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-3.643693898636091, w1=-3.389113491682445\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-3.8041745787099623, w1=-3.5378923916124756\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-3.9646552587420563, w1=-3.686671291526187\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-4.125135938756086, w1=-3.83545019143311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 24/29: loss=nan, w0=-4.285616618762257, w1=-3.9842290913371907\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-4.446097298764984, w1=-4.133007991240075\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-4.606577978766197, w1=-4.281786891142453\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-4.767058658766738, w1=-4.430565791044614\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-4.92753933876698, w1=-4.579344690946684\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-5.088020018767089, w1=-4.728123590848714\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-3019.497095619333, w0=-0.4154786399999999, w1=-0.39915775875846254\n",
      "Logistic regression iter. 1/29: loss=-937.8935168592527, w0=-0.5886255809766131, w1=-0.5579507837192194\n",
      "Logistic regression iter. 2/29: loss=-316.2175583166667, w0=-0.7530295206791574, w1=-0.7096746868859556\n",
      "Logistic regression iter. 3/29: loss=-112.1680596267255, w0=-0.9148164618292415, w1=-0.8593930460017075\n",
      "Logistic regression iter. 4/29: loss=-42.269177721496305, w0=-1.0757490606899869, w1=-1.0084847524566498\n",
      "Logistic regression iter. 5/29: loss=-17.790403592598935, w0=-1.236390392600894, w1=-1.1573709478891194\n",
      "Logistic regression iter. 6/29: loss=-9.195786350319244, w0=-1.396929461677118, w1=-1.3061875317027334\n",
      "Logistic regression iter. 7/29: loss=-6.292822300619796, w0=-1.5574317750216293, w1=-1.4549799346857608\n",
      "Logistic regression iter. 8/29: loss=-5.471096125851257, w0=-1.7179206089093935, w1=-1.6037637585790456\n",
      "Logistic regression iter. 9/29: loss=-5.424076677208526, w0=-1.8784044103552493, w1=-1.7525444822553413\n",
      "Logistic regression iter. 10/29: loss=-5.670089590924666, w0=-2.038886302414814, w1=-1.901324067206898\n",
      "Logistic regression iter. 11/29: loss=-6.028577333090397, w0=-2.199367459269591, w1=-2.050103227733966\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.3598483291706156, w1=-2.1988822279564477\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-2.520329085657252, w1=-2.3476611668929155\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-2.6808097967917006, w1=-2.4964400821359236\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-2.841290489591652, w1=-2.6452189881236614\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.0017711749032925, w1=-2.7939978904610565\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.162251857126943, w1=-2.942776791345699\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.322732538065596, w1=-3.0915556916472657\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-3.4832132184649356, w1=-3.2403345917129354\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-3.643693898636091, w1=-3.389113491682445\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-3.8041745787099623, w1=-3.5378923916124756\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-3.9646552587420563, w1=-3.686671291526187\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-4.125135938756086, w1=-3.83545019143311\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-4.285616618762257, w1=-3.9842290913371907\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-4.446097298764984, w1=-4.133007991240075\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-4.606577978766197, w1=-4.281786891142453\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-4.767058658766738, w1=-4.430565791044614\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-4.92753933876698, w1=-4.579344690946684\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-5.088020018767089, w1=-4.728123590848714\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-2688.8420363479604, w0=-0.4317719199999999, w1=-0.4148110041999709\n",
      "Logistic regression iter. 1/29: loss=-808.7233279649091, w0=-0.6102536741916045, w1=-0.5786274829088907\n",
      "Logistic regression iter. 2/29: loss=-263.7742958521792, w0=-0.7805348362308988, w1=-0.7358571634270993\n",
      "Logistic regression iter. 3/29: loss=-90.82902027521673, w0=-0.9484365718377213, w1=-0.8912759946324473\n",
      "Logistic regression iter. 4/29: loss=-33.63624830584687, w0=-1.115587429971465, w1=-1.0461481873685954\n",
      "Logistic regression iter. 5/29: loss=-14.347674184215583, w0=-1.2824909296870384, w1=-1.2008472776381456\n",
      "Logistic regression iter. 6/29: loss=-7.878323225955464, w0=-1.4493104768198763, w1=-1.3554897338987\n",
      "Logistic regression iter. 7/29: loss=-5.853035830238069, w0=-1.6161008356227822, w1=-1.5101131779454493\n",
      "Logistic regression iter. 8/29: loss=-5.400878294864584, w0=-1.7828808328266526, w1=-1.6647300993739536\n",
      "Logistic regression iter. 9/29: loss=-5.515938583078162, w0=-1.9496570829616195, w1=-1.8193447402940692\n",
      "Logistic regression iter. 10/29: loss=-5.838948490450562, w0=-2.1164319551499875, w1=-1.9739585703715672\n",
      "Logistic regression iter. 11/29: loss=-6.239364258946694, w0=-2.2832063127990123, w1=-2.1285721077666833\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.4499804755890384, w1=-2.283185538058239\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-2.616754563616508, w1=-2.4377989286625725\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-2.783528622609444, w1=-2.5924123043901917\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-2.9503026701981865, w1=-2.747025674481372\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.1170767132598014, w1=-2.901639042415674\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.2838507545062807, w1=-3.056252409516887\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.450624795018132, w1=-3.2108657762934936\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-3.617398835230023, w1=-3.365479142942573\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-3.7841728753184105, w1=-3.5200925095411586\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-3.9509469153555457, w1=-3.6747058761196034\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-4.1177209553712535, w1=-3.8293192426899583\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-4.28449499537794, w1=-3.9839326092570415\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-4.451269035380803, w1=-4.138545975822794\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-4.618043075382036, w1=-4.293159342388001\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-4.784817115382569, w1=-4.447772708952983\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-4.9515911553828005, w1=-4.602386075517873\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-5.118365195382902, w1=-4.756999442082724\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-5.285139235382946, w1=-4.911612808647559\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-2688.8420363479604, w0=-0.4317719199999999, w1=-0.4148110041999709\n",
      "Logistic regression iter. 1/29: loss=-808.7233279649091, w0=-0.6102536741916045, w1=-0.5786274829088907\n",
      "Logistic regression iter. 2/29: loss=-263.7742958521792, w0=-0.7805348362308988, w1=-0.7358571634270993\n",
      "Logistic regression iter. 3/29: loss=-90.82902027521673, w0=-0.9484365718377213, w1=-0.8912759946324473\n",
      "Logistic regression iter. 4/29: loss=-33.63624830584687, w0=-1.115587429971465, w1=-1.0461481873685954\n",
      "Logistic regression iter. 5/29: loss=-14.347674184215583, w0=-1.2824909296870384, w1=-1.2008472776381456\n",
      "Logistic regression iter. 6/29: loss=-7.878323225955464, w0=-1.4493104768198763, w1=-1.3554897338987\n",
      "Logistic regression iter. 7/29: loss=-5.853035830238069, w0=-1.6161008356227822, w1=-1.5101131779454493\n",
      "Logistic regression iter. 8/29: loss=-5.400878294864584, w0=-1.7828808328266526, w1=-1.6647300993739536\n",
      "Logistic regression iter. 9/29: loss=-5.515938583078162, w0=-1.9496570829616195, w1=-1.8193447402940692\n",
      "Logistic regression iter. 10/29: loss=-5.838948490450562, w0=-2.1164319551499875, w1=-1.9739585703715672\n",
      "Logistic regression iter. 11/29: loss=-6.239364258946694, w0=-2.2832063127990123, w1=-2.1285721077666833\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.4499804755890384, w1=-2.283185538058239\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-2.616754563616508, w1=-2.4377989286625725\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-2.783528622609444, w1=-2.5924123043901917\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-2.9503026701981865, w1=-2.747025674481372\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.1170767132598014, w1=-2.901639042415674\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.2838507545062807, w1=-3.056252409516887\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.450624795018132, w1=-3.2108657762934936\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-3.617398835230023, w1=-3.365479142942573\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-3.7841728753184105, w1=-3.5200925095411586\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-3.9509469153555457, w1=-3.6747058761196034\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-4.1177209553712535, w1=-3.8293192426899583\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-4.28449499537794, w1=-3.9839326092570415\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-4.451269035380803, w1=-4.138545975822794\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-4.618043075382036, w1=-4.293159342388001\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-4.784817115382569, w1=-4.447772708952983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 27/29: loss=nan, w0=-4.9515911553828005, w1=-4.602386075517873\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-5.118365195382902, w1=-4.756999442082724\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-5.285139235382946, w1=-4.911612808647559\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-2688.8420363479604, w0=-0.4317719199999999, w1=-0.4148110041999709\n",
      "Logistic regression iter. 1/29: loss=-808.7233279649091, w0=-0.6102536741916045, w1=-0.5786274829088907\n",
      "Logistic regression iter. 2/29: loss=-263.7742958521792, w0=-0.7805348362308988, w1=-0.7358571634270993\n",
      "Logistic regression iter. 3/29: loss=-90.82902027521673, w0=-0.9484365718377213, w1=-0.8912759946324473\n",
      "Logistic regression iter. 4/29: loss=-33.63624830584687, w0=-1.115587429971465, w1=-1.0461481873685954\n",
      "Logistic regression iter. 5/29: loss=-14.347674184215583, w0=-1.2824909296870384, w1=-1.2008472776381456\n",
      "Logistic regression iter. 6/29: loss=-7.878323225955464, w0=-1.4493104768198763, w1=-1.3554897338987\n",
      "Logistic regression iter. 7/29: loss=-5.853035830238069, w0=-1.6161008356227822, w1=-1.5101131779454493\n",
      "Logistic regression iter. 8/29: loss=-5.400878294864584, w0=-1.7828808328266526, w1=-1.6647300993739536\n",
      "Logistic regression iter. 9/29: loss=-5.515938583078162, w0=-1.9496570829616195, w1=-1.8193447402940692\n",
      "Logistic regression iter. 10/29: loss=-5.838948490450562, w0=-2.1164319551499875, w1=-1.9739585703715672\n",
      "Logistic regression iter. 11/29: loss=-6.239364258946694, w0=-2.2832063127990123, w1=-2.1285721077666833\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.4499804755890384, w1=-2.283185538058239\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-2.616754563616508, w1=-2.4377989286625725\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-2.783528622609444, w1=-2.5924123043901917\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-2.9503026701981865, w1=-2.747025674481372\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.1170767132598014, w1=-2.901639042415674\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.2838507545062807, w1=-3.056252409516887\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.450624795018132, w1=-3.2108657762934936\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-3.617398835230023, w1=-3.365479142942573\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-3.7841728753184105, w1=-3.5200925095411586\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-3.9509469153555457, w1=-3.6747058761196034\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-4.1177209553712535, w1=-3.8293192426899583\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-4.28449499537794, w1=-3.9839326092570415\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-4.451269035380803, w1=-4.138545975822794\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-4.618043075382036, w1=-4.293159342388001\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-4.784817115382569, w1=-4.447772708952983\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-4.9515911553828005, w1=-4.602386075517873\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-5.118365195382902, w1=-4.756999442082724\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-5.285139235382946, w1=-4.911612808647559\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-2688.8420363479604, w0=-0.4317719199999999, w1=-0.4148110041999709\n",
      "Logistic regression iter. 1/29: loss=-808.7233279649091, w0=-0.6102536741916045, w1=-0.5786274829088907\n",
      "Logistic regression iter. 2/29: loss=-263.7742958521792, w0=-0.7805348362308988, w1=-0.7358571634270993\n",
      "Logistic regression iter. 3/29: loss=-90.82902027521673, w0=-0.9484365718377213, w1=-0.8912759946324473\n",
      "Logistic regression iter. 4/29: loss=-33.63624830584687, w0=-1.115587429971465, w1=-1.0461481873685954\n",
      "Logistic regression iter. 5/29: loss=-14.347674184215583, w0=-1.2824909296870384, w1=-1.2008472776381456\n",
      "Logistic regression iter. 6/29: loss=-7.878323225955464, w0=-1.4493104768198763, w1=-1.3554897338987\n",
      "Logistic regression iter. 7/29: loss=-5.853035830238069, w0=-1.6161008356227822, w1=-1.5101131779454493\n",
      "Logistic regression iter. 8/29: loss=-5.400878294864584, w0=-1.7828808328266526, w1=-1.6647300993739536\n",
      "Logistic regression iter. 9/29: loss=-5.515938583078162, w0=-1.9496570829616195, w1=-1.8193447402940692\n",
      "Logistic regression iter. 10/29: loss=-5.838948490450562, w0=-2.1164319551499875, w1=-1.9739585703715672\n",
      "Logistic regression iter. 11/29: loss=-6.239364258946694, w0=-2.2832063127990123, w1=-2.1285721077666833\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.4499804755890384, w1=-2.283185538058239\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-2.616754563616508, w1=-2.4377989286625725\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-2.783528622609444, w1=-2.5924123043901917\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-2.9503026701981865, w1=-2.747025674481372\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.1170767132598014, w1=-2.901639042415674\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.2838507545062807, w1=-3.056252409516887\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.450624795018132, w1=-3.2108657762934936\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-3.617398835230023, w1=-3.365479142942573\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-3.7841728753184105, w1=-3.5200925095411586\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-3.9509469153555457, w1=-3.6747058761196034\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-4.1177209553712535, w1=-3.8293192426899583\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-4.28449499537794, w1=-3.9839326092570415\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-4.451269035380803, w1=-4.138545975822794\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-4.618043075382036, w1=-4.293159342388001\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-4.784817115382569, w1=-4.447772708952983\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-4.9515911553828005, w1=-4.602386075517873\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-5.118365195382902, w1=-4.756999442082724\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-5.285139235382946, w1=-4.911612808647559\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-2395.8176204455326, w0=-0.44806519999999994, w1=-0.4304642496414793\n",
      "Logistic regression iter. 1/29: loss=-697.6689464440361, w0=-0.6319443563267686, w1=-0.5993633499969382\n",
      "Logistic regression iter. 2/29: loss=-220.20041625389166, w0=-0.808143630950474, w1=-0.7621333889120161\n",
      "Logistic regression iter. 3/29: loss=-73.71726668994077, w0=-0.9821836124251933, w1=-0.9232712062462733\n",
      "Logistic regression iter. 4/29: loss=-26.965448684571424, w0=-1.1555649488102058, w1=-1.0839330446732869\n",
      "Logistic regression iter. 5/29: loss=-11.79825447447321, w0=-1.328736607214001, w1=-1.2444493296003454\n",
      "Logistic regression iter. 6/29: loss=-6.962458537453515, w0=-1.501839460343211, w1=-1.4049196106657464\n",
      "Logistic regression iter. 7/29: loss=-5.592466574912958, w0=-1.6749191682443452, w1=-1.565374963817539\n",
      "Logistic regression iter. 8/29: loss=-5.410391725005155, w0=-1.8479909209237515, w1=-1.7258253636604466\n",
      "Logistic regression iter. 9/29: loss=-5.643313886094641, w0=-2.021059886319319, w1=-1.886274087587322\n",
      "Logistic regression iter. 10/29: loss=-6.02373557818792, w0=-2.1941278580181636, w1=-2.0467222345842764\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.367195469786987, w1=-2.2071701798615186\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.540262949266663, w1=-2.3676180536061873\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-2.713330379462956, w1=-2.528065901654705\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-2.8863977910668375, w1=-2.688513740362088\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.059465195573855, w1=-2.848961575636102\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.232532597342028, w1=-3.009409409635135\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.405599998042266, w1=-3.169857243156129\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.578667398322042, w1=-3.330305076496261\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-3.7517347984347587, w1=-3.4907529097673806\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-3.9248021984805272, w1=-3.651200743011954\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-4.097869598499249, w1=-3.8116485762462378\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-4.27093699850696, w1=-3.9720964094765043\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-4.444004398510156, w1=-4.132544242705192\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-4.617071798511489, w1=-4.292992075933255\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-4.790139198512048, w1=-4.4534399091610695\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-4.963206598512283, w1=-4.613887742388783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 27/29: loss=nan, w0=-5.136273998512382, w1=-4.774335575616457\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-5.309341398512424, w1=-4.934783408844115\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-5.4824087985124415, w1=-5.095231242071766\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-2395.8176204455326, w0=-0.44806519999999994, w1=-0.4304642496414793\n",
      "Logistic regression iter. 1/29: loss=-697.6689464440361, w0=-0.6319443563267686, w1=-0.5993633499969382\n",
      "Logistic regression iter. 2/29: loss=-220.20041625389166, w0=-0.808143630950474, w1=-0.7621333889120161\n",
      "Logistic regression iter. 3/29: loss=-73.71726668994077, w0=-0.9821836124251933, w1=-0.9232712062462733\n",
      "Logistic regression iter. 4/29: loss=-26.965448684571424, w0=-1.1555649488102058, w1=-1.0839330446732869\n",
      "Logistic regression iter. 5/29: loss=-11.79825447447321, w0=-1.328736607214001, w1=-1.2444493296003454\n",
      "Logistic regression iter. 6/29: loss=-6.962458537453515, w0=-1.501839460343211, w1=-1.4049196106657464\n",
      "Logistic regression iter. 7/29: loss=-5.592466574912958, w0=-1.6749191682443452, w1=-1.565374963817539\n",
      "Logistic regression iter. 8/29: loss=-5.410391725005155, w0=-1.8479909209237515, w1=-1.7258253636604466\n",
      "Logistic regression iter. 9/29: loss=-5.643313886094641, w0=-2.021059886319319, w1=-1.886274087587322\n",
      "Logistic regression iter. 10/29: loss=-6.02373557818792, w0=-2.1941278580181636, w1=-2.0467222345842764\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.367195469786987, w1=-2.2071701798615186\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.540262949266663, w1=-2.3676180536061873\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-2.713330379462956, w1=-2.528065901654705\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-2.8863977910668375, w1=-2.688513740362088\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.059465195573855, w1=-2.848961575636102\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.232532597342028, w1=-3.009409409635135\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.405599998042266, w1=-3.169857243156129\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.578667398322042, w1=-3.330305076496261\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-3.7517347984347587, w1=-3.4907529097673806\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-3.9248021984805272, w1=-3.651200743011954\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-4.097869598499249, w1=-3.8116485762462378\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-4.27093699850696, w1=-3.9720964094765043\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-4.444004398510156, w1=-4.132544242705192\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-4.617071798511489, w1=-4.292992075933255\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-4.790139198512048, w1=-4.4534399091610695\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-4.963206598512283, w1=-4.613887742388783\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-5.136273998512382, w1=-4.774335575616457\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-5.309341398512424, w1=-4.934783408844115\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-5.4824087985124415, w1=-5.095231242071766\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-2395.8176204455326, w0=-0.44806519999999994, w1=-0.4304642496414793\n",
      "Logistic regression iter. 1/29: loss=-697.6689464440361, w0=-0.6319443563267686, w1=-0.5993633499969382\n",
      "Logistic regression iter. 2/29: loss=-220.20041625389166, w0=-0.808143630950474, w1=-0.7621333889120161\n",
      "Logistic regression iter. 3/29: loss=-73.71726668994077, w0=-0.9821836124251933, w1=-0.9232712062462733\n",
      "Logistic regression iter. 4/29: loss=-26.965448684571424, w0=-1.1555649488102058, w1=-1.0839330446732869\n",
      "Logistic regression iter. 5/29: loss=-11.79825447447321, w0=-1.328736607214001, w1=-1.2444493296003454\n",
      "Logistic regression iter. 6/29: loss=-6.962458537453515, w0=-1.501839460343211, w1=-1.4049196106657464\n",
      "Logistic regression iter. 7/29: loss=-5.592466574912958, w0=-1.6749191682443452, w1=-1.565374963817539\n",
      "Logistic regression iter. 8/29: loss=-5.410391725005155, w0=-1.8479909209237515, w1=-1.7258253636604466\n",
      "Logistic regression iter. 9/29: loss=-5.643313886094641, w0=-2.021059886319319, w1=-1.886274087587322\n",
      "Logistic regression iter. 10/29: loss=-6.02373557818792, w0=-2.1941278580181636, w1=-2.0467222345842764\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.367195469786987, w1=-2.2071701798615186\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.540262949266663, w1=-2.3676180536061873\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-2.713330379462956, w1=-2.528065901654705\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-2.8863977910668375, w1=-2.688513740362088\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.059465195573855, w1=-2.848961575636102\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.232532597342028, w1=-3.009409409635135\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.405599998042266, w1=-3.169857243156129\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.578667398322042, w1=-3.330305076496261\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-3.7517347984347587, w1=-3.4907529097673806\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-3.9248021984805272, w1=-3.651200743011954\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-4.097869598499249, w1=-3.8116485762462378\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-4.27093699850696, w1=-3.9720964094765043\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-4.444004398510156, w1=-4.132544242705192\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-4.617071798511489, w1=-4.292992075933255\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-4.790139198512048, w1=-4.4534399091610695\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-4.963206598512283, w1=-4.613887742388783\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-5.136273998512382, w1=-4.774335575616457\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-5.309341398512424, w1=-4.934783408844115\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-5.4824087985124415, w1=-5.095231242071766\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-2395.8176204455326, w0=-0.44806519999999994, w1=-0.4304642496414793\n",
      "Logistic regression iter. 1/29: loss=-697.6689464440361, w0=-0.6319443563267686, w1=-0.5993633499969382\n",
      "Logistic regression iter. 2/29: loss=-220.20041625389166, w0=-0.808143630950474, w1=-0.7621333889120161\n",
      "Logistic regression iter. 3/29: loss=-73.71726668994077, w0=-0.9821836124251933, w1=-0.9232712062462733\n",
      "Logistic regression iter. 4/29: loss=-26.965448684571424, w0=-1.1555649488102058, w1=-1.0839330446732869\n",
      "Logistic regression iter. 5/29: loss=-11.79825447447321, w0=-1.328736607214001, w1=-1.2444493296003454\n",
      "Logistic regression iter. 6/29: loss=-6.962458537453515, w0=-1.501839460343211, w1=-1.4049196106657464\n",
      "Logistic regression iter. 7/29: loss=-5.592466574912958, w0=-1.6749191682443452, w1=-1.565374963817539\n",
      "Logistic regression iter. 8/29: loss=-5.410391725005155, w0=-1.8479909209237515, w1=-1.7258253636604466\n",
      "Logistic regression iter. 9/29: loss=-5.643313886094641, w0=-2.021059886319319, w1=-1.886274087587322\n",
      "Logistic regression iter. 10/29: loss=-6.02373557818792, w0=-2.1941278580181636, w1=-2.0467222345842764\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.367195469786987, w1=-2.2071701798615186\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.540262949266663, w1=-2.3676180536061873\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-2.713330379462956, w1=-2.528065901654705\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-2.8863977910668375, w1=-2.688513740362088\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.059465195573855, w1=-2.848961575636102\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.232532597342028, w1=-3.009409409635135\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.405599998042266, w1=-3.169857243156129\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.578667398322042, w1=-3.330305076496261\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-3.7517347984347587, w1=-3.4907529097673806\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-3.9248021984805272, w1=-3.651200743011954\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-4.097869598499249, w1=-3.8116485762462378\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-4.27093699850696, w1=-3.9720964094765043\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-4.444004398510156, w1=-4.132544242705192\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-4.617071798511489, w1=-4.292992075933255\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-4.790139198512048, w1=-4.4534399091610695\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-4.963206598512283, w1=-4.613887742388783\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-5.136273998512382, w1=-4.774335575616457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 28/29: loss=nan, w0=-5.309341398512424, w1=-4.934783408844115\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-5.4824087985124415, w1=-5.095231242071766\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-2135.973372201124, w0=-0.4643584799999999, w1=-0.4461174950829876\n",
      "Logistic regression iter. 1/29: loss=-602.1557050939982, w0=-0.6536954485570934, w1=-0.6201553986012089\n",
      "Logistic regression iter. 2/29: loss=-183.98675408447198, w0=-0.8358504044444891, w1=-0.788497210246522\n",
      "Logistic regression iter. 3/29: loss=-59.99333479066212, w0=-1.01604924473189, w1=-0.9553701264941772\n",
      "Logistic regression iter. 4/29: loss=-21.81335352893208, w0=-1.1956713680465831, w1=-1.1218292640813237\n",
      "Logistic regression iter. 5/29: loss=-9.917069531920577, w0=-1.3751160486327514, w1=-1.2881662001707062\n",
      "Logistic regression iter. 6/29: loss=-6.337187121874132, w0=-1.5545044212390453, w1=-1.4544658184596937\n",
      "Logistic regression iter. 7/29: loss=-5.457466352678895, w0=-1.7338744635817196, w1=-1.620753729592471\n",
      "Logistic regression iter. 8/29: loss=-5.474069387599609, w0=-1.9132384044827746, w1=-1.7870378828801405\n",
      "Logistic regression iter. 9/29: loss=-5.794130158267567, w0=-2.0926002736838285, w1=-1.9533208055388567\n",
      "Logistic regression iter. 10/29: loss=-6.21877971610043, w0=-2.271961426686219, w1=-2.119603317948611\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.451322327991809, w1=-2.2858856913883714\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.6306831394946655, w1=-2.4521680170633986\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-2.810043918505335, w1=-2.6184503261015606\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-2.989404685605844, w1=-2.78473262927347\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.1687654482873313, w1=-2.951014930353256\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.3481262093105157, w1=-3.1172972306789646\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.527486969704742, w1=-3.2835795307301674\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.7068477298580245, w1=-3.449861830680507\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-3.8862084899181353, w1=-3.616144130593458\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-4.065569249941898, w1=-3.782426430492435\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-4.2449300099513625, w1=-3.9487087303861474\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-4.424290769955158, w1=-4.114991030277862\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-4.60365152995669, w1=-4.281273330168813\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-4.783012289957313, w1=-4.44755563005947\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-4.9623730499575665, w1=-4.613837929950013\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-5.14173380995767, w1=-4.780120229840512\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-5.321094569957713, w1=-4.946402529730993\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-5.50045532995773, w1=-5.112684829621468\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-5.679816089957737, w1=-5.2789671295119405\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-2135.973372201124, w0=-0.4643584799999999, w1=-0.4461174950829876\n",
      "Logistic regression iter. 1/29: loss=-602.1557050939982, w0=-0.6536954485570934, w1=-0.6201553986012089\n",
      "Logistic regression iter. 2/29: loss=-183.98675408447198, w0=-0.8358504044444891, w1=-0.788497210246522\n",
      "Logistic regression iter. 3/29: loss=-59.99333479066212, w0=-1.01604924473189, w1=-0.9553701264941772\n",
      "Logistic regression iter. 4/29: loss=-21.81335352893208, w0=-1.1956713680465831, w1=-1.1218292640813237\n",
      "Logistic regression iter. 5/29: loss=-9.917069531920577, w0=-1.3751160486327514, w1=-1.2881662001707062\n",
      "Logistic regression iter. 6/29: loss=-6.337187121874132, w0=-1.5545044212390453, w1=-1.4544658184596937\n",
      "Logistic regression iter. 7/29: loss=-5.457466352678895, w0=-1.7338744635817196, w1=-1.620753729592471\n",
      "Logistic regression iter. 8/29: loss=-5.474069387599609, w0=-1.9132384044827746, w1=-1.7870378828801405\n",
      "Logistic regression iter. 9/29: loss=-5.794130158267567, w0=-2.0926002736838285, w1=-1.9533208055388567\n",
      "Logistic regression iter. 10/29: loss=-6.21877971610043, w0=-2.271961426686219, w1=-2.119603317948611\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.451322327991809, w1=-2.2858856913883714\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.6306831394946655, w1=-2.4521680170633986\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-2.810043918505335, w1=-2.6184503261015606\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-2.989404685605844, w1=-2.78473262927347\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.1687654482873313, w1=-2.951014930353256\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.3481262093105157, w1=-3.1172972306789646\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.527486969704742, w1=-3.2835795307301674\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.7068477298580245, w1=-3.449861830680507\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-3.8862084899181353, w1=-3.616144130593458\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-4.065569249941898, w1=-3.782426430492435\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-4.2449300099513625, w1=-3.9487087303861474\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-4.424290769955158, w1=-4.114991030277862\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-4.60365152995669, w1=-4.281273330168813\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-4.783012289957313, w1=-4.44755563005947\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-4.9623730499575665, w1=-4.613837929950013\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-5.14173380995767, w1=-4.780120229840512\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-5.321094569957713, w1=-4.946402529730993\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-5.50045532995773, w1=-5.112684829621468\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-5.679816089957737, w1=-5.2789671295119405\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-2135.973372201124, w0=-0.4643584799999999, w1=-0.4461174950829876\n",
      "Logistic regression iter. 1/29: loss=-602.1557050939982, w0=-0.6536954485570934, w1=-0.6201553986012089\n",
      "Logistic regression iter. 2/29: loss=-183.98675408447198, w0=-0.8358504044444891, w1=-0.788497210246522\n",
      "Logistic regression iter. 3/29: loss=-59.99333479066212, w0=-1.01604924473189, w1=-0.9553701264941772\n",
      "Logistic regression iter. 4/29: loss=-21.81335352893208, w0=-1.1956713680465831, w1=-1.1218292640813237\n",
      "Logistic regression iter. 5/29: loss=-9.917069531920577, w0=-1.3751160486327514, w1=-1.2881662001707062\n",
      "Logistic regression iter. 6/29: loss=-6.337187121874132, w0=-1.5545044212390453, w1=-1.4544658184596937\n",
      "Logistic regression iter. 7/29: loss=-5.457466352678895, w0=-1.7338744635817196, w1=-1.620753729592471\n",
      "Logistic regression iter. 8/29: loss=-5.474069387599609, w0=-1.9132384044827746, w1=-1.7870378828801405\n",
      "Logistic regression iter. 9/29: loss=-5.794130158267567, w0=-2.0926002736838285, w1=-1.9533208055388567\n",
      "Logistic regression iter. 10/29: loss=-6.21877971610043, w0=-2.271961426686219, w1=-2.119603317948611\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.451322327991809, w1=-2.2858856913883714\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.6306831394946655, w1=-2.4521680170633986\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-2.810043918505335, w1=-2.6184503261015606\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-2.989404685605844, w1=-2.78473262927347\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.1687654482873313, w1=-2.951014930353256\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.3481262093105157, w1=-3.1172972306789646\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.527486969704742, w1=-3.2835795307301674\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.7068477298580245, w1=-3.449861830680507\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-3.8862084899181353, w1=-3.616144130593458\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-4.065569249941898, w1=-3.782426430492435\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-4.2449300099513625, w1=-3.9487087303861474\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-4.424290769955158, w1=-4.114991030277862\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-4.60365152995669, w1=-4.281273330168813\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-4.783012289957313, w1=-4.44755563005947\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-4.9623730499575665, w1=-4.613837929950013\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-5.14173380995767, w1=-4.780120229840512\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-5.321094569957713, w1=-4.946402529730993\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-5.50045532995773, w1=-5.112684829621468\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-5.679816089957737, w1=-5.2789671295119405\n",
      "4\n",
      "4,5\n",
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-2135.973372201124, w0=-0.4643584799999999, w1=-0.4461174950829876\n",
      "Logistic regression iter. 1/29: loss=-602.1557050939982, w0=-0.6536954485570934, w1=-0.6201553986012089\n",
      "Logistic regression iter. 2/29: loss=-183.98675408447198, w0=-0.8358504044444891, w1=-0.788497210246522\n",
      "Logistic regression iter. 3/29: loss=-59.99333479066212, w0=-1.01604924473189, w1=-0.9553701264941772\n",
      "Logistic regression iter. 4/29: loss=-21.81335352893208, w0=-1.1956713680465831, w1=-1.1218292640813237\n",
      "Logistic regression iter. 5/29: loss=-9.917069531920577, w0=-1.3751160486327514, w1=-1.2881662001707062\n",
      "Logistic regression iter. 6/29: loss=-6.337187121874132, w0=-1.5545044212390453, w1=-1.4544658184596937\n",
      "Logistic regression iter. 7/29: loss=-5.457466352678895, w0=-1.7338744635817196, w1=-1.620753729592471\n",
      "Logistic regression iter. 8/29: loss=-5.474069387599609, w0=-1.9132384044827746, w1=-1.7870378828801405\n",
      "Logistic regression iter. 9/29: loss=-5.794130158267567, w0=-2.0926002736838285, w1=-1.9533208055388567\n",
      "Logistic regression iter. 10/29: loss=-6.21877971610043, w0=-2.271961426686219, w1=-2.119603317948611\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.451322327991809, w1=-2.2858856913883714\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.6306831394946655, w1=-2.4521680170633986\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-2.810043918505335, w1=-2.6184503261015606\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-2.989404685605844, w1=-2.78473262927347\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.1687654482873313, w1=-2.951014930353256\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.3481262093105157, w1=-3.1172972306789646\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.527486969704742, w1=-3.2835795307301674\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.7068477298580245, w1=-3.449861830680507\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-3.8862084899181353, w1=-3.616144130593458\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-4.065569249941898, w1=-3.782426430492435\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-4.2449300099513625, w1=-3.9487087303861474\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-4.424290769955158, w1=-4.114991030277862\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-4.60365152995669, w1=-4.281273330168813\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-4.783012289957313, w1=-4.44755563005947\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-4.9623730499575665, w1=-4.613837929950013\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-5.14173380995767, w1=-4.780120229840512\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-5.321094569957713, w1=-4.946402529730993\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-5.50045532995773, w1=-5.112684829621468\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-5.679816089957737, w1=-5.2789671295119405\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-1905.4057747363252, w0=-0.48065175999999993, w1=-0.46177074052449596\n",
      "Logistic regression iter. 1/29: loss=-519.9799777447104, w0=-0.6755044314341414, w1=-0.6410004990519396\n",
      "Logistic regression iter. 2/29: loss=-153.8824063029005, w0=-0.8636494324998445, w1=-0.8149425149217839\n",
      "Logistic regression iter. 3/29: loss=-48.98530913635293, w0=-1.050025190115274, w1=-0.9875645132484097\n",
      "Logistic regression iter. 4/29: loss=-17.837354206958228, w0=-1.2358967754242587, w1=-1.1598273310610836\n",
      "Logistic regression iter. 5/29: loss=-8.536147534448803, w0=-1.4216184188957606, w1=-1.3319876922828577\n",
      "Logistic regression iter. 6/29: loss=-5.922419750265129, w0=-1.6072940410965597, w1=-1.5041178180041466\n",
      "Logistic regression iter. 7/29: loss=-5.410579404834833, w0=-1.7929551619427324, w1=-1.6762387717761258\n",
      "Logistic regression iter. 8/29: loss=-5.5746211185482055, w0=-1.9786116071439324, w1=-1.8483568770171872\n",
      "Logistic regression iter. 9/29: loss=-5.960470536333448, w0=-2.1642665134850785, w1=-2.0204740791888067\n",
      "Logistic regression iter. 10/29: loss=-6.420468931569391, w0=-2.3499209038403723, w1=-2.192590989771513\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.535575118217085, w1=-2.364707804640919\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.7212292716296385, w1=-2.5368245876192876\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-2.9068834036146143, w1=-2.70894135982527\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.092537527966342, w1=-2.8810581283462455\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.278191649564555, w1=-3.0531748955918103\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.463845770157805, w1=-3.2252916623910997\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.649499890380227, w1=-3.3974084290326303\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.835154010464401, w1=-3.569525195617855\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.020808130496537, w1=-3.7416419621828\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-4.206462250508906, w1=-3.9137587287403792\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-4.392116370513704, w1=-4.0858754952952605\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-4.577770490515578, w1=-4.257992261849147\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-4.763424610516314, w1=-4.430109028402663\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-4.949078730516605, w1=-4.602225794956041\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-5.13473285051672, w1=-4.774342561509367\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-5.320386970516766, w1=-4.946459328062673\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-5.506041090516784, w1=-5.118576094615971\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-5.691695210516791, w1=-5.2906928611692665\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-5.877349330516793, w1=-5.462809627722561\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-1905.4057747363252, w0=-0.48065175999999993, w1=-0.46177074052449596\n",
      "Logistic regression iter. 1/29: loss=-519.9799777447104, w0=-0.6755044314341414, w1=-0.6410004990519396\n",
      "Logistic regression iter. 2/29: loss=-153.8824063029005, w0=-0.8636494324998445, w1=-0.8149425149217839\n",
      "Logistic regression iter. 3/29: loss=-48.98530913635293, w0=-1.050025190115274, w1=-0.9875645132484097\n",
      "Logistic regression iter. 4/29: loss=-17.837354206958228, w0=-1.2358967754242587, w1=-1.1598273310610836\n",
      "Logistic regression iter. 5/29: loss=-8.536147534448803, w0=-1.4216184188957606, w1=-1.3319876922828577\n",
      "Logistic regression iter. 6/29: loss=-5.922419750265129, w0=-1.6072940410965597, w1=-1.5041178180041466\n",
      "Logistic regression iter. 7/29: loss=-5.410579404834833, w0=-1.7929551619427324, w1=-1.6762387717761258\n",
      "Logistic regression iter. 8/29: loss=-5.5746211185482055, w0=-1.9786116071439324, w1=-1.8483568770171872\n",
      "Logistic regression iter. 9/29: loss=-5.960470536333448, w0=-2.1642665134850785, w1=-2.0204740791888067\n",
      "Logistic regression iter. 10/29: loss=-6.420468931569391, w0=-2.3499209038403723, w1=-2.192590989771513\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.535575118217085, w1=-2.364707804640919\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.7212292716296385, w1=-2.5368245876192876\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-2.9068834036146143, w1=-2.70894135982527\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.092537527966342, w1=-2.8810581283462455\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.278191649564555, w1=-3.0531748955918103\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.463845770157805, w1=-3.2252916623910997\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.649499890380227, w1=-3.3974084290326303\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.835154010464401, w1=-3.569525195617855\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.020808130496537, w1=-3.7416419621828\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-4.206462250508906, w1=-3.9137587287403792\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-4.392116370513704, w1=-4.0858754952952605\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-4.577770490515578, w1=-4.257992261849147\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-4.763424610516314, w1=-4.430109028402663\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-4.949078730516605, w1=-4.602225794956041\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-5.13473285051672, w1=-4.774342561509367\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-5.320386970516766, w1=-4.946459328062673\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-5.506041090516784, w1=-5.118576094615971\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-5.691695210516791, w1=-5.2906928611692665\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-5.877349330516793, w1=-5.462809627722561\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-1905.4057747363252, w0=-0.48065175999999993, w1=-0.46177074052449596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 1/29: loss=-519.9799777447104, w0=-0.6755044314341414, w1=-0.6410004990519396\n",
      "Logistic regression iter. 2/29: loss=-153.8824063029005, w0=-0.8636494324998445, w1=-0.8149425149217839\n",
      "Logistic regression iter. 3/29: loss=-48.98530913635293, w0=-1.050025190115274, w1=-0.9875645132484097\n",
      "Logistic regression iter. 4/29: loss=-17.837354206958228, w0=-1.2358967754242587, w1=-1.1598273310610836\n",
      "Logistic regression iter. 5/29: loss=-8.536147534448803, w0=-1.4216184188957606, w1=-1.3319876922828577\n",
      "Logistic regression iter. 6/29: loss=-5.922419750265129, w0=-1.6072940410965597, w1=-1.5041178180041466\n",
      "Logistic regression iter. 7/29: loss=-5.410579404834833, w0=-1.7929551619427324, w1=-1.6762387717761258\n",
      "Logistic regression iter. 8/29: loss=-5.5746211185482055, w0=-1.9786116071439324, w1=-1.8483568770171872\n",
      "Logistic regression iter. 9/29: loss=-5.960470536333448, w0=-2.1642665134850785, w1=-2.0204740791888067\n",
      "Logistic regression iter. 10/29: loss=-6.420468931569391, w0=-2.3499209038403723, w1=-2.192590989771513\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.535575118217085, w1=-2.364707804640919\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.7212292716296385, w1=-2.5368245876192876\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-2.9068834036146143, w1=-2.70894135982527\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.092537527966342, w1=-2.8810581283462455\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.278191649564555, w1=-3.0531748955918103\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.463845770157805, w1=-3.2252916623910997\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.649499890380227, w1=-3.3974084290326303\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.835154010464401, w1=-3.569525195617855\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.020808130496537, w1=-3.7416419621828\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-4.206462250508906, w1=-3.9137587287403792\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-4.392116370513704, w1=-4.0858754952952605\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-4.577770490515578, w1=-4.257992261849147\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-4.763424610516314, w1=-4.430109028402663\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-4.949078730516605, w1=-4.602225794956041\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-5.13473285051672, w1=-4.774342561509367\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-5.320386970516766, w1=-4.946459328062673\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-5.506041090516784, w1=-5.118576094615971\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-5.691695210516791, w1=-5.2906928611692665\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-5.877349330516793, w1=-5.462809627722561\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-1905.4057747363252, w0=-0.48065175999999993, w1=-0.46177074052449596\n",
      "Logistic regression iter. 1/29: loss=-519.9799777447104, w0=-0.6755044314341414, w1=-0.6410004990519396\n",
      "Logistic regression iter. 2/29: loss=-153.8824063029005, w0=-0.8636494324998445, w1=-0.8149425149217839\n",
      "Logistic regression iter. 3/29: loss=-48.98530913635293, w0=-1.050025190115274, w1=-0.9875645132484097\n",
      "Logistic regression iter. 4/29: loss=-17.837354206958228, w0=-1.2358967754242587, w1=-1.1598273310610836\n",
      "Logistic regression iter. 5/29: loss=-8.536147534448803, w0=-1.4216184188957606, w1=-1.3319876922828577\n",
      "Logistic regression iter. 6/29: loss=-5.922419750265129, w0=-1.6072940410965597, w1=-1.5041178180041466\n",
      "Logistic regression iter. 7/29: loss=-5.410579404834833, w0=-1.7929551619427324, w1=-1.6762387717761258\n",
      "Logistic regression iter. 8/29: loss=-5.5746211185482055, w0=-1.9786116071439324, w1=-1.8483568770171872\n",
      "Logistic regression iter. 9/29: loss=-5.960470536333448, w0=-2.1642665134850785, w1=-2.0204740791888067\n",
      "Logistic regression iter. 10/29: loss=-6.420468931569391, w0=-2.3499209038403723, w1=-2.192590989771513\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.535575118217085, w1=-2.364707804640919\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.7212292716296385, w1=-2.5368245876192876\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-2.9068834036146143, w1=-2.70894135982527\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.092537527966342, w1=-2.8810581283462455\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.278191649564555, w1=-3.0531748955918103\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.463845770157805, w1=-3.2252916623910997\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.649499890380227, w1=-3.3974084290326303\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.835154010464401, w1=-3.569525195617855\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.020808130496537, w1=-3.7416419621828\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-4.206462250508906, w1=-3.9137587287403792\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-4.392116370513704, w1=-4.0858754952952605\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-4.577770490515578, w1=-4.257992261849147\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-4.763424610516314, w1=-4.430109028402663\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-4.949078730516605, w1=-4.602225794956041\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-5.13473285051672, w1=-4.774342561509367\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-5.320386970516766, w1=-4.946459328062673\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-5.506041090516784, w1=-5.118576094615971\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-5.691695210516791, w1=-5.2906928611692665\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-5.877349330516793, w1=-5.462809627722561\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-1700.688511143077, w0=-0.49694503999999984, w1=-0.4774239859660042\n",
      "Logistic regression iter. 1/29: loss=-449.25465225194876, w0=-0.6973685614433529, w1=-0.6618954649520808\n",
      "Logistic regression iter. 2/29: loss=-128.8502736778233, w0=-0.891534918668283, w1=-0.8414633332650929\n",
      "Logistic regression iter. 3/29: loss=-40.155259728842644, w0=-1.084103368452497, w1=-1.0198465183536602\n",
      "Logistic regression iter. 4/29: loss=-14.772629198101376, w0=-1.27623169995254, w1=-1.1979183263872042\n",
      "Logistic regression iter. 5/29: loss=-7.5300461067383395, w0=-1.4682334940540787, w1=-1.3759043357868548\n",
      "Logistic regression iter. 6/29: loss=-5.66031450011968, w0=-1.660197716583575, w1=-1.5538658734693271\n",
      "Logistic regression iter. 7/29: loss=-5.425619592485583, w0=-1.852150477380024, w1=-1.7318202316552957\n",
      "Logistic regression iter. 8/29: loss=-5.700333473961138, w0=-2.044099657661611, w1=-1.9097724320845477\n",
      "Logistic regression iter. 9/29: loss=-6.137131164660578, w0=-2.2360476954322004, w1=-2.087723970157394\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.427995361560264, w1=-2.265675301050313\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.6199429046559977, w1=-2.443626566032986\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.8118904063584256, w1=-2.6215778097227256\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.0038378939247594, w1=-2.79952904643602\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.19578537659609, w1=-2.9774802808336585\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.3877328575503545, w1=-3.1554315144533547\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.5796803378950006, w1=-3.333382747808748\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.7716278180207574, w1=-3.5113339810733963\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.963575298067086, w1=-3.6892852143065786\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.155522778084307, w1=-3.867236447528748\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-4.347470258090762, w1=-4.045187680747029\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-4.539417738093199, w1=-4.2231389139639255\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-4.731365218094126, w1=-4.401090147180326\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-4.923312698094481, w1=-4.579041380396546\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-5.115260178094617, w1=-4.756992613612701\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-5.307207658094669, w1=-4.9349438468288325\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-5.4991551380946895, w1=-5.112895080044955\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-5.691102618094697, w1=-5.290846313261074\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-5.8830500980947, w1=-5.468797546477192\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-6.074997578094702, w1=-5.64674877969331\n",
      "4\n",
      "4,5\n",
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-1700.688511143077, w0=-0.49694503999999984, w1=-0.4774239859660042\n",
      "Logistic regression iter. 1/29: loss=-449.25465225194876, w0=-0.6973685614433529, w1=-0.6618954649520808\n",
      "Logistic regression iter. 2/29: loss=-128.8502736778233, w0=-0.891534918668283, w1=-0.8414633332650929\n",
      "Logistic regression iter. 3/29: loss=-40.155259728842644, w0=-1.084103368452497, w1=-1.0198465183536602\n",
      "Logistic regression iter. 4/29: loss=-14.772629198101376, w0=-1.27623169995254, w1=-1.1979183263872042\n",
      "Logistic regression iter. 5/29: loss=-7.5300461067383395, w0=-1.4682334940540787, w1=-1.3759043357868548\n",
      "Logistic regression iter. 6/29: loss=-5.66031450011968, w0=-1.660197716583575, w1=-1.5538658734693271\n",
      "Logistic regression iter. 7/29: loss=-5.425619592485583, w0=-1.852150477380024, w1=-1.7318202316552957\n",
      "Logistic regression iter. 8/29: loss=-5.700333473961138, w0=-2.044099657661611, w1=-1.9097724320845477\n",
      "Logistic regression iter. 9/29: loss=-6.137131164660578, w0=-2.2360476954322004, w1=-2.087723970157394\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.427995361560264, w1=-2.265675301050313\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.6199429046559977, w1=-2.443626566032986\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.8118904063584256, w1=-2.6215778097227256\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.0038378939247594, w1=-2.79952904643602\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.19578537659609, w1=-2.9774802808336585\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.3877328575503545, w1=-3.1554315144533547\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.5796803378950006, w1=-3.333382747808748\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.7716278180207574, w1=-3.5113339810733963\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.963575298067086, w1=-3.6892852143065786\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.155522778084307, w1=-3.867236447528748\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-4.347470258090762, w1=-4.045187680747029\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-4.539417738093199, w1=-4.2231389139639255\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-4.731365218094126, w1=-4.401090147180326\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-4.923312698094481, w1=-4.579041380396546\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-5.115260178094617, w1=-4.756992613612701\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-5.307207658094669, w1=-4.9349438468288325\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-5.4991551380946895, w1=-5.112895080044955\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-5.691102618094697, w1=-5.290846313261074\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-5.8830500980947, w1=-5.468797546477192\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-6.074997578094702, w1=-5.64674877969331\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-1700.688511143077, w0=-0.49694503999999984, w1=-0.4774239859660042\n",
      "Logistic regression iter. 1/29: loss=-449.25465225194876, w0=-0.6973685614433529, w1=-0.6618954649520808\n",
      "Logistic regression iter. 2/29: loss=-128.8502736778233, w0=-0.891534918668283, w1=-0.8414633332650929\n",
      "Logistic regression iter. 3/29: loss=-40.155259728842644, w0=-1.084103368452497, w1=-1.0198465183536602\n",
      "Logistic regression iter. 4/29: loss=-14.772629198101376, w0=-1.27623169995254, w1=-1.1979183263872042\n",
      "Logistic regression iter. 5/29: loss=-7.5300461067383395, w0=-1.4682334940540787, w1=-1.3759043357868548\n",
      "Logistic regression iter. 6/29: loss=-5.66031450011968, w0=-1.660197716583575, w1=-1.5538658734693271\n",
      "Logistic regression iter. 7/29: loss=-5.425619592485583, w0=-1.852150477380024, w1=-1.7318202316552957\n",
      "Logistic regression iter. 8/29: loss=-5.700333473961138, w0=-2.044099657661611, w1=-1.9097724320845477\n",
      "Logistic regression iter. 9/29: loss=-6.137131164660578, w0=-2.2360476954322004, w1=-2.087723970157394\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.427995361560264, w1=-2.265675301050313\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.6199429046559977, w1=-2.443626566032986\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.8118904063584256, w1=-2.6215778097227256\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.0038378939247594, w1=-2.79952904643602\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.19578537659609, w1=-2.9774802808336585\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.3877328575503545, w1=-3.1554315144533547\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.5796803378950006, w1=-3.333382747808748\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.7716278180207574, w1=-3.5113339810733963\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.963575298067086, w1=-3.6892852143065786\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.155522778084307, w1=-3.867236447528748\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-4.347470258090762, w1=-4.045187680747029\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-4.539417738093199, w1=-4.2231389139639255\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-4.731365218094126, w1=-4.401090147180326\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-4.923312698094481, w1=-4.579041380396546\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-5.115260178094617, w1=-4.756992613612701\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-5.307207658094669, w1=-4.9349438468288325\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-5.4991551380946895, w1=-5.112895080044955\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-5.691102618094697, w1=-5.290846313261074\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-5.8830500980947, w1=-5.468797546477192\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-6.074997578094702, w1=-5.64674877969331\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-1700.688511143077, w0=-0.49694503999999984, w1=-0.4774239859660042\n",
      "Logistic regression iter. 1/29: loss=-449.25465225194876, w0=-0.6973685614433529, w1=-0.6618954649520808\n",
      "Logistic regression iter. 2/29: loss=-128.8502736778233, w0=-0.891534918668283, w1=-0.8414633332650929\n",
      "Logistic regression iter. 3/29: loss=-40.155259728842644, w0=-1.084103368452497, w1=-1.0198465183536602\n",
      "Logistic regression iter. 4/29: loss=-14.772629198101376, w0=-1.27623169995254, w1=-1.1979183263872042\n",
      "Logistic regression iter. 5/29: loss=-7.5300461067383395, w0=-1.4682334940540787, w1=-1.3759043357868548\n",
      "Logistic regression iter. 6/29: loss=-5.66031450011968, w0=-1.660197716583575, w1=-1.5538658734693271\n",
      "Logistic regression iter. 7/29: loss=-5.425619592485583, w0=-1.852150477380024, w1=-1.7318202316552957\n",
      "Logistic regression iter. 8/29: loss=-5.700333473961138, w0=-2.044099657661611, w1=-1.9097724320845477\n",
      "Logistic regression iter. 9/29: loss=-6.137131164660578, w0=-2.2360476954322004, w1=-2.087723970157394\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.427995361560264, w1=-2.265675301050313\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.6199429046559977, w1=-2.443626566032986\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.8118904063584256, w1=-2.6215778097227256\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.0038378939247594, w1=-2.79952904643602\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.19578537659609, w1=-2.9774802808336585\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.3877328575503545, w1=-3.1554315144533547\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.5796803378950006, w1=-3.333382747808748\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.7716278180207574, w1=-3.5113339810733963\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-3.963575298067086, w1=-3.6892852143065786\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.155522778084307, w1=-3.867236447528748\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-4.347470258090762, w1=-4.045187680747029\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-4.539417738093199, w1=-4.2231389139639255\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-4.731365218094126, w1=-4.401090147180326\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-4.923312698094481, w1=-4.579041380396546\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-5.115260178094617, w1=-4.756992613612701\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-5.307207658094669, w1=-4.9349438468288325\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-5.4991551380946895, w1=-5.112895080044955\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-5.691102618094697, w1=-5.290846313261074\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-5.8830500980947, w1=-5.468797546477192\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-6.074997578094702, w1=-5.64674877969331\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-1518.8119128853828, w0=-0.5132383199999999, w1=-0.4930772314075126\n",
      "Logistic regression iter. 1/29: loss=-388.36292880104395, w0=-0.7192849632069693, w1=-0.6828371187147705\n",
      "Logistic regression iter. 2/29: loss=-108.03034798519737, w0=-0.9195011092259054, w1=-0.86805391090746\n",
      "Logistic regression iter. 3/29: loss=-33.07242917611104, w0=-1.1182759959757924, w1=-1.0522087382597123\n",
      "Logistic regression iter. 4/29: loss=-12.414395609858333, w0=-1.3166671781964443, w1=-1.2360939478497508\n",
      "Logistic regression iter. 5/29: loss=-6.805017269643519, w0=-1.5149516979548865, w1=-1.4199073848114785\n",
      "Logistic regression iter. 6/29: loss=-5.509053707547121, w0=-1.7132055743448362, w1=-1.6037010330549994\n",
      "Logistic regression iter. 7/29: loss=-5.484254269459927, w0=-1.9114503985029059, w1=-1.7874890655762552\n",
      "Logistic regression iter. 8/29: loss=-5.843256614172423, w0=-2.109692482330023, w1=-1.9712754645282098\n",
      "Logistic regression iter. 9/29: loss=-6.320683010577622, w0=-2.3079337182154673, w1=-2.1550613778677254\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.5061746864626793, w1=-2.338847144035199\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.7044155686889515, w1=-2.5226328648181413\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.9026564228026595, w1=-2.7064185713827076\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.100897267586226, w1=-2.8902042734279068\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.299138109228675, w1=-3.073989974017296\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.497378949799466, w1=-3.2577756741318815\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.6956197900000767, w1=-3.441561374089813\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.893860630071328, w1=-3.6253470739954965\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.092101470096884, w1=-3.8091327738835763\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.2903423101061335, w1=-3.992918473765668\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-4.48858315010951, w1=-4.176704173645704\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-4.686823990110752, w1=-4.360489873525029\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-4.885064830111212, w1=-4.5442755734041045\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-5.083305670111383, w1=-4.728061273283093\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-5.281546510111446, w1=-4.911846973162051\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-5.47978735011147, w1=-5.095632673040998\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-5.678028190111478, w1=-5.279418372919941\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-5.876269030111481, w1=-5.463204072798883\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-6.074509870111482, w1=-5.6469897726778235\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-6.272750710111483, w1=-5.830775472556764\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-1518.8119128853828, w0=-0.5132383199999999, w1=-0.4930772314075126\n",
      "Logistic regression iter. 1/29: loss=-388.36292880104395, w0=-0.7192849632069693, w1=-0.6828371187147705\n",
      "Logistic regression iter. 2/29: loss=-108.03034798519737, w0=-0.9195011092259054, w1=-0.86805391090746\n",
      "Logistic regression iter. 3/29: loss=-33.07242917611104, w0=-1.1182759959757924, w1=-1.0522087382597123\n",
      "Logistic regression iter. 4/29: loss=-12.414395609858333, w0=-1.3166671781964443, w1=-1.2360939478497508\n",
      "Logistic regression iter. 5/29: loss=-6.805017269643519, w0=-1.5149516979548865, w1=-1.4199073848114785\n",
      "Logistic regression iter. 6/29: loss=-5.509053707547121, w0=-1.7132055743448362, w1=-1.6037010330549994\n",
      "Logistic regression iter. 7/29: loss=-5.484254269459927, w0=-1.9114503985029059, w1=-1.7874890655762552\n",
      "Logistic regression iter. 8/29: loss=-5.843256614172423, w0=-2.109692482330023, w1=-1.9712754645282098\n",
      "Logistic regression iter. 9/29: loss=-6.320683010577622, w0=-2.3079337182154673, w1=-2.1550613778677254\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.5061746864626793, w1=-2.338847144035199\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.7044155686889515, w1=-2.5226328648181413\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.9026564228026595, w1=-2.7064185713827076\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.100897267586226, w1=-2.8902042734279068\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.299138109228675, w1=-3.073989974017296\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.497378949799466, w1=-3.2577756741318815\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.6956197900000767, w1=-3.441561374089813\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.893860630071328, w1=-3.6253470739954965\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.092101470096884, w1=-3.8091327738835763\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.2903423101061335, w1=-3.992918473765668\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-4.48858315010951, w1=-4.176704173645704\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-4.686823990110752, w1=-4.360489873525029\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-4.885064830111212, w1=-4.5442755734041045\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-5.083305670111383, w1=-4.728061273283093\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-5.281546510111446, w1=-4.911846973162051\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-5.47978735011147, w1=-5.095632673040998\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-5.678028190111478, w1=-5.279418372919941\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-5.876269030111481, w1=-5.463204072798883\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-6.074509870111482, w1=-5.6469897726778235\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-6.272750710111483, w1=-5.830775472556764\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-1518.8119128853828, w0=-0.5132383199999999, w1=-0.4930772314075126\n",
      "Logistic regression iter. 1/29: loss=-388.36292880104395, w0=-0.7192849632069693, w1=-0.6828371187147705\n",
      "Logistic regression iter. 2/29: loss=-108.03034798519737, w0=-0.9195011092259054, w1=-0.86805391090746\n",
      "Logistic regression iter. 3/29: loss=-33.07242917611104, w0=-1.1182759959757924, w1=-1.0522087382597123\n",
      "Logistic regression iter. 4/29: loss=-12.414395609858333, w0=-1.3166671781964443, w1=-1.2360939478497508\n",
      "Logistic regression iter. 5/29: loss=-6.805017269643519, w0=-1.5149516979548865, w1=-1.4199073848114785\n",
      "Logistic regression iter. 6/29: loss=-5.509053707547121, w0=-1.7132055743448362, w1=-1.6037010330549994\n",
      "Logistic regression iter. 7/29: loss=-5.484254269459927, w0=-1.9114503985029059, w1=-1.7874890655762552\n",
      "Logistic regression iter. 8/29: loss=-5.843256614172423, w0=-2.109692482330023, w1=-1.9712754645282098\n",
      "Logistic regression iter. 9/29: loss=-6.320683010577622, w0=-2.3079337182154673, w1=-2.1550613778677254\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.5061746864626793, w1=-2.338847144035199\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.7044155686889515, w1=-2.5226328648181413\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.9026564228026595, w1=-2.7064185713827076\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.100897267586226, w1=-2.8902042734279068\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.299138109228675, w1=-3.073989974017296\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.497378949799466, w1=-3.2577756741318815\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.6956197900000767, w1=-3.441561374089813\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.893860630071328, w1=-3.6253470739954965\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.092101470096884, w1=-3.8091327738835763\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.2903423101061335, w1=-3.992918473765668\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-4.48858315010951, w1=-4.176704173645704\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-4.686823990110752, w1=-4.360489873525029\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-4.885064830111212, w1=-4.5442755734041045\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-5.083305670111383, w1=-4.728061273283093\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-5.281546510111446, w1=-4.911846973162051\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-5.47978735011147, w1=-5.095632673040998\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-5.678028190111478, w1=-5.279418372919941\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-5.876269030111481, w1=-5.463204072798883\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-6.074509870111482, w1=-5.6469897726778235\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-6.272750710111483, w1=-5.830775472556764\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-1518.8119128853828, w0=-0.5132383199999999, w1=-0.4930772314075126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 1/29: loss=-388.36292880104395, w0=-0.7192849632069693, w1=-0.6828371187147705\n",
      "Logistic regression iter. 2/29: loss=-108.03034798519737, w0=-0.9195011092259054, w1=-0.86805391090746\n",
      "Logistic regression iter. 3/29: loss=-33.07242917611104, w0=-1.1182759959757924, w1=-1.0522087382597123\n",
      "Logistic regression iter. 4/29: loss=-12.414395609858333, w0=-1.3166671781964443, w1=-1.2360939478497508\n",
      "Logistic regression iter. 5/29: loss=-6.805017269643519, w0=-1.5149516979548865, w1=-1.4199073848114785\n",
      "Logistic regression iter. 6/29: loss=-5.509053707547121, w0=-1.7132055743448362, w1=-1.6037010330549994\n",
      "Logistic regression iter. 7/29: loss=-5.484254269459927, w0=-1.9114503985029059, w1=-1.7874890655762552\n",
      "Logistic regression iter. 8/29: loss=-5.843256614172423, w0=-2.109692482330023, w1=-1.9712754645282098\n",
      "Logistic regression iter. 9/29: loss=-6.320683010577622, w0=-2.3079337182154673, w1=-2.1550613778677254\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.5061746864626793, w1=-2.338847144035199\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.7044155686889515, w1=-2.5226328648181413\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.9026564228026595, w1=-2.7064185713827076\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.100897267586226, w1=-2.8902042734279068\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.299138109228675, w1=-3.073989974017296\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.497378949799466, w1=-3.2577756741318815\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.6956197900000767, w1=-3.441561374089813\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-3.893860630071328, w1=-3.6253470739954965\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.092101470096884, w1=-3.8091327738835763\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.2903423101061335, w1=-3.992918473765668\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-4.48858315010951, w1=-4.176704173645704\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-4.686823990110752, w1=-4.360489873525029\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-4.885064830111212, w1=-4.5442755734041045\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-5.083305670111383, w1=-4.728061273283093\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-5.281546510111446, w1=-4.911846973162051\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-5.47978735011147, w1=-5.095632673040998\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-5.678028190111478, w1=-5.279418372919941\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-5.876269030111481, w1=-5.463204072798883\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-6.074509870111482, w1=-5.6469897726778235\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-6.272750710111483, w1=-5.830775472556764\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-1357.1303637166855, w0=-0.5295315999999999, w1=-0.508730476849021\n",
      "Logistic regression iter. 1/29: loss=-335.91906620295924, w0=-0.7412507017443005, w1=-0.7038223403474133\n",
      "Logistic regression iter. 2/29: loss=-90.70935255585235, w0=-0.9475423785812727, w1=-0.8947087579410045\n",
      "Logistic regression iter. 3/29: loss=-27.39180054480125, w0=-1.152535651364059, w1=-1.0846442411026949\n",
      "Logistic regression iter. 4/29: loss=-10.604225406068212, w0=-1.3571947916272522, w1=-1.274346511719828\n",
      "Logistic regression iter. 5/29: loss=-6.290947309211459, w0=-1.5617641146372, w1=-1.463988799058972\n",
      "Logistic regression iter. 6/29: loss=-5.4383725797228015, w0=-1.766308465971816, w1=-1.6536150970657126\n",
      "Logistic regression iter. 7/29: loss=-5.57363075019685, w0=-1.9708456726735126, w1=-1.8432370052444271\n",
      "Logistic regression iter. 8/29: loss=-5.997984869911384, w0=-2.1753807830128, w1=-2.0328576772644045\n",
      "Logistic regression iter. 9/29: loss=-6.508860511699312, w0=-2.379915264186516, w1=-2.2224779933577743\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.5844495526291356, w1=-2.412098204919871\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.7889837809188083, w1=-2.601718385230047\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.9935179901088813, w1=-2.7913385560440367\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.198052193137487, w1=-2.980958723929313\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.402586394148965, w1=-3.1705788908988706\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.6071205944909885, w1=-3.3601990575784413\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.8116547946079846, w1=-3.5498192241650828\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.016188994648438, w1=-3.7394393907216115\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.220723194662566, w1=-3.92905955726828\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.425257394667545, w1=-4.118679723811688\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-4.629791594669315, w1=-4.308299890354008\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-4.834325794669948, w1=-4.497920056895961\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-5.038859994670177, w1=-4.68754022343779\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-5.243394194670259, w1=-4.8771603899795775\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-5.4479283946702886, w1=-5.0667805565213495\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-5.6524625946702995, w1=-5.256400723063116\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-5.856996794670303, w1=-5.446020889604881\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-6.061530994670305, w1=-5.635641056146646\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-6.266065194670305, w1=-5.82526122268841\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-6.470599394670305, w1=-6.014881389230174\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-1357.1303637166855, w0=-0.5295315999999999, w1=-0.508730476849021\n",
      "Logistic regression iter. 1/29: loss=-335.91906620295924, w0=-0.7412507017443005, w1=-0.7038223403474133\n",
      "Logistic regression iter. 2/29: loss=-90.70935255585235, w0=-0.9475423785812727, w1=-0.8947087579410045\n",
      "Logistic regression iter. 3/29: loss=-27.39180054480125, w0=-1.152535651364059, w1=-1.0846442411026949\n",
      "Logistic regression iter. 4/29: loss=-10.604225406068212, w0=-1.3571947916272522, w1=-1.274346511719828\n",
      "Logistic regression iter. 5/29: loss=-6.290947309211459, w0=-1.5617641146372, w1=-1.463988799058972\n",
      "Logistic regression iter. 6/29: loss=-5.4383725797228015, w0=-1.766308465971816, w1=-1.6536150970657126\n",
      "Logistic regression iter. 7/29: loss=-5.57363075019685, w0=-1.9708456726735126, w1=-1.8432370052444271\n",
      "Logistic regression iter. 8/29: loss=-5.997984869911384, w0=-2.1753807830128, w1=-2.0328576772644045\n",
      "Logistic regression iter. 9/29: loss=-6.508860511699312, w0=-2.379915264186516, w1=-2.2224779933577743\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.5844495526291356, w1=-2.412098204919871\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.7889837809188083, w1=-2.601718385230047\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.9935179901088813, w1=-2.7913385560440367\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.198052193137487, w1=-2.980958723929313\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.402586394148965, w1=-3.1705788908988706\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.6071205944909885, w1=-3.3601990575784413\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.8116547946079846, w1=-3.5498192241650828\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.016188994648438, w1=-3.7394393907216115\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.220723194662566, w1=-3.92905955726828\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.425257394667545, w1=-4.118679723811688\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-4.629791594669315, w1=-4.308299890354008\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-4.834325794669948, w1=-4.497920056895961\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-5.038859994670177, w1=-4.68754022343779\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-5.243394194670259, w1=-4.8771603899795775\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-5.4479283946702886, w1=-5.0667805565213495\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-5.6524625946702995, w1=-5.256400723063116\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-5.856996794670303, w1=-5.446020889604881\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-6.061530994670305, w1=-5.635641056146646\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-6.266065194670305, w1=-5.82526122268841\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-6.470599394670305, w1=-6.014881389230174\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-1357.1303637166855, w0=-0.5295315999999999, w1=-0.508730476849021\n",
      "Logistic regression iter. 1/29: loss=-335.91906620295924, w0=-0.7412507017443005, w1=-0.7038223403474133\n",
      "Logistic regression iter. 2/29: loss=-90.70935255585235, w0=-0.9475423785812727, w1=-0.8947087579410045\n",
      "Logistic regression iter. 3/29: loss=-27.39180054480125, w0=-1.152535651364059, w1=-1.0846442411026949\n",
      "Logistic regression iter. 4/29: loss=-10.604225406068212, w0=-1.3571947916272522, w1=-1.274346511719828\n",
      "Logistic regression iter. 5/29: loss=-6.290947309211459, w0=-1.5617641146372, w1=-1.463988799058972\n",
      "Logistic regression iter. 6/29: loss=-5.4383725797228015, w0=-1.766308465971816, w1=-1.6536150970657126\n",
      "Logistic regression iter. 7/29: loss=-5.57363075019685, w0=-1.9708456726735126, w1=-1.8432370052444271\n",
      "Logistic regression iter. 8/29: loss=-5.997984869911384, w0=-2.1753807830128, w1=-2.0328576772644045\n",
      "Logistic regression iter. 9/29: loss=-6.508860511699312, w0=-2.379915264186516, w1=-2.2224779933577743\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.5844495526291356, w1=-2.412098204919871\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.7889837809188083, w1=-2.601718385230047\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.9935179901088813, w1=-2.7913385560440367\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.198052193137487, w1=-2.980958723929313\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.402586394148965, w1=-3.1705788908988706\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.6071205944909885, w1=-3.3601990575784413\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.8116547946079846, w1=-3.5498192241650828\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.016188994648438, w1=-3.7394393907216115\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.220723194662566, w1=-3.92905955726828\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.425257394667545, w1=-4.118679723811688\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-4.629791594669315, w1=-4.308299890354008\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-4.834325794669948, w1=-4.497920056895961\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-5.038859994670177, w1=-4.68754022343779\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-5.243394194670259, w1=-4.8771603899795775\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-5.4479283946702886, w1=-5.0667805565213495\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-5.6524625946702995, w1=-5.256400723063116\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-5.856996794670303, w1=-5.446020889604881\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-6.061530994670305, w1=-5.635641056146646\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-6.266065194670305, w1=-5.82526122268841\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-6.470599394670305, w1=-6.014881389230174\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-1357.1303637166855, w0=-0.5295315999999999, w1=-0.508730476849021\n",
      "Logistic regression iter. 1/29: loss=-335.91906620295924, w0=-0.7412507017443005, w1=-0.7038223403474133\n",
      "Logistic regression iter. 2/29: loss=-90.70935255585235, w0=-0.9475423785812727, w1=-0.8947087579410045\n",
      "Logistic regression iter. 3/29: loss=-27.39180054480125, w0=-1.152535651364059, w1=-1.0846442411026949\n",
      "Logistic regression iter. 4/29: loss=-10.604225406068212, w0=-1.3571947916272522, w1=-1.274346511719828\n",
      "Logistic regression iter. 5/29: loss=-6.290947309211459, w0=-1.5617641146372, w1=-1.463988799058972\n",
      "Logistic regression iter. 6/29: loss=-5.4383725797228015, w0=-1.766308465971816, w1=-1.6536150970657126\n",
      "Logistic regression iter. 7/29: loss=-5.57363075019685, w0=-1.9708456726735126, w1=-1.8432370052444271\n",
      "Logistic regression iter. 8/29: loss=-5.997984869911384, w0=-2.1753807830128, w1=-2.0328576772644045\n",
      "Logistic regression iter. 9/29: loss=-6.508860511699312, w0=-2.379915264186516, w1=-2.2224779933577743\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.5844495526291356, w1=-2.412098204919871\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.7889837809188083, w1=-2.601718385230047\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-2.9935179901088813, w1=-2.7913385560440367\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.198052193137487, w1=-2.980958723929313\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.402586394148965, w1=-3.1705788908988706\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.6071205944909885, w1=-3.3601990575784413\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.8116547946079846, w1=-3.5498192241650828\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.016188994648438, w1=-3.7394393907216115\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.220723194662566, w1=-3.92905955726828\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.425257394667545, w1=-4.118679723811688\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-4.629791594669315, w1=-4.308299890354008\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-4.834325794669948, w1=-4.497920056895961\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-5.038859994670177, w1=-4.68754022343779\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-5.243394194670259, w1=-4.8771603899795775\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-5.4479283946702886, w1=-5.0667805565213495\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-5.6524625946702995, w1=-5.256400723063116\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-5.856996794670303, w1=-5.446020889604881\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-6.061530994670305, w1=-5.635641056146646\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-6.266065194670305, w1=-5.82526122268841\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-6.470599394670305, w1=-6.014881389230174\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-1213.3165813018115, w0=-0.54582488, w1=-0.5243837222905293\n",
      "Logistic regression iter. 1/29: loss=-290.73496201252584, w0=-0.7632628384582683, w1=-0.7248481029425892\n",
      "Logistic regression iter. 2/29: loss=-76.295613064597, w0=-0.9756532910047164, w1=-0.921422679997771\n",
      "Logistic regression iter. 3/29: loss=-22.836960594949513, w0=-1.1868753171962232, w1=-1.1171465762355006\n",
      "Logistic regression iter. 4/29: loss=-9.219492338369946, w0=-1.397806682297154, w1=-1.3126689398526787\n",
      "Logistic regression iter. 5/29: loss=-5.935357634285621, w0=-1.6086624831269256, w1=-1.5081412143016795\n",
      "Logistic regression iter. 6/29: loss=-5.426343577253107, w0=-1.8194979489349807, w1=-1.7036005780339185\n",
      "Logistic regression iter. 7/29: loss=-5.684725133248748, w0=-2.0303277787251677, w1=-1.8990565119954685\n",
      "Logistic regression iter. 8/29: loss=-6.160835292346059, w0=-2.2411560053363564, w1=-2.0945115108631054\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.4519837651798455, w1=-2.289966248909041\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.662811386227416, w1=-2.4854209127132756\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.873638965200919, w1=-2.6808755549949206\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.084466531192618, w1=-2.876330190932527\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.2952940931128873, w1=-3.071784824971408\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.506121653736795, w1=-3.2672394584339544\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.716949213942101, w1=-3.462694091719259\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.9277767740104625, w1=-3.658148724949387\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.138604334033476, w1=-3.8536033581621414\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.349431894041302, w1=-4.0490579913693665\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.5602594540439885, w1=-4.2445126245748135\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-4.771087014044918, w1=-4.439967257779684\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-4.981914574045241, w1=-4.635421890984365\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-5.192742134045354, w1=-4.830876524188985\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-5.403569694045394, w1=-5.0263311573935825\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-5.614397254045407, w1=-5.221785790598174\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-5.825224814045412, w1=-5.417240423802763\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-6.036052374045414, w1=-5.612695057007351\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-6.246879934045414, w1=-5.808149690211939\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-6.457707494045414, w1=-6.0036043234165275\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-6.668535054045415, w1=-6.199058956621116\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-1213.3165813018115, w0=-0.54582488, w1=-0.5243837222905293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 1/29: loss=-290.73496201252584, w0=-0.7632628384582683, w1=-0.7248481029425892\n",
      "Logistic regression iter. 2/29: loss=-76.295613064597, w0=-0.9756532910047164, w1=-0.921422679997771\n",
      "Logistic regression iter. 3/29: loss=-22.836960594949513, w0=-1.1868753171962232, w1=-1.1171465762355006\n",
      "Logistic regression iter. 4/29: loss=-9.219492338369946, w0=-1.397806682297154, w1=-1.3126689398526787\n",
      "Logistic regression iter. 5/29: loss=-5.935357634285621, w0=-1.6086624831269256, w1=-1.5081412143016795\n",
      "Logistic regression iter. 6/29: loss=-5.426343577253107, w0=-1.8194979489349807, w1=-1.7036005780339185\n",
      "Logistic regression iter. 7/29: loss=-5.684725133248748, w0=-2.0303277787251677, w1=-1.8990565119954685\n",
      "Logistic regression iter. 8/29: loss=-6.160835292346059, w0=-2.2411560053363564, w1=-2.0945115108631054\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.4519837651798455, w1=-2.289966248909041\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.662811386227416, w1=-2.4854209127132756\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.873638965200919, w1=-2.6808755549949206\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.084466531192618, w1=-2.876330190932527\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.2952940931128873, w1=-3.071784824971408\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.506121653736795, w1=-3.2672394584339544\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.716949213942101, w1=-3.462694091719259\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.9277767740104625, w1=-3.658148724949387\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.138604334033476, w1=-3.8536033581621414\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.349431894041302, w1=-4.0490579913693665\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.5602594540439885, w1=-4.2445126245748135\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-4.771087014044918, w1=-4.439967257779684\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-4.981914574045241, w1=-4.635421890984365\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-5.192742134045354, w1=-4.830876524188985\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-5.403569694045394, w1=-5.0263311573935825\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-5.614397254045407, w1=-5.221785790598174\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-5.825224814045412, w1=-5.417240423802763\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-6.036052374045414, w1=-5.612695057007351\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-6.246879934045414, w1=-5.808149690211939\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-6.457707494045414, w1=-6.0036043234165275\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-6.668535054045415, w1=-6.199058956621116\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-1213.3165813018115, w0=-0.54582488, w1=-0.5243837222905293\n",
      "Logistic regression iter. 1/29: loss=-290.73496201252584, w0=-0.7632628384582683, w1=-0.7248481029425892\n",
      "Logistic regression iter. 2/29: loss=-76.295613064597, w0=-0.9756532910047164, w1=-0.921422679997771\n",
      "Logistic regression iter. 3/29: loss=-22.836960594949513, w0=-1.1868753171962232, w1=-1.1171465762355006\n",
      "Logistic regression iter. 4/29: loss=-9.219492338369946, w0=-1.397806682297154, w1=-1.3126689398526787\n",
      "Logistic regression iter. 5/29: loss=-5.935357634285621, w0=-1.6086624831269256, w1=-1.5081412143016795\n",
      "Logistic regression iter. 6/29: loss=-5.426343577253107, w0=-1.8194979489349807, w1=-1.7036005780339185\n",
      "Logistic regression iter. 7/29: loss=-5.684725133248748, w0=-2.0303277787251677, w1=-1.8990565119954685\n",
      "Logistic regression iter. 8/29: loss=-6.160835292346059, w0=-2.2411560053363564, w1=-2.0945115108631054\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.4519837651798455, w1=-2.289966248909041\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.662811386227416, w1=-2.4854209127132756\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.873638965200919, w1=-2.6808755549949206\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.084466531192618, w1=-2.876330190932527\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.2952940931128873, w1=-3.071784824971408\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.506121653736795, w1=-3.2672394584339544\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.716949213942101, w1=-3.462694091719259\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.9277767740104625, w1=-3.658148724949387\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.138604334033476, w1=-3.8536033581621414\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.349431894041302, w1=-4.0490579913693665\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.5602594540439885, w1=-4.2445126245748135\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-4.771087014044918, w1=-4.439967257779684\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-4.981914574045241, w1=-4.635421890984365\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-5.192742134045354, w1=-4.830876524188985\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-5.403569694045394, w1=-5.0263311573935825\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-5.614397254045407, w1=-5.221785790598174\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-5.825224814045412, w1=-5.417240423802763\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-6.036052374045414, w1=-5.612695057007351\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-6.246879934045414, w1=-5.808149690211939\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-6.457707494045414, w1=-6.0036043234165275\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-6.668535054045415, w1=-6.199058956621116\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-1213.3165813018115, w0=-0.54582488, w1=-0.5243837222905293\n",
      "Logistic regression iter. 1/29: loss=-290.73496201252584, w0=-0.7632628384582683, w1=-0.7248481029425892\n",
      "Logistic regression iter. 2/29: loss=-76.295613064597, w0=-0.9756532910047164, w1=-0.921422679997771\n",
      "Logistic regression iter. 3/29: loss=-22.836960594949513, w0=-1.1868753171962232, w1=-1.1171465762355006\n",
      "Logistic regression iter. 4/29: loss=-9.219492338369946, w0=-1.397806682297154, w1=-1.3126689398526787\n",
      "Logistic regression iter. 5/29: loss=-5.935357634285621, w0=-1.6086624831269256, w1=-1.5081412143016795\n",
      "Logistic regression iter. 6/29: loss=-5.426343577253107, w0=-1.8194979489349807, w1=-1.7036005780339185\n",
      "Logistic regression iter. 7/29: loss=-5.684725133248748, w0=-2.0303277787251677, w1=-1.8990565119954685\n",
      "Logistic regression iter. 8/29: loss=-6.160835292346059, w0=-2.2411560053363564, w1=-2.0945115108631054\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.4519837651798455, w1=-2.289966248909041\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.662811386227416, w1=-2.4854209127132756\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.873638965200919, w1=-2.6808755549949206\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.084466531192618, w1=-2.876330190932527\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.2952940931128873, w1=-3.071784824971408\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.506121653736795, w1=-3.2672394584339544\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.716949213942101, w1=-3.462694091719259\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-3.9277767740104625, w1=-3.658148724949387\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.138604334033476, w1=-3.8536033581621414\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.349431894041302, w1=-4.0490579913693665\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.5602594540439885, w1=-4.2445126245748135\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-4.771087014044918, w1=-4.439967257779684\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-4.981914574045241, w1=-4.635421890984365\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-5.192742134045354, w1=-4.830876524188985\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-5.403569694045394, w1=-5.0263311573935825\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-5.614397254045407, w1=-5.221785790598174\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-5.825224814045412, w1=-5.417240423802763\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-6.036052374045414, w1=-5.612695057007351\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-6.246879934045414, w1=-5.808149690211939\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-6.457707494045414, w1=-6.0036043234165275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 29/29: loss=nan, w0=-6.668535054045415, w1=-6.199058956621116\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-1085.3218483165924, w0=-0.5621181599999999, w1=-0.5400369677320377\n",
      "Logistic regression iter. 1/29: loss=-251.79165468946542, w0=-0.785318473891755, w1=-0.7459114976744257\n",
      "Logistic regression iter. 2/29: loss=-64.29824519828743, w0=-1.0038286435295172, w1=-0.9481907954578821\n",
      "Logistic regression iter. 3/29: loss=-19.18639520636767, w0=-1.2212884025136408, w1=-1.1497097709306814\n",
      "Logistic regression iter. 4/29: loss=-8.165231272690384, w0=-1.4384955525700707, w1=-1.3510547369404455\n",
      "Logistic regression iter. 5/29: loss=-5.698937207148597, w0=-1.655639179813162, w1=-1.5523579060359405\n",
      "Logistic regression iter. 6/29: loss=-5.457061828630745, w0=-1.872766257977113, w1=-1.753650656261198\n",
      "Logistic regression iter. 7/29: loss=-5.811192210980056, w0=-2.0898888921101033, w1=-1.954940727941866\n",
      "Logistic regression iter. 8/29: loss=-6.329293416985365, w0=-2.3070103005228306, w1=-2.1562300924611857\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.52413136266778, w1=-2.357519265874152\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.741252324845315, w1=-2.558808386556423\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.958373257584775, w1=-2.760097492413779\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.1754941814962447, w1=-2.961386594031499\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.3926151027155482, w1=-3.1626756944176466\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.6097360231010103, w1=-3.3639647944408275\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.8268569432244637, w1=-3.5652538943555907\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.043977863264482, w1=-3.7665429942375623\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.2610987832776, w1=-3.9678320941094993\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.478219703281944, w1=-4.169121193978332\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.695340623283396, w1=-4.370410293846193\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-4.912461543283886, w1=-4.5716993937137484\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-5.129582463284051, w1=-4.772988493581206\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-5.346703383284107, w1=-4.974277593448632\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-5.563824303284126, w1=-5.175566693316049\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-5.7809452232841325, w1=-5.376855793183462\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-5.9980661432841345, w1=-5.578144893050874\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-6.2151870632841355, w1=-5.779433992918285\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-6.432307983284136, w1=-5.980723092785697\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-6.649428903284136, w1=-6.182012192653108\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-6.866549823284136, w1=-6.383301292520519\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-1085.3218483165924, w0=-0.5621181599999999, w1=-0.5400369677320377\n",
      "Logistic regression iter. 1/29: loss=-251.79165468946542, w0=-0.785318473891755, w1=-0.7459114976744257\n",
      "Logistic regression iter. 2/29: loss=-64.29824519828743, w0=-1.0038286435295172, w1=-0.9481907954578821\n",
      "Logistic regression iter. 3/29: loss=-19.18639520636767, w0=-1.2212884025136408, w1=-1.1497097709306814\n",
      "Logistic regression iter. 4/29: loss=-8.165231272690384, w0=-1.4384955525700707, w1=-1.3510547369404455\n",
      "Logistic regression iter. 5/29: loss=-5.698937207148597, w0=-1.655639179813162, w1=-1.5523579060359405\n",
      "Logistic regression iter. 6/29: loss=-5.457061828630745, w0=-1.872766257977113, w1=-1.753650656261198\n",
      "Logistic regression iter. 7/29: loss=-5.811192210980056, w0=-2.0898888921101033, w1=-1.954940727941866\n",
      "Logistic regression iter. 8/29: loss=-6.329293416985365, w0=-2.3070103005228306, w1=-2.1562300924611857\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.52413136266778, w1=-2.357519265874152\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.741252324845315, w1=-2.558808386556423\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.958373257584775, w1=-2.760097492413779\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.1754941814962447, w1=-2.961386594031499\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.3926151027155482, w1=-3.1626756944176466\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.6097360231010103, w1=-3.3639647944408275\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.8268569432244637, w1=-3.5652538943555907\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.043977863264482, w1=-3.7665429942375623\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.2610987832776, w1=-3.9678320941094993\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.478219703281944, w1=-4.169121193978332\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.695340623283396, w1=-4.370410293846193\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-4.912461543283886, w1=-4.5716993937137484\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-5.129582463284051, w1=-4.772988493581206\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-5.346703383284107, w1=-4.974277593448632\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-5.563824303284126, w1=-5.175566693316049\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-5.7809452232841325, w1=-5.376855793183462\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-5.9980661432841345, w1=-5.578144893050874\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-6.2151870632841355, w1=-5.779433992918285\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-6.432307983284136, w1=-5.980723092785697\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-6.649428903284136, w1=-6.182012192653108\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-6.866549823284136, w1=-6.383301292520519\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-1085.3218483165924, w0=-0.5621181599999999, w1=-0.5400369677320377\n",
      "Logistic regression iter. 1/29: loss=-251.79165468946542, w0=-0.785318473891755, w1=-0.7459114976744257\n",
      "Logistic regression iter. 2/29: loss=-64.29824519828743, w0=-1.0038286435295172, w1=-0.9481907954578821\n",
      "Logistic regression iter. 3/29: loss=-19.18639520636767, w0=-1.2212884025136408, w1=-1.1497097709306814\n",
      "Logistic regression iter. 4/29: loss=-8.165231272690384, w0=-1.4384955525700707, w1=-1.3510547369404455\n",
      "Logistic regression iter. 5/29: loss=-5.698937207148597, w0=-1.655639179813162, w1=-1.5523579060359405\n",
      "Logistic regression iter. 6/29: loss=-5.457061828630745, w0=-1.872766257977113, w1=-1.753650656261198\n",
      "Logistic regression iter. 7/29: loss=-5.811192210980056, w0=-2.0898888921101033, w1=-1.954940727941866\n",
      "Logistic regression iter. 8/29: loss=-6.329293416985365, w0=-2.3070103005228306, w1=-2.1562300924611857\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.52413136266778, w1=-2.357519265874152\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.741252324845315, w1=-2.558808386556423\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.958373257584775, w1=-2.760097492413779\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.1754941814962447, w1=-2.961386594031499\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.3926151027155482, w1=-3.1626756944176466\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.6097360231010103, w1=-3.3639647944408275\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.8268569432244637, w1=-3.5652538943555907\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.043977863264482, w1=-3.7665429942375623\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.2610987832776, w1=-3.9678320941094993\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.478219703281944, w1=-4.169121193978332\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.695340623283396, w1=-4.370410293846193\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-4.912461543283886, w1=-4.5716993937137484\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-5.129582463284051, w1=-4.772988493581206\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-5.346703383284107, w1=-4.974277593448632\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-5.563824303284126, w1=-5.175566693316049\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-5.7809452232841325, w1=-5.376855793183462\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-5.9980661432841345, w1=-5.578144893050874\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-6.2151870632841355, w1=-5.779433992918285\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-6.432307983284136, w1=-5.980723092785697\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-6.649428903284136, w1=-6.182012192653108\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-6.866549823284136, w1=-6.383301292520519\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-1085.3218483165924, w0=-0.5621181599999999, w1=-0.5400369677320377\n",
      "Logistic regression iter. 1/29: loss=-251.79165468946542, w0=-0.785318473891755, w1=-0.7459114976744257\n",
      "Logistic regression iter. 2/29: loss=-64.29824519828743, w0=-1.0038286435295172, w1=-0.9481907954578821\n",
      "Logistic regression iter. 3/29: loss=-19.18639520636767, w0=-1.2212884025136408, w1=-1.1497097709306814\n",
      "Logistic regression iter. 4/29: loss=-8.165231272690384, w0=-1.4384955525700707, w1=-1.3510547369404455\n",
      "Logistic regression iter. 5/29: loss=-5.698937207148597, w0=-1.655639179813162, w1=-1.5523579060359405\n",
      "Logistic regression iter. 6/29: loss=-5.457061828630745, w0=-1.872766257977113, w1=-1.753650656261198\n",
      "Logistic regression iter. 7/29: loss=-5.811192210980056, w0=-2.0898888921101033, w1=-1.954940727941866\n",
      "Logistic regression iter. 8/29: loss=-6.329293416985365, w0=-2.3070103005228306, w1=-2.1562300924611857\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.52413136266778, w1=-2.357519265874152\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.741252324845315, w1=-2.558808386556423\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-2.958373257584775, w1=-2.760097492413779\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.1754941814962447, w1=-2.961386594031499\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.3926151027155482, w1=-3.1626756944176466\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.6097360231010103, w1=-3.3639647944408275\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.8268569432244637, w1=-3.5652538943555907\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.043977863264482, w1=-3.7665429942375623\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.2610987832776, w1=-3.9678320941094993\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.478219703281944, w1=-4.169121193978332\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.695340623283396, w1=-4.370410293846193\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-4.912461543283886, w1=-4.5716993937137484\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-5.129582463284051, w1=-4.772988493581206\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-5.346703383284107, w1=-4.974277593448632\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-5.563824303284126, w1=-5.175566693316049\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-5.7809452232841325, w1=-5.376855793183462\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-5.9980661432841345, w1=-5.578144893050874\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-6.2151870632841355, w1=-5.779433992918285\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-6.432307983284136, w1=-5.980723092785697\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-6.649428903284136, w1=-6.182012192653108\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-6.866549823284136, w1=-6.383301292520519\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-971.3413933573664, w0=-0.5784114399999999, w1=-0.5556902131735459\n",
      "Logistic regression iter. 1/29: loss=-218.21499425437338, w0=-0.8074147797724175, w1=-0.7670097505591702\n",
      "Logistic regression iter. 2/29: loss=-54.309911706736436, w0=-1.0320634940107913, w1=-0.9750085421539905\n",
      "Logistic regression iter. 3/29: loss=-16.262528722971133, w0=-1.2557687511069633, w1=-1.1823283179443322\n",
      "Logistic regression iter. 4/29: loss=-7.367856939539149, w0=-1.479254653395386, w1=-1.38949796134159\n",
      "Logistic regression iter. 5/29: loss=-5.552213680964967, w0=-1.7026871923782259, w1=-1.5966327492264178\n",
      "Logistic regression iter. 6/29: loss=-5.518977585872675, w0=-1.9261062703723362, w1=-1.8037591332363696\n",
      "Logistic regression iter. 7/29: loss=-5.948563391695048, w0=-2.1495218454201654, w1=-2.0108834260969637\n",
      "Logistic regression iter. 8/29: loss=-6.5016387193272, w0=-2.372936483485927, w1=-2.218007184272683\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.596350864670096, w1=-2.425130802429655\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.8197651738383582, w1=-2.632254383130544\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.043179462401747, w1=-2.8393779536174044\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.266593744958522, w1=-3.046501521269855\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.490008025733861, w1=-3.2536250881230346\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.713422305972386, w1=-3.4607486547474657\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.9368365860467485, w1=-3.667872221305523\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.160250866070219, w1=-3.8749957878440724\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.3836651460777105, w1=-4.08211935437682\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.607079426080126, w1=-4.289242920907822\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.8304937060809126, w1=-4.496366487438293\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-5.05390798608117, w1=-4.7034900539686015\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-5.277322266081255, w1=-4.910613620498859\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-5.500736546081282, w1=-5.1177371870291015\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-5.724150826081291, w1=-5.324860753559339\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-5.947565106081294, w1=-5.531984320089574\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-6.170979386081295, w1=-5.739107886619809\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-6.394393666081295, w1=-5.946231453150044\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-6.617807946081295, w1=-6.153355019680278\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-6.841222226081295, w1=-6.360478586210513\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-7.064636506081295, w1=-6.567602152740747\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-971.3413933573664, w0=-0.5784114399999999, w1=-0.5556902131735459\n",
      "Logistic regression iter. 1/29: loss=-218.21499425437338, w0=-0.8074147797724175, w1=-0.7670097505591702\n",
      "Logistic regression iter. 2/29: loss=-54.309911706736436, w0=-1.0320634940107913, w1=-0.9750085421539905\n",
      "Logistic regression iter. 3/29: loss=-16.262528722971133, w0=-1.2557687511069633, w1=-1.1823283179443322\n",
      "Logistic regression iter. 4/29: loss=-7.367856939539149, w0=-1.479254653395386, w1=-1.38949796134159\n",
      "Logistic regression iter. 5/29: loss=-5.552213680964967, w0=-1.7026871923782259, w1=-1.5966327492264178\n",
      "Logistic regression iter. 6/29: loss=-5.518977585872675, w0=-1.9261062703723362, w1=-1.8037591332363696\n",
      "Logistic regression iter. 7/29: loss=-5.948563391695048, w0=-2.1495218454201654, w1=-2.0108834260969637\n",
      "Logistic regression iter. 8/29: loss=-6.5016387193272, w0=-2.372936483485927, w1=-2.218007184272683\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.596350864670096, w1=-2.425130802429655\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.8197651738383582, w1=-2.632254383130544\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.043179462401747, w1=-2.8393779536174044\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.266593744958522, w1=-3.046501521269855\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.490008025733861, w1=-3.2536250881230346\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.713422305972386, w1=-3.4607486547474657\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.9368365860467485, w1=-3.667872221305523\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.160250866070219, w1=-3.8749957878440724\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.3836651460777105, w1=-4.08211935437682\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.607079426080126, w1=-4.289242920907822\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.8304937060809126, w1=-4.496366487438293\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-5.05390798608117, w1=-4.7034900539686015\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-5.277322266081255, w1=-4.910613620498859\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-5.500736546081282, w1=-5.1177371870291015\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-5.724150826081291, w1=-5.324860753559339\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-5.947565106081294, w1=-5.531984320089574\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-6.170979386081295, w1=-5.739107886619809\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-6.394393666081295, w1=-5.946231453150044\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-6.617807946081295, w1=-6.153355019680278\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-6.841222226081295, w1=-6.360478586210513\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-7.064636506081295, w1=-6.567602152740747\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-971.3413933573664, w0=-0.5784114399999999, w1=-0.5556902131735459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 1/29: loss=-218.21499425437338, w0=-0.8074147797724175, w1=-0.7670097505591702\n",
      "Logistic regression iter. 2/29: loss=-54.309911706736436, w0=-1.0320634940107913, w1=-0.9750085421539905\n",
      "Logistic regression iter. 3/29: loss=-16.262528722971133, w0=-1.2557687511069633, w1=-1.1823283179443322\n",
      "Logistic regression iter. 4/29: loss=-7.367856939539149, w0=-1.479254653395386, w1=-1.38949796134159\n",
      "Logistic regression iter. 5/29: loss=-5.552213680964967, w0=-1.7026871923782259, w1=-1.5966327492264178\n",
      "Logistic regression iter. 6/29: loss=-5.518977585872675, w0=-1.9261062703723362, w1=-1.8037591332363696\n",
      "Logistic regression iter. 7/29: loss=-5.948563391695048, w0=-2.1495218454201654, w1=-2.0108834260969637\n",
      "Logistic regression iter. 8/29: loss=-6.5016387193272, w0=-2.372936483485927, w1=-2.218007184272683\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.596350864670096, w1=-2.425130802429655\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.8197651738383582, w1=-2.632254383130544\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.043179462401747, w1=-2.8393779536174044\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.266593744958522, w1=-3.046501521269855\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.490008025733861, w1=-3.2536250881230346\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.713422305972386, w1=-3.4607486547474657\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.9368365860467485, w1=-3.667872221305523\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.160250866070219, w1=-3.8749957878440724\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.3836651460777105, w1=-4.08211935437682\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.607079426080126, w1=-4.289242920907822\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.8304937060809126, w1=-4.496366487438293\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-5.05390798608117, w1=-4.7034900539686015\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-5.277322266081255, w1=-4.910613620498859\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-5.500736546081282, w1=-5.1177371870291015\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-5.724150826081291, w1=-5.324860753559339\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-5.947565106081294, w1=-5.531984320089574\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-6.170979386081295, w1=-5.739107886619809\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-6.394393666081295, w1=-5.946231453150044\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-6.617807946081295, w1=-6.153355019680278\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-6.841222226081295, w1=-6.360478586210513\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-7.064636506081295, w1=-6.567602152740747\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-971.3413933573664, w0=-0.5784114399999999, w1=-0.5556902131735459\n",
      "Logistic regression iter. 1/29: loss=-218.21499425437338, w0=-0.8074147797724175, w1=-0.7670097505591702\n",
      "Logistic regression iter. 2/29: loss=-54.309911706736436, w0=-1.0320634940107913, w1=-0.9750085421539905\n",
      "Logistic regression iter. 3/29: loss=-16.262528722971133, w0=-1.2557687511069633, w1=-1.1823283179443322\n",
      "Logistic regression iter. 4/29: loss=-7.367856939539149, w0=-1.479254653395386, w1=-1.38949796134159\n",
      "Logistic regression iter. 5/29: loss=-5.552213680964967, w0=-1.7026871923782259, w1=-1.5966327492264178\n",
      "Logistic regression iter. 6/29: loss=-5.518977585872675, w0=-1.9261062703723362, w1=-1.8037591332363696\n",
      "Logistic regression iter. 7/29: loss=-5.948563391695048, w0=-2.1495218454201654, w1=-2.0108834260969637\n",
      "Logistic regression iter. 8/29: loss=-6.5016387193272, w0=-2.372936483485927, w1=-2.218007184272683\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.596350864670096, w1=-2.425130802429655\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.8197651738383582, w1=-2.632254383130544\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.043179462401747, w1=-2.8393779536174044\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.266593744958522, w1=-3.046501521269855\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.490008025733861, w1=-3.2536250881230346\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.713422305972386, w1=-3.4607486547474657\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-3.9368365860467485, w1=-3.667872221305523\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.160250866070219, w1=-3.8749957878440724\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.3836651460777105, w1=-4.08211935437682\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.607079426080126, w1=-4.289242920907822\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.8304937060809126, w1=-4.496366487438293\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-5.05390798608117, w1=-4.7034900539686015\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-5.277322266081255, w1=-4.910613620498859\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-5.500736546081282, w1=-5.1177371870291015\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-5.724150826081291, w1=-5.324860753559339\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-5.947565106081294, w1=-5.531984320089574\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-6.170979386081295, w1=-5.739107886619809\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-6.394393666081295, w1=-5.946231453150044\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-6.617807946081295, w1=-6.153355019680278\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-6.841222226081295, w1=-6.360478586210513\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-7.064636506081295, w1=-6.567602152740747\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-869.7842324502637, w0=-0.5947047199999999, w1=-0.5713434586150543\n",
      "Logistic regression iter. 1/29: loss=-189.25485353735087, w0=-0.8295490224253034, w1=-0.7881402327988029\n",
      "Logistic regression iter. 2/29: loss=-45.99253421067806, w0=-1.060353177600026, w1=-1.001871676251075\n",
      "Logistic regression iter. 3/29: loss=-13.922957518210069, w0=-1.2903106392067323, w1=-1.214997156796403\n",
      "Logistic regression iter. 4/29: loss=-6.770316164351944, w0=-1.52007776460615, w1=-1.4279931920596536\n",
      "Logistic regression iter. 5/29: loss=-5.473071549875865, w0=-1.7498000882972753, w1=-1.6409601762842085\n",
      "Logistic regression iter. 6/29: loss=-5.603693715507488, w0=-1.9795114676004024, w1=-1.8539203846925676\n",
      "Logistic regression iter. 7/29: loss=-6.093686612659965, w0=-2.2092200864730462, w1=-2.0668789609666938\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.4389279891452493, w1=-2.2798371330155454\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.6686357012302246, w1=-2.4927952024744293\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.8983433614215515, w1=-2.7057532453229682\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.1280510071843923, w1=-2.918711281132148\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.3577586488578284, w1=-3.1316693150455364\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.5874662893515685, w1=-3.3446273484399054\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.8171739294994023, w1=-3.5575853816900067\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.04688156954427, w1=-3.770543414899438\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.276589209558059, w1=-3.9835014480972526\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.506296849562346, w1=-4.196459481291709\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.736004489563692, w1=-4.409417514485182\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.9657121295641184, w1=-4.622375547678365\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-5.195419769564254, w1=-4.835333580871461\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-5.425127409564298, w1=-5.048291614064531\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-5.654835049564311, w1=-5.261249647257593\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-5.884542689564316, w1=-5.474207680450652\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-6.1142503295643165, w1=-5.687165713643711\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-6.3439579695643165, w1=-5.900123746836769\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-6.5736656095643164, w1=-6.113081780029828\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-6.803373249564316, w1=-6.326039813222886\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-7.033080889564316, w1=-6.538997846415945\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-7.262788529564316, w1=-6.751955879609003\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-869.7842324502637, w0=-0.5947047199999999, w1=-0.5713434586150543\n",
      "Logistic regression iter. 1/29: loss=-189.25485353735087, w0=-0.8295490224253034, w1=-0.7881402327988029\n",
      "Logistic regression iter. 2/29: loss=-45.99253421067806, w0=-1.060353177600026, w1=-1.001871676251075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 3/29: loss=-13.922957518210069, w0=-1.2903106392067323, w1=-1.214997156796403\n",
      "Logistic regression iter. 4/29: loss=-6.770316164351944, w0=-1.52007776460615, w1=-1.4279931920596536\n",
      "Logistic regression iter. 5/29: loss=-5.473071549875865, w0=-1.7498000882972753, w1=-1.6409601762842085\n",
      "Logistic regression iter. 6/29: loss=-5.603693715507488, w0=-1.9795114676004024, w1=-1.8539203846925676\n",
      "Logistic regression iter. 7/29: loss=-6.093686612659965, w0=-2.2092200864730462, w1=-2.0668789609666938\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.4389279891452493, w1=-2.2798371330155454\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.6686357012302246, w1=-2.4927952024744293\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.8983433614215515, w1=-2.7057532453229682\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.1280510071843923, w1=-2.918711281132148\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.3577586488578284, w1=-3.1316693150455364\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.5874662893515685, w1=-3.3446273484399054\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.8171739294994023, w1=-3.5575853816900067\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.04688156954427, w1=-3.770543414899438\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.276589209558059, w1=-3.9835014480972526\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.506296849562346, w1=-4.196459481291709\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.736004489563692, w1=-4.409417514485182\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.9657121295641184, w1=-4.622375547678365\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-5.195419769564254, w1=-4.835333580871461\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-5.425127409564298, w1=-5.048291614064531\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-5.654835049564311, w1=-5.261249647257593\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-5.884542689564316, w1=-5.474207680450652\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-6.1142503295643165, w1=-5.687165713643711\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-6.3439579695643165, w1=-5.900123746836769\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-6.5736656095643164, w1=-6.113081780029828\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-6.803373249564316, w1=-6.326039813222886\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-7.033080889564316, w1=-6.538997846415945\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-7.262788529564316, w1=-6.751955879609003\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-869.7842324502637, w0=-0.5947047199999999, w1=-0.5713434586150543\n",
      "Logistic regression iter. 1/29: loss=-189.25485353735087, w0=-0.8295490224253034, w1=-0.7881402327988029\n",
      "Logistic regression iter. 2/29: loss=-45.99253421067806, w0=-1.060353177600026, w1=-1.001871676251075\n",
      "Logistic regression iter. 3/29: loss=-13.922957518210069, w0=-1.2903106392067323, w1=-1.214997156796403\n",
      "Logistic regression iter. 4/29: loss=-6.770316164351944, w0=-1.52007776460615, w1=-1.4279931920596536\n",
      "Logistic regression iter. 5/29: loss=-5.473071549875865, w0=-1.7498000882972753, w1=-1.6409601762842085\n",
      "Logistic regression iter. 6/29: loss=-5.603693715507488, w0=-1.9795114676004024, w1=-1.8539203846925676\n",
      "Logistic regression iter. 7/29: loss=-6.093686612659965, w0=-2.2092200864730462, w1=-2.0668789609666938\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.4389279891452493, w1=-2.2798371330155454\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.6686357012302246, w1=-2.4927952024744293\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.8983433614215515, w1=-2.7057532453229682\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.1280510071843923, w1=-2.918711281132148\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.3577586488578284, w1=-3.1316693150455364\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.5874662893515685, w1=-3.3446273484399054\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.8171739294994023, w1=-3.5575853816900067\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.04688156954427, w1=-3.770543414899438\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.276589209558059, w1=-3.9835014480972526\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.506296849562346, w1=-4.196459481291709\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.736004489563692, w1=-4.409417514485182\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.9657121295641184, w1=-4.622375547678365\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-5.195419769564254, w1=-4.835333580871461\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-5.425127409564298, w1=-5.048291614064531\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-5.654835049564311, w1=-5.261249647257593\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-5.884542689564316, w1=-5.474207680450652\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-6.1142503295643165, w1=-5.687165713643711\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-6.3439579695643165, w1=-5.900123746836769\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-6.5736656095643164, w1=-6.113081780029828\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-6.803373249564316, w1=-6.326039813222886\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-7.033080889564316, w1=-6.538997846415945\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-7.262788529564316, w1=-6.751955879609003\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-869.7842324502637, w0=-0.5947047199999999, w1=-0.5713434586150543\n",
      "Logistic regression iter. 1/29: loss=-189.25485353735087, w0=-0.8295490224253034, w1=-0.7881402327988029\n",
      "Logistic regression iter. 2/29: loss=-45.99253421067806, w0=-1.060353177600026, w1=-1.001871676251075\n",
      "Logistic regression iter. 3/29: loss=-13.922957518210069, w0=-1.2903106392067323, w1=-1.214997156796403\n",
      "Logistic regression iter. 4/29: loss=-6.770316164351944, w0=-1.52007776460615, w1=-1.4279931920596536\n",
      "Logistic regression iter. 5/29: loss=-5.473071549875865, w0=-1.7498000882972753, w1=-1.6409601762842085\n",
      "Logistic regression iter. 6/29: loss=-5.603693715507488, w0=-1.9795114676004024, w1=-1.8539203846925676\n",
      "Logistic regression iter. 7/29: loss=-6.093686612659965, w0=-2.2092200864730462, w1=-2.0668789609666938\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.4389279891452493, w1=-2.2798371330155454\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.6686357012302246, w1=-2.4927952024744293\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.8983433614215515, w1=-2.7057532453229682\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.1280510071843923, w1=-2.918711281132148\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.3577586488578284, w1=-3.1316693150455364\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.5874662893515685, w1=-3.3446273484399054\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.8171739294994023, w1=-3.5575853816900067\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.04688156954427, w1=-3.770543414899438\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.276589209558059, w1=-3.9835014480972526\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.506296849562346, w1=-4.196459481291709\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.736004489563692, w1=-4.409417514485182\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-4.9657121295641184, w1=-4.622375547678365\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-5.195419769564254, w1=-4.835333580871461\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-5.425127409564298, w1=-5.048291614064531\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-5.654835049564311, w1=-5.261249647257593\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-5.884542689564316, w1=-5.474207680450652\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-6.1142503295643165, w1=-5.687165713643711\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-6.3439579695643165, w1=-5.900123746836769\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-6.5736656095643164, w1=-6.113081780029828\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-6.803373249564316, w1=-6.326039813222886\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-7.033080889564316, w1=-6.538997846415945\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-7.262788529564316, w1=-6.751955879609003\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-779.2468768516104, w0=-0.6109979999999998, w1=-0.5869967040565626\n",
      "Logistic regression iter. 1/29: loss=-164.26735331937437, w0=-0.851718579265815, w1=-0.8093004661686233\n",
      "Logistic regression iter. 2/29: loss=-39.065452696145236, w0=-1.0886933142838262, w1=-1.028776265420216\n",
      "Logistic regression iter. 3/29: loss=-12.053438337188565, w0=-1.324908765488143, w1=-1.247711650956454\n",
      "Logistic regression iter. 4/29: loss=-6.32834537313029, w0=-1.5609591699176657, w1=-1.4665354937762416\n",
      "Logistic regression iter. 5/29: loss=-5.44490063904332, w0=-1.7969719801714636, w1=-1.6853351348161654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 6/29: loss=-5.705097699902064, w0=-2.03297589532088, w1=-1.904129314536567\n",
      "Logistic regression iter. 7/29: loss=-6.24433472610142, w0=-2.268977635573661, w1=-2.122922220636129\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.5049788283940213, w1=-2.3417148211577548\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.74097987979097, w1=-2.560507346507382\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.976980893782031, w1=-2.779299852947907\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.2129818976646725, w1=-2.9980923545353035\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.4489828987613778, w1=-3.2168848548540816\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.6849838990762462, w1=-3.4356773548356134\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.9209848991680136, w1=-3.6544698547260883\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.156985899195131, w1=-3.8732623545916205\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.392986899203247, w1=-4.092054854450228\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.628987899205703, w1=-4.310847354306889\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.8649888992064545, w1=-4.529639854162995\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-5.100989899206686, w1=-4.748432354018942\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-5.336990899206758, w1=-4.967224853874844\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-5.57299189920678, w1=-5.1860173537307315\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-5.808992899206787, w1=-5.404809853586615\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-6.044993899206789, w1=-5.6236023534424975\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-6.28099489920679, w1=-5.842394853298379\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-6.51699589920679, w1=-6.061187353154261\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-6.7529968992067895, w1=-6.2799798530101425\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-6.988997899206789, w1=-6.498772352866024\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-7.224998899206789, w1=-6.717564852721906\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-7.460999899206789, w1=-6.9363573525777875\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-779.2468768516104, w0=-0.6109979999999998, w1=-0.5869967040565626\n",
      "Logistic regression iter. 1/29: loss=-164.26735331937437, w0=-0.851718579265815, w1=-0.8093004661686233\n",
      "Logistic regression iter. 2/29: loss=-39.065452696145236, w0=-1.0886933142838262, w1=-1.028776265420216\n",
      "Logistic regression iter. 3/29: loss=-12.053438337188565, w0=-1.324908765488143, w1=-1.247711650956454\n",
      "Logistic regression iter. 4/29: loss=-6.32834537313029, w0=-1.5609591699176657, w1=-1.4665354937762416\n",
      "Logistic regression iter. 5/29: loss=-5.44490063904332, w0=-1.7969719801714636, w1=-1.6853351348161654\n",
      "Logistic regression iter. 6/29: loss=-5.705097699902064, w0=-2.03297589532088, w1=-1.904129314536567\n",
      "Logistic regression iter. 7/29: loss=-6.24433472610142, w0=-2.268977635573661, w1=-2.122922220636129\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.5049788283940213, w1=-2.3417148211577548\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.74097987979097, w1=-2.560507346507382\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.976980893782031, w1=-2.779299852947907\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.2129818976646725, w1=-2.9980923545353035\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.4489828987613778, w1=-3.2168848548540816\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.6849838990762462, w1=-3.4356773548356134\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.9209848991680136, w1=-3.6544698547260883\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.156985899195131, w1=-3.8732623545916205\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.392986899203247, w1=-4.092054854450228\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.628987899205703, w1=-4.310847354306889\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.8649888992064545, w1=-4.529639854162995\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-5.100989899206686, w1=-4.748432354018942\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-5.336990899206758, w1=-4.967224853874844\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-5.57299189920678, w1=-5.1860173537307315\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-5.808992899206787, w1=-5.404809853586615\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-6.044993899206789, w1=-5.6236023534424975\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-6.28099489920679, w1=-5.842394853298379\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-6.51699589920679, w1=-6.061187353154261\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-6.7529968992067895, w1=-6.2799798530101425\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-6.988997899206789, w1=-6.498772352866024\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-7.224998899206789, w1=-6.717564852721906\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-7.460999899206789, w1=-6.9363573525777875\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-779.2468768516104, w0=-0.6109979999999998, w1=-0.5869967040565626\n",
      "Logistic regression iter. 1/29: loss=-164.26735331937437, w0=-0.851718579265815, w1=-0.8093004661686233\n",
      "Logistic regression iter. 2/29: loss=-39.065452696145236, w0=-1.0886933142838262, w1=-1.028776265420216\n",
      "Logistic regression iter. 3/29: loss=-12.053438337188565, w0=-1.324908765488143, w1=-1.247711650956454\n",
      "Logistic regression iter. 4/29: loss=-6.32834537313029, w0=-1.5609591699176657, w1=-1.4665354937762416\n",
      "Logistic regression iter. 5/29: loss=-5.44490063904332, w0=-1.7969719801714636, w1=-1.6853351348161654\n",
      "Logistic regression iter. 6/29: loss=-5.705097699902064, w0=-2.03297589532088, w1=-1.904129314536567\n",
      "Logistic regression iter. 7/29: loss=-6.24433472610142, w0=-2.268977635573661, w1=-2.122922220636129\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.5049788283940213, w1=-2.3417148211577548\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.74097987979097, w1=-2.560507346507382\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.976980893782031, w1=-2.779299852947907\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.2129818976646725, w1=-2.9980923545353035\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.4489828987613778, w1=-3.2168848548540816\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.6849838990762462, w1=-3.4356773548356134\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.9209848991680136, w1=-3.6544698547260883\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.156985899195131, w1=-3.8732623545916205\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.392986899203247, w1=-4.092054854450228\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.628987899205703, w1=-4.310847354306889\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.8649888992064545, w1=-4.529639854162995\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-5.100989899206686, w1=-4.748432354018942\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-5.336990899206758, w1=-4.967224853874844\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-5.57299189920678, w1=-5.1860173537307315\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-5.808992899206787, w1=-5.404809853586615\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-6.044993899206789, w1=-5.6236023534424975\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-6.28099489920679, w1=-5.842394853298379\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-6.51699589920679, w1=-6.061187353154261\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-6.7529968992067895, w1=-6.2799798530101425\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-6.988997899206789, w1=-6.498772352866024\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-7.224998899206789, w1=-6.717564852721906\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-7.460999899206789, w1=-6.9363573525777875\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-779.2468768516104, w0=-0.6109979999999998, w1=-0.5869967040565626\n",
      "Logistic regression iter. 1/29: loss=-164.26735331937437, w0=-0.851718579265815, w1=-0.8093004661686233\n",
      "Logistic regression iter. 2/29: loss=-39.065452696145236, w0=-1.0886933142838262, w1=-1.028776265420216\n",
      "Logistic regression iter. 3/29: loss=-12.053438337188565, w0=-1.324908765488143, w1=-1.247711650956454\n",
      "Logistic regression iter. 4/29: loss=-6.32834537313029, w0=-1.5609591699176657, w1=-1.4665354937762416\n",
      "Logistic regression iter. 5/29: loss=-5.44490063904332, w0=-1.7969719801714636, w1=-1.6853351348161654\n",
      "Logistic regression iter. 6/29: loss=-5.705097699902064, w0=-2.03297589532088, w1=-1.904129314536567\n",
      "Logistic regression iter. 7/29: loss=-6.24433472610142, w0=-2.268977635573661, w1=-2.122922220636129\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.5049788283940213, w1=-2.3417148211577548\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.74097987979097, w1=-2.560507346507382\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-2.976980893782031, w1=-2.779299852947907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 11/29: loss=nan, w0=-3.2129818976646725, w1=-2.9980923545353035\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.4489828987613778, w1=-3.2168848548540816\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.6849838990762462, w1=-3.4356773548356134\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-3.9209848991680136, w1=-3.6544698547260883\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.156985899195131, w1=-3.8732623545916205\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.392986899203247, w1=-4.092054854450228\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.628987899205703, w1=-4.310847354306889\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.8649888992064545, w1=-4.529639854162995\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-5.100989899206686, w1=-4.748432354018942\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-5.336990899206758, w1=-4.967224853874844\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-5.57299189920678, w1=-5.1860173537307315\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-5.808992899206787, w1=-5.404809853586615\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-6.044993899206789, w1=-5.6236023534424975\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-6.28099489920679, w1=-5.842394853298379\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-6.51699589920679, w1=-6.061187353154261\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-6.7529968992067895, w1=-6.2799798530101425\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-6.988997899206789, w1=-6.498772352866024\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-7.224998899206789, w1=-6.717564852721906\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-7.460999899206789, w1=-6.9363573525777875\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-698.4903943786063, w0=-0.6272912799999999, w1=-0.6026499494980709\n",
      "Logistic regression iter. 1/29: loss=-142.69965721732265, w0=-0.8739209497799779, w1=-0.8304881246189766\n",
      "Logistic regression iter. 2/29: loss=-33.29561339779703, w0=-1.1170798096279557, w1=-1.055718677970466\n",
      "Logistic regression iter. 3/29: loss=-10.562279937489826, w0=-1.3595582356733842, w1=-1.2804675625942208\n",
      "Logistic regression iter. 4/29: loss=-6.007580438102099, w0=-1.6018936286612466, w1=-1.505120381324935\n",
      "Logistic regression iter. 5/29: loss=-5.455213923822199, w0=-1.844197489571399, w1=-1.7297530462215047\n",
      "Logistic regression iter. 6/29: loss=-5.818734470341001, w0=-2.08649412301798, w1=-1.954381310485294\n",
      "Logistic regression iter. 7/29: loss=-6.3989313197643085, w0=-2.328789043110522, w1=-2.1790085810299917\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.5710835447515947, w1=-2.403635620571842\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.813377941428941, w1=-2.628262605025078\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.055672311133431, w1=-2.8528895760381676\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.297966673752334, w1=-3.077516543704059\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.540261034472004, w1=-3.3021435105205708\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.7825553946730883, w1=-3.526770477117802\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.02484975473014, w1=-3.7513974436575155\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.267144114746556, w1=-3.976024410181917\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.509438474751341, w1=-4.2006513767021865\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.751732834752751, w1=-4.4252783432213265\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.994027194753171, w1=-4.649905309740154\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-5.236321554753297, w1=-4.874532276258894\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-5.478615914753335, w1=-5.09915924277761\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-5.720910274753346, w1=-5.323786209296318\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-5.96320463475335, w1=-5.548413175815024\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-6.20549899475335, w1=-5.77304014233373\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-6.44779335475335, w1=-5.997667108852435\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-6.69008771475335, w1=-6.222294075371141\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-6.93238207475335, w1=-6.446921041889847\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-7.17467643475335, w1=-6.6715480084085526\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-7.416970794753349, w1=-6.896174974927258\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-7.659265154753349, w1=-7.120801941445964\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-698.4903943786063, w0=-0.6272912799999999, w1=-0.6026499494980709\n",
      "Logistic regression iter. 1/29: loss=-142.69965721732265, w0=-0.8739209497799779, w1=-0.8304881246189766\n",
      "Logistic regression iter. 2/29: loss=-33.29561339779703, w0=-1.1170798096279557, w1=-1.055718677970466\n",
      "Logistic regression iter. 3/29: loss=-10.562279937489826, w0=-1.3595582356733842, w1=-1.2804675625942208\n",
      "Logistic regression iter. 4/29: loss=-6.007580438102099, w0=-1.6018936286612466, w1=-1.505120381324935\n",
      "Logistic regression iter. 5/29: loss=-5.455213923822199, w0=-1.844197489571399, w1=-1.7297530462215047\n",
      "Logistic regression iter. 6/29: loss=-5.818734470341001, w0=-2.08649412301798, w1=-1.954381310485294\n",
      "Logistic regression iter. 7/29: loss=-6.3989313197643085, w0=-2.328789043110522, w1=-2.1790085810299917\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.5710835447515947, w1=-2.403635620571842\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.813377941428941, w1=-2.628262605025078\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.055672311133431, w1=-2.8528895760381676\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.297966673752334, w1=-3.077516543704059\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.540261034472004, w1=-3.3021435105205708\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.7825553946730883, w1=-3.526770477117802\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.02484975473014, w1=-3.7513974436575155\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.267144114746556, w1=-3.976024410181917\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.509438474751341, w1=-4.2006513767021865\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.751732834752751, w1=-4.4252783432213265\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.994027194753171, w1=-4.649905309740154\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-5.236321554753297, w1=-4.874532276258894\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-5.478615914753335, w1=-5.09915924277761\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-5.720910274753346, w1=-5.323786209296318\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-5.96320463475335, w1=-5.548413175815024\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-6.20549899475335, w1=-5.77304014233373\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-6.44779335475335, w1=-5.997667108852435\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-6.69008771475335, w1=-6.222294075371141\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-6.93238207475335, w1=-6.446921041889847\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-7.17467643475335, w1=-6.6715480084085526\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-7.416970794753349, w1=-6.896174974927258\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-7.659265154753349, w1=-7.120801941445964\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-698.4903943786063, w0=-0.6272912799999999, w1=-0.6026499494980709\n",
      "Logistic regression iter. 1/29: loss=-142.69965721732265, w0=-0.8739209497799779, w1=-0.8304881246189766\n",
      "Logistic regression iter. 2/29: loss=-33.29561339779703, w0=-1.1170798096279557, w1=-1.055718677970466\n",
      "Logistic regression iter. 3/29: loss=-10.562279937489826, w0=-1.3595582356733842, w1=-1.2804675625942208\n",
      "Logistic regression iter. 4/29: loss=-6.007580438102099, w0=-1.6018936286612466, w1=-1.505120381324935\n",
      "Logistic regression iter. 5/29: loss=-5.455213923822199, w0=-1.844197489571399, w1=-1.7297530462215047\n",
      "Logistic regression iter. 6/29: loss=-5.818734470341001, w0=-2.08649412301798, w1=-1.954381310485294\n",
      "Logistic regression iter. 7/29: loss=-6.3989313197643085, w0=-2.328789043110522, w1=-2.1790085810299917\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.5710835447515947, w1=-2.403635620571842\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.813377941428941, w1=-2.628262605025078\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.055672311133431, w1=-2.8528895760381676\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.297966673752334, w1=-3.077516543704059\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.540261034472004, w1=-3.3021435105205708\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.7825553946730883, w1=-3.526770477117802\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.02484975473014, w1=-3.7513974436575155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 15/29: loss=nan, w0=-4.267144114746556, w1=-3.976024410181917\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.509438474751341, w1=-4.2006513767021865\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.751732834752751, w1=-4.4252783432213265\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.994027194753171, w1=-4.649905309740154\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-5.236321554753297, w1=-4.874532276258894\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-5.478615914753335, w1=-5.09915924277761\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-5.720910274753346, w1=-5.323786209296318\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-5.96320463475335, w1=-5.548413175815024\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-6.20549899475335, w1=-5.77304014233373\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-6.44779335475335, w1=-5.997667108852435\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-6.69008771475335, w1=-6.222294075371141\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-6.93238207475335, w1=-6.446921041889847\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-7.17467643475335, w1=-6.6715480084085526\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-7.416970794753349, w1=-6.896174974927258\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-7.659265154753349, w1=-7.120801941445964\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-698.4903943786063, w0=-0.6272912799999999, w1=-0.6026499494980709\n",
      "Logistic regression iter. 1/29: loss=-142.69965721732265, w0=-0.8739209497799779, w1=-0.8304881246189766\n",
      "Logistic regression iter. 2/29: loss=-33.29561339779703, w0=-1.1170798096279557, w1=-1.055718677970466\n",
      "Logistic regression iter. 3/29: loss=-10.562279937489826, w0=-1.3595582356733842, w1=-1.2804675625942208\n",
      "Logistic regression iter. 4/29: loss=-6.007580438102099, w0=-1.6018936286612466, w1=-1.505120381324935\n",
      "Logistic regression iter. 5/29: loss=-5.455213923822199, w0=-1.844197489571399, w1=-1.7297530462215047\n",
      "Logistic regression iter. 6/29: loss=-5.818734470341001, w0=-2.08649412301798, w1=-1.954381310485294\n",
      "Logistic regression iter. 7/29: loss=-6.3989313197643085, w0=-2.328789043110522, w1=-2.1790085810299917\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.5710835447515947, w1=-2.403635620571842\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.813377941428941, w1=-2.628262605025078\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.055672311133431, w1=-2.8528895760381676\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.297966673752334, w1=-3.077516543704059\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.540261034472004, w1=-3.3021435105205708\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.7825553946730883, w1=-3.526770477117802\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.02484975473014, w1=-3.7513974436575155\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.267144114746556, w1=-3.976024410181917\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.509438474751341, w1=-4.2006513767021865\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.751732834752751, w1=-4.4252783432213265\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-4.994027194753171, w1=-4.649905309740154\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-5.236321554753297, w1=-4.874532276258894\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-5.478615914753335, w1=-5.09915924277761\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-5.720910274753346, w1=-5.323786209296318\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-5.96320463475335, w1=-5.548413175815024\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-6.20549899475335, w1=-5.77304014233373\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-6.44779335475335, w1=-5.997667108852435\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-6.69008771475335, w1=-6.222294075371141\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-6.93238207475335, w1=-6.446921041889847\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-7.17467643475335, w1=-6.6715480084085526\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-7.416970794753349, w1=-6.896174974927258\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-7.659265154753349, w1=-7.120801941445964\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-626.4203816022136, w0=-0.6435845599999999, w1=-0.6183031949395793\n",
      "Logistic regression iter. 1/29: loss=-124.0769602347748, w0=-0.8961537621449099, w1=-0.8517010330253649\n",
      "Logistic regression iter. 2/29: loss=-28.489437840116338, w0=-1.1455088504471966, w1=-1.0826955692358833\n",
      "Logistic regression iter. 3/29: loss=-9.37585686057882, w0=-1.3942545435068607, w1=-1.3132610261361082\n",
      "Logistic regression iter. 4/29: loss=-5.781323920528028, w0=-1.6428763457810753, w1=-1.5437437845938002\n",
      "Logistic regression iter. 5/29: loss=-5.494614982795978, w0=-1.8914717106120695, w1=-1.7742097658634703\n",
      "Logistic regression iter. 6/29: loss=-5.941352785022787, w0=-2.14006120429187, w1=-2.004672201947101\n",
      "Logistic regression iter. 7/29: loss=-6.556358498033956, w0=-2.3886493483018456, w1=-2.2351338627673107\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.6372371724212074, w1=-2.465595348952748\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.8858249186170553, w1=-2.6960567947604708\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.1344126453551078, w1=-2.9265182310123787\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.3830003671236417, w1=-3.1569796649547297\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.631588087596506, w1=-3.387441098328074\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.8801758077251054, w1=-3.6179025315587414\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.128763527760628, w1=-3.8483639647530463\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.377351247770582, w1=-4.078825397937943\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.625938967773407, w1=-4.309286831120371\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.8745266877742175, w1=-4.539748264302142\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-5.123114407774452, w1=-4.770209697483738\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-5.37170212777452, w1=-5.000671130665285\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-5.62028984777454, w1=-5.23113256384682\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-5.868877567774546, w1=-5.46159399702835\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-6.1174652877745475, w1=-5.692055430209879\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-6.366053007774548, w1=-5.922516863391408\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-6.614640727774548, w1=-6.152978296572937\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-6.8632284477745475, w1=-6.383439729754466\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-7.111816167774547, w1=-6.613901162935995\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-7.360403887774547, w1=-6.844362596117524\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-7.608991607774547, w1=-7.0748240292990525\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-7.8575793277745465, w1=-7.305285462480581\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-626.4203816022136, w0=-0.6435845599999999, w1=-0.6183031949395793\n",
      "Logistic regression iter. 1/29: loss=-124.0769602347748, w0=-0.8961537621449099, w1=-0.8517010330253649\n",
      "Logistic regression iter. 2/29: loss=-28.489437840116338, w0=-1.1455088504471966, w1=-1.0826955692358833\n",
      "Logistic regression iter. 3/29: loss=-9.37585686057882, w0=-1.3942545435068607, w1=-1.3132610261361082\n",
      "Logistic regression iter. 4/29: loss=-5.781323920528028, w0=-1.6428763457810753, w1=-1.5437437845938002\n",
      "Logistic regression iter. 5/29: loss=-5.494614982795978, w0=-1.8914717106120695, w1=-1.7742097658634703\n",
      "Logistic regression iter. 6/29: loss=-5.941352785022787, w0=-2.14006120429187, w1=-2.004672201947101\n",
      "Logistic regression iter. 7/29: loss=-6.556358498033956, w0=-2.3886493483018456, w1=-2.2351338627673107\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.6372371724212074, w1=-2.465595348952748\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.8858249186170553, w1=-2.6960567947604708\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.1344126453551078, w1=-2.9265182310123787\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.3830003671236417, w1=-3.1569796649547297\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.631588087596506, w1=-3.387441098328074\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.8801758077251054, w1=-3.6179025315587414\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.128763527760628, w1=-3.8483639647530463\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.377351247770582, w1=-4.078825397937943\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.625938967773407, w1=-4.309286831120371\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.8745266877742175, w1=-4.539748264302142\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-5.123114407774452, w1=-4.770209697483738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 19/29: loss=nan, w0=-5.37170212777452, w1=-5.000671130665285\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-5.62028984777454, w1=-5.23113256384682\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-5.868877567774546, w1=-5.46159399702835\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-6.1174652877745475, w1=-5.692055430209879\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-6.366053007774548, w1=-5.922516863391408\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-6.614640727774548, w1=-6.152978296572937\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-6.8632284477745475, w1=-6.383439729754466\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-7.111816167774547, w1=-6.613901162935995\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-7.360403887774547, w1=-6.844362596117524\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-7.608991607774547, w1=-7.0748240292990525\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-7.8575793277745465, w1=-7.305285462480581\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-626.4203816022136, w0=-0.6435845599999999, w1=-0.6183031949395793\n",
      "Logistic regression iter. 1/29: loss=-124.0769602347748, w0=-0.8961537621449099, w1=-0.8517010330253649\n",
      "Logistic regression iter. 2/29: loss=-28.489437840116338, w0=-1.1455088504471966, w1=-1.0826955692358833\n",
      "Logistic regression iter. 3/29: loss=-9.37585686057882, w0=-1.3942545435068607, w1=-1.3132610261361082\n",
      "Logistic regression iter. 4/29: loss=-5.781323920528028, w0=-1.6428763457810753, w1=-1.5437437845938002\n",
      "Logistic regression iter. 5/29: loss=-5.494614982795978, w0=-1.8914717106120695, w1=-1.7742097658634703\n",
      "Logistic regression iter. 6/29: loss=-5.941352785022787, w0=-2.14006120429187, w1=-2.004672201947101\n",
      "Logistic regression iter. 7/29: loss=-6.556358498033956, w0=-2.3886493483018456, w1=-2.2351338627673107\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.6372371724212074, w1=-2.465595348952748\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.8858249186170553, w1=-2.6960567947604708\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.1344126453551078, w1=-2.9265182310123787\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.3830003671236417, w1=-3.1569796649547297\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.631588087596506, w1=-3.387441098328074\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.8801758077251054, w1=-3.6179025315587414\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.128763527760628, w1=-3.8483639647530463\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.377351247770582, w1=-4.078825397937943\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.625938967773407, w1=-4.309286831120371\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.8745266877742175, w1=-4.539748264302142\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-5.123114407774452, w1=-4.770209697483738\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-5.37170212777452, w1=-5.000671130665285\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-5.62028984777454, w1=-5.23113256384682\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-5.868877567774546, w1=-5.46159399702835\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-6.1174652877745475, w1=-5.692055430209879\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-6.366053007774548, w1=-5.922516863391408\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-6.614640727774548, w1=-6.152978296572937\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-6.8632284477745475, w1=-6.383439729754466\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-7.111816167774547, w1=-6.613901162935995\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-7.360403887774547, w1=-6.844362596117524\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-7.608991607774547, w1=-7.0748240292990525\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-7.8575793277745465, w1=-7.305285462480581\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-626.4203816022136, w0=-0.6435845599999999, w1=-0.6183031949395793\n",
      "Logistic regression iter. 1/29: loss=-124.0769602347748, w0=-0.8961537621449099, w1=-0.8517010330253649\n",
      "Logistic regression iter. 2/29: loss=-28.489437840116338, w0=-1.1455088504471966, w1=-1.0826955692358833\n",
      "Logistic regression iter. 3/29: loss=-9.37585686057882, w0=-1.3942545435068607, w1=-1.3132610261361082\n",
      "Logistic regression iter. 4/29: loss=-5.781323920528028, w0=-1.6428763457810753, w1=-1.5437437845938002\n",
      "Logistic regression iter. 5/29: loss=-5.494614982795978, w0=-1.8914717106120695, w1=-1.7742097658634703\n",
      "Logistic regression iter. 6/29: loss=-5.941352785022787, w0=-2.14006120429187, w1=-2.004672201947101\n",
      "Logistic regression iter. 7/29: loss=-6.556358498033956, w0=-2.3886493483018456, w1=-2.2351338627673107\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.6372371724212074, w1=-2.465595348952748\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.8858249186170553, w1=-2.6960567947604708\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.1344126453551078, w1=-2.9265182310123787\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.3830003671236417, w1=-3.1569796649547297\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.631588087596506, w1=-3.387441098328074\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.8801758077251054, w1=-3.6179025315587414\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.128763527760628, w1=-3.8483639647530463\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.377351247770582, w1=-4.078825397937943\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.625938967773407, w1=-4.309286831120371\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.8745266877742175, w1=-4.539748264302142\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-5.123114407774452, w1=-4.770209697483738\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-5.37170212777452, w1=-5.000671130665285\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-5.62028984777454, w1=-5.23113256384682\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-5.868877567774546, w1=-5.46159399702835\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-6.1174652877745475, w1=-5.692055430209879\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-6.366053007774548, w1=-5.922516863391408\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-6.614640727774548, w1=-6.152978296572937\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-6.8632284477745475, w1=-6.383439729754466\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-7.111816167774547, w1=-6.613901162935995\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-7.360403887774547, w1=-6.844362596117524\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-7.608991607774547, w1=-7.0748240292990525\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-7.8575793277745465, w1=-7.305285462480581\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-562.0694645036557, w0=-0.6598778399999998, w1=-0.6339564403810876\n",
      "Logistic regression iter. 1/29: loss=-107.9913515419706, w0=-0.9184147764315791, w1=-0.8729371638300811\n",
      "Logistic regression iter. 2/29: loss=-24.486085234661648, w0=-1.1739768967748523, w1=-1.1097038662197654\n",
      "Logistic regression iter. 3/29: loss=-8.435020418877697, w0=-1.4289935494696535, w1=-1.3460885215407532\n",
      "Logistic regression iter. 4/29: loss=-5.628819457499332, w0=-1.6839029412242987, w1=-1.5824020145394582\n",
      "Logistic regression iter. 5/29: loss=-5.556026066792056, w0=-1.9387901741294633, w1=-1.8187015452833548\n",
      "Logistic regression iter. 6/29: loss=-6.070576782269856, w0=-2.193672638471162, w1=-2.054998220465758\n",
      "Logistic regression iter. 7/29: loss=-6.7158219382915005, w0=-2.448554039643393, w1=-2.2912942908399514\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.703435196235973, w1=-2.527590229181575\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.958316294961997, w1=-2.7638861379217388\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.2131973796451425, w1=-3.000182039865518\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.468078460840804, w1=-3.236477940214968\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.7229595411518996, w1=-3.4727738401830095\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.977840621234257, w1=-3.70906974005815\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.232721701256407, w1=-3.9453656399102823\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.487602781262451, w1=-4.181661539756629\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.742483861264121, w1=-4.417957439601499\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.997364941264588, w1=-4.654253339445987\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-5.252246021264719, w1=-4.890549239290375\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-5.507127101264756, w1=-5.126845139134737\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-5.7620081812647665, w1=-5.363141038979092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 21/29: loss=nan, w0=-6.016889261264769, w1=-5.599436938823445\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-6.271770341264769, w1=-5.835732838667798\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-6.52665142126477, w1=-6.07202873851215\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-6.78153250126477, w1=-6.308324638356502\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-7.036413581264769, w1=-6.544620538200854\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-7.291294661264769, w1=-6.780916438045206\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-7.5461757412647685, w1=-7.017212337889558\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-7.801056821264768, w1=-7.25350823773391\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-8.055937901264768, w1=-7.489804137578262\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-562.0694645036557, w0=-0.6598778399999998, w1=-0.6339564403810876\n",
      "Logistic regression iter. 1/29: loss=-107.9913515419706, w0=-0.9184147764315791, w1=-0.8729371638300811\n",
      "Logistic regression iter. 2/29: loss=-24.486085234661648, w0=-1.1739768967748523, w1=-1.1097038662197654\n",
      "Logistic regression iter. 3/29: loss=-8.435020418877697, w0=-1.4289935494696535, w1=-1.3460885215407532\n",
      "Logistic regression iter. 4/29: loss=-5.628819457499332, w0=-1.6839029412242987, w1=-1.5824020145394582\n",
      "Logistic regression iter. 5/29: loss=-5.556026066792056, w0=-1.9387901741294633, w1=-1.8187015452833548\n",
      "Logistic regression iter. 6/29: loss=-6.070576782269856, w0=-2.193672638471162, w1=-2.054998220465758\n",
      "Logistic regression iter. 7/29: loss=-6.7158219382915005, w0=-2.448554039643393, w1=-2.2912942908399514\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.703435196235973, w1=-2.527590229181575\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.958316294961997, w1=-2.7638861379217388\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.2131973796451425, w1=-3.000182039865518\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.468078460840804, w1=-3.236477940214968\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.7229595411518996, w1=-3.4727738401830095\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.977840621234257, w1=-3.70906974005815\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.232721701256407, w1=-3.9453656399102823\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.487602781262451, w1=-4.181661539756629\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.742483861264121, w1=-4.417957439601499\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.997364941264588, w1=-4.654253339445987\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-5.252246021264719, w1=-4.890549239290375\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-5.507127101264756, w1=-5.126845139134737\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-5.7620081812647665, w1=-5.363141038979092\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-6.016889261264769, w1=-5.599436938823445\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-6.271770341264769, w1=-5.835732838667798\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-6.52665142126477, w1=-6.07202873851215\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-6.78153250126477, w1=-6.308324638356502\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-7.036413581264769, w1=-6.544620538200854\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-7.291294661264769, w1=-6.780916438045206\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-7.5461757412647685, w1=-7.017212337889558\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-7.801056821264768, w1=-7.25350823773391\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-8.055937901264768, w1=-7.489804137578262\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-562.0694645036557, w0=-0.6598778399999998, w1=-0.6339564403810876\n",
      "Logistic regression iter. 1/29: loss=-107.9913515419706, w0=-0.9184147764315791, w1=-0.8729371638300811\n",
      "Logistic regression iter. 2/29: loss=-24.486085234661648, w0=-1.1739768967748523, w1=-1.1097038662197654\n",
      "Logistic regression iter. 3/29: loss=-8.435020418877697, w0=-1.4289935494696535, w1=-1.3460885215407532\n",
      "Logistic regression iter. 4/29: loss=-5.628819457499332, w0=-1.6839029412242987, w1=-1.5824020145394582\n",
      "Logistic regression iter. 5/29: loss=-5.556026066792056, w0=-1.9387901741294633, w1=-1.8187015452833548\n",
      "Logistic regression iter. 6/29: loss=-6.070576782269856, w0=-2.193672638471162, w1=-2.054998220465758\n",
      "Logistic regression iter. 7/29: loss=-6.7158219382915005, w0=-2.448554039643393, w1=-2.2912942908399514\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.703435196235973, w1=-2.527590229181575\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.958316294961997, w1=-2.7638861379217388\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.2131973796451425, w1=-3.000182039865518\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.468078460840804, w1=-3.236477940214968\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.7229595411518996, w1=-3.4727738401830095\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.977840621234257, w1=-3.70906974005815\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.232721701256407, w1=-3.9453656399102823\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.487602781262451, w1=-4.181661539756629\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.742483861264121, w1=-4.417957439601499\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.997364941264588, w1=-4.654253339445987\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-5.252246021264719, w1=-4.890549239290375\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-5.507127101264756, w1=-5.126845139134737\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-5.7620081812647665, w1=-5.363141038979092\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-6.016889261264769, w1=-5.599436938823445\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-6.271770341264769, w1=-5.835732838667798\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-6.52665142126477, w1=-6.07202873851215\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-6.78153250126477, w1=-6.308324638356502\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-7.036413581264769, w1=-6.544620538200854\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-7.291294661264769, w1=-6.780916438045206\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-7.5461757412647685, w1=-7.017212337889558\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-7.801056821264768, w1=-7.25350823773391\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-8.055937901264768, w1=-7.489804137578262\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-562.0694645036557, w0=-0.6598778399999998, w1=-0.6339564403810876\n",
      "Logistic regression iter. 1/29: loss=-107.9913515419706, w0=-0.9184147764315791, w1=-0.8729371638300811\n",
      "Logistic regression iter. 2/29: loss=-24.486085234661648, w0=-1.1739768967748523, w1=-1.1097038662197654\n",
      "Logistic regression iter. 3/29: loss=-8.435020418877697, w0=-1.4289935494696535, w1=-1.3460885215407532\n",
      "Logistic regression iter. 4/29: loss=-5.628819457499332, w0=-1.6839029412242987, w1=-1.5824020145394582\n",
      "Logistic regression iter. 5/29: loss=-5.556026066792056, w0=-1.9387901741294633, w1=-1.8187015452833548\n",
      "Logistic regression iter. 6/29: loss=-6.070576782269856, w0=-2.193672638471162, w1=-2.054998220465758\n",
      "Logistic regression iter. 7/29: loss=-6.7158219382915005, w0=-2.448554039643393, w1=-2.2912942908399514\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.703435196235973, w1=-2.527590229181575\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-2.958316294961997, w1=-2.7638861379217388\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.2131973796451425, w1=-3.000182039865518\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.468078460840804, w1=-3.236477940214968\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.7229595411518996, w1=-3.4727738401830095\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-3.977840621234257, w1=-3.70906974005815\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.232721701256407, w1=-3.9453656399102823\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.487602781262451, w1=-4.181661539756629\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.742483861264121, w1=-4.417957439601499\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-4.997364941264588, w1=-4.654253339445987\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-5.252246021264719, w1=-4.890549239290375\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-5.507127101264756, w1=-5.126845139134737\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-5.7620081812647665, w1=-5.363141038979092\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-6.016889261264769, w1=-5.599436938823445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 22/29: loss=nan, w0=-6.271770341264769, w1=-5.835732838667798\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-6.52665142126477, w1=-6.07202873851215\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-6.78153250126477, w1=-6.308324638356502\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-7.036413581264769, w1=-6.544620538200854\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-7.291294661264769, w1=-6.780916438045206\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-7.5461757412647685, w1=-7.017212337889558\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-7.801056821264768, w1=-7.25350823773391\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-8.055937901264768, w1=-7.489804137578262\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-504.58199704235057, w0=-0.6761711199999998, w1=-0.6496096858225959\n",
      "Logistic regression iter. 1/29: loss=-94.09227952592165, w0=-0.9407018851573212, w1=-0.8941946321639095\n",
      "Logistic regression iter. 2/29: loss=-21.151869586460517, w0=-1.2024806712211433, w1=-1.1367407512620842\n",
      "Logistic regression iter. 3/29: loss=-7.692227010452604, w0=-1.463771458272372, w1=-1.3789468479513913\n",
      "Logistic regression iter. 4/29: loss=-5.533917479200713, w0=-1.7249694195443628, w1=-1.6210917307651125\n",
      "Logistic regression iter. 5/29: loss=-5.634110551504251, w0=-1.9861488130607188, w1=-1.8632249967315788\n",
      "Logistic regression iter. 6/29: loss=-6.204667908593089, w0=-2.247324333993276, w1=-2.1053559628850156\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.508499017411575, w1=-2.3474864572067187\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.769673513796166, w1=-2.589616851692866\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.0308479671941853, w1=-2.831747224472431\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.2920224104524265, w1=-3.073877592416414\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.5531968512617116, w1=-3.316007959259259\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.81437129146664, w1=-3.558138325846285\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.075545731519456, w1=-3.8002686923727755\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.336720171533288, w1=-4.042399058884696\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.597894611536963, w1=-4.284529425393054\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.859069051537952, w1=-4.526659791900529\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.120243491538221, w1=-4.768790158407781\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-5.381417931538294, w1=-5.010920524914977\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-5.642592371538314, w1=-5.253050891422158\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-5.90376681153832, w1=-5.495181257929335\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-6.1649412515383215, w1=-5.737311624436511\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-6.426115691538322, w1=-5.979441990943688\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-6.687290131538322, w1=-6.221572357450864\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-6.948464571538322, w1=-6.46370272395804\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-7.2096390115383215, w1=-6.705833090465216\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-7.470813451538321, w1=-6.947963456972392\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-7.731987891538321, w1=-7.190093823479568\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-7.99316233153832, w1=-7.432224189986744\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-8.25433677153832, w1=-7.67435455649392\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-504.58199704235057, w0=-0.6761711199999998, w1=-0.6496096858225959\n",
      "Logistic regression iter. 1/29: loss=-94.09227952592165, w0=-0.9407018851573212, w1=-0.8941946321639095\n",
      "Logistic regression iter. 2/29: loss=-21.151869586460517, w0=-1.2024806712211433, w1=-1.1367407512620842\n",
      "Logistic regression iter. 3/29: loss=-7.692227010452604, w0=-1.463771458272372, w1=-1.3789468479513913\n",
      "Logistic regression iter. 4/29: loss=-5.533917479200713, w0=-1.7249694195443628, w1=-1.6210917307651125\n",
      "Logistic regression iter. 5/29: loss=-5.634110551504251, w0=-1.9861488130607188, w1=-1.8632249967315788\n",
      "Logistic regression iter. 6/29: loss=-6.204667908593089, w0=-2.247324333993276, w1=-2.1053559628850156\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.508499017411575, w1=-2.3474864572067187\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.769673513796166, w1=-2.589616851692866\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.0308479671941853, w1=-2.831747224472431\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.2920224104524265, w1=-3.073877592416414\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.5531968512617116, w1=-3.316007959259259\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.81437129146664, w1=-3.558138325846285\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.075545731519456, w1=-3.8002686923727755\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.336720171533288, w1=-4.042399058884696\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.597894611536963, w1=-4.284529425393054\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.859069051537952, w1=-4.526659791900529\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.120243491538221, w1=-4.768790158407781\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-5.381417931538294, w1=-5.010920524914977\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-5.642592371538314, w1=-5.253050891422158\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-5.90376681153832, w1=-5.495181257929335\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-6.1649412515383215, w1=-5.737311624436511\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-6.426115691538322, w1=-5.979441990943688\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-6.687290131538322, w1=-6.221572357450864\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-6.948464571538322, w1=-6.46370272395804\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-7.2096390115383215, w1=-6.705833090465216\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-7.470813451538321, w1=-6.947963456972392\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-7.731987891538321, w1=-7.190093823479568\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-7.99316233153832, w1=-7.432224189986744\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-8.25433677153832, w1=-7.67435455649392\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-504.58199704235057, w0=-0.6761711199999998, w1=-0.6496096858225959\n",
      "Logistic regression iter. 1/29: loss=-94.09227952592165, w0=-0.9407018851573212, w1=-0.8941946321639095\n",
      "Logistic regression iter. 2/29: loss=-21.151869586460517, w0=-1.2024806712211433, w1=-1.1367407512620842\n",
      "Logistic regression iter. 3/29: loss=-7.692227010452604, w0=-1.463771458272372, w1=-1.3789468479513913\n",
      "Logistic regression iter. 4/29: loss=-5.533917479200713, w0=-1.7249694195443628, w1=-1.6210917307651125\n",
      "Logistic regression iter. 5/29: loss=-5.634110551504251, w0=-1.9861488130607188, w1=-1.8632249967315788\n",
      "Logistic regression iter. 6/29: loss=-6.204667908593089, w0=-2.247324333993276, w1=-2.1053559628850156\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.508499017411575, w1=-2.3474864572067187\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.769673513796166, w1=-2.589616851692866\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.0308479671941853, w1=-2.831747224472431\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.2920224104524265, w1=-3.073877592416414\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.5531968512617116, w1=-3.316007959259259\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.81437129146664, w1=-3.558138325846285\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.075545731519456, w1=-3.8002686923727755\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.336720171533288, w1=-4.042399058884696\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.597894611536963, w1=-4.284529425393054\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.859069051537952, w1=-4.526659791900529\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.120243491538221, w1=-4.768790158407781\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-5.381417931538294, w1=-5.010920524914977\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-5.642592371538314, w1=-5.253050891422158\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-5.90376681153832, w1=-5.495181257929335\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-6.1649412515383215, w1=-5.737311624436511\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-6.426115691538322, w1=-5.979441990943688\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-6.687290131538322, w1=-6.221572357450864\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-6.948464571538322, w1=-6.46370272395804\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-7.2096390115383215, w1=-6.705833090465216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 26/29: loss=nan, w0=-7.470813451538321, w1=-6.947963456972392\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-7.731987891538321, w1=-7.190093823479568\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-7.99316233153832, w1=-7.432224189986744\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-8.25433677153832, w1=-7.67435455649392\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-504.58199704235057, w0=-0.6761711199999998, w1=-0.6496096858225959\n",
      "Logistic regression iter. 1/29: loss=-94.09227952592165, w0=-0.9407018851573212, w1=-0.8941946321639095\n",
      "Logistic regression iter. 2/29: loss=-21.151869586460517, w0=-1.2024806712211433, w1=-1.1367407512620842\n",
      "Logistic regression iter. 3/29: loss=-7.692227010452604, w0=-1.463771458272372, w1=-1.3789468479513913\n",
      "Logistic regression iter. 4/29: loss=-5.533917479200713, w0=-1.7249694195443628, w1=-1.6210917307651125\n",
      "Logistic regression iter. 5/29: loss=-5.634110551504251, w0=-1.9861488130607188, w1=-1.8632249967315788\n",
      "Logistic regression iter. 6/29: loss=-6.204667908593089, w0=-2.247324333993276, w1=-2.1053559628850156\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.508499017411575, w1=-2.3474864572067187\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.769673513796166, w1=-2.589616851692866\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.0308479671941853, w1=-2.831747224472431\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.2920224104524265, w1=-3.073877592416414\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.5531968512617116, w1=-3.316007959259259\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.81437129146664, w1=-3.558138325846285\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.075545731519456, w1=-3.8002686923727755\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.336720171533288, w1=-4.042399058884696\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.597894611536963, w1=-4.284529425393054\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.859069051537952, w1=-4.526659791900529\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.120243491538221, w1=-4.768790158407781\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-5.381417931538294, w1=-5.010920524914977\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-5.642592371538314, w1=-5.253050891422158\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-5.90376681153832, w1=-5.495181257929335\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-6.1649412515383215, w1=-5.737311624436511\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-6.426115691538322, w1=-5.979441990943688\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-6.687290131538322, w1=-6.221572357450864\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-6.948464571538322, w1=-6.46370272395804\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-7.2096390115383215, w1=-6.705833090465216\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-7.470813451538321, w1=-6.947963456972392\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-7.731987891538321, w1=-7.190093823479568\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-7.99316233153832, w1=-7.432224189986744\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-8.25433677153832, w1=-7.67435455649392\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-453.20067170642886, w0=-0.6924643999999999, w1=-0.6652629312641043\n",
      "Logistic regression iter. 1/29: loss=-82.07838717965923, w0=-0.9630131118112559, w1=-0.9154716899116018\n",
      "Logistic regression iter. 2/29: loss=-18.375633597655906, w0=-1.2310171465774329, w1=-1.16380364530823\n",
      "Logistic regression iter. 3/29: loss=-7.109239916706053, w0=-1.498584795905622, w1=-1.411833098185457\n",
      "Logistic regression iter. 4/29: loss=-5.48404298229282, w0=-1.7660721402977535, w1=-1.6598099109423012\n",
      "Logistic regression iter. 5/29: loss=-5.724840470571531, w0=-2.033543929426212, w1=-1.9077770601500355\n",
      "Logistic regression iter. 6/29: loss=-6.342352161891892, w0=-2.3010125738284786, w1=-2.155742357276819\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.5684805584270007, w1=-2.4037072862965716\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.835948399966106, w1=-2.651672139814366\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.1034162095605478, w1=-2.899636977410427\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.3708840118297374, w1=-3.147601811564634\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.638351812378129, w1=-3.3955666449579343\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.9058196125132905, w1=-3.6435314781795416\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.173287412547206, w1=-3.8914963113616734\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.440755212555857, w1=-4.13946114453457\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.708223012558095, w1=-4.387425977705272\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.975690812558682, w1=-4.635390810875443\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.243158612558837, w1=-4.883355644045485\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-5.510626412558878, w1=-5.131320477215495\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-5.778094212558889, w1=-5.379285310385497\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-6.045562012558892, w1=-5.627250143555497\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-6.313029812558892, w1=-5.875214976725497\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-6.580497612558893, w1=-6.123179809895496\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-6.847965412558893, w1=-6.371144643065495\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-7.1154332125588935, w1=-6.6191094762354945\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-7.382901012558894, w1=-6.867074309405494\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-7.650368812558894, w1=-7.115039142575493\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-7.917836612558895, w1=-7.363003975745492\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-8.185304412558894, w1=-7.610968808915492\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-8.452772212558894, w1=-7.858933642085491\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-453.20067170642886, w0=-0.6924643999999999, w1=-0.6652629312641043\n",
      "Logistic regression iter. 1/29: loss=-82.07838717965923, w0=-0.9630131118112559, w1=-0.9154716899116018\n",
      "Logistic regression iter. 2/29: loss=-18.375633597655906, w0=-1.2310171465774329, w1=-1.16380364530823\n",
      "Logistic regression iter. 3/29: loss=-7.109239916706053, w0=-1.498584795905622, w1=-1.411833098185457\n",
      "Logistic regression iter. 4/29: loss=-5.48404298229282, w0=-1.7660721402977535, w1=-1.6598099109423012\n",
      "Logistic regression iter. 5/29: loss=-5.724840470571531, w0=-2.033543929426212, w1=-1.9077770601500355\n",
      "Logistic regression iter. 6/29: loss=-6.342352161891892, w0=-2.3010125738284786, w1=-2.155742357276819\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.5684805584270007, w1=-2.4037072862965716\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.835948399966106, w1=-2.651672139814366\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.1034162095605478, w1=-2.899636977410427\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.3708840118297374, w1=-3.147601811564634\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.638351812378129, w1=-3.3955666449579343\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.9058196125132905, w1=-3.6435314781795416\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.173287412547206, w1=-3.8914963113616734\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.440755212555857, w1=-4.13946114453457\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.708223012558095, w1=-4.387425977705272\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.975690812558682, w1=-4.635390810875443\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.243158612558837, w1=-4.883355644045485\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-5.510626412558878, w1=-5.131320477215495\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-5.778094212558889, w1=-5.379285310385497\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-6.045562012558892, w1=-5.627250143555497\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-6.313029812558892, w1=-5.875214976725497\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-6.580497612558893, w1=-6.123179809895496\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-6.847965412558893, w1=-6.371144643065495\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-7.1154332125588935, w1=-6.6191094762354945\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-7.382901012558894, w1=-6.867074309405494\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-7.650368812558894, w1=-7.115039142575493\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-7.917836612558895, w1=-7.363003975745492\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-8.185304412558894, w1=-7.610968808915492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 29/29: loss=nan, w0=-8.452772212558894, w1=-7.858933642085491\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-453.20067170642886, w0=-0.6924643999999999, w1=-0.6652629312641043\n",
      "Logistic regression iter. 1/29: loss=-82.07838717965923, w0=-0.9630131118112559, w1=-0.9154716899116018\n",
      "Logistic regression iter. 2/29: loss=-18.375633597655906, w0=-1.2310171465774329, w1=-1.16380364530823\n",
      "Logistic regression iter. 3/29: loss=-7.109239916706053, w0=-1.498584795905622, w1=-1.411833098185457\n",
      "Logistic regression iter. 4/29: loss=-5.48404298229282, w0=-1.7660721402977535, w1=-1.6598099109423012\n",
      "Logistic regression iter. 5/29: loss=-5.724840470571531, w0=-2.033543929426212, w1=-1.9077770601500355\n",
      "Logistic regression iter. 6/29: loss=-6.342352161891892, w0=-2.3010125738284786, w1=-2.155742357276819\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.5684805584270007, w1=-2.4037072862965716\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.835948399966106, w1=-2.651672139814366\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.1034162095605478, w1=-2.899636977410427\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.3708840118297374, w1=-3.147601811564634\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.638351812378129, w1=-3.3955666449579343\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.9058196125132905, w1=-3.6435314781795416\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.173287412547206, w1=-3.8914963113616734\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.440755212555857, w1=-4.13946114453457\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.708223012558095, w1=-4.387425977705272\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.975690812558682, w1=-4.635390810875443\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.243158612558837, w1=-4.883355644045485\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-5.510626412558878, w1=-5.131320477215495\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-5.778094212558889, w1=-5.379285310385497\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-6.045562012558892, w1=-5.627250143555497\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-6.313029812558892, w1=-5.875214976725497\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-6.580497612558893, w1=-6.123179809895496\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-6.847965412558893, w1=-6.371144643065495\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-7.1154332125588935, w1=-6.6191094762354945\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-7.382901012558894, w1=-6.867074309405494\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-7.650368812558894, w1=-7.115039142575493\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-7.917836612558895, w1=-7.363003975745492\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-8.185304412558894, w1=-7.610968808915492\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-8.452772212558894, w1=-7.858933642085491\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-453.20067170642886, w0=-0.6924643999999999, w1=-0.6652629312641043\n",
      "Logistic regression iter. 1/29: loss=-82.07838717965923, w0=-0.9630131118112559, w1=-0.9154716899116018\n",
      "Logistic regression iter. 2/29: loss=-18.375633597655906, w0=-1.2310171465774329, w1=-1.16380364530823\n",
      "Logistic regression iter. 3/29: loss=-7.109239916706053, w0=-1.498584795905622, w1=-1.411833098185457\n",
      "Logistic regression iter. 4/29: loss=-5.48404298229282, w0=-1.7660721402977535, w1=-1.6598099109423012\n",
      "Logistic regression iter. 5/29: loss=-5.724840470571531, w0=-2.033543929426212, w1=-1.9077770601500355\n",
      "Logistic regression iter. 6/29: loss=-6.342352161891892, w0=-2.3010125738284786, w1=-2.155742357276819\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.5684805584270007, w1=-2.4037072862965716\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.835948399966106, w1=-2.651672139814366\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.1034162095605478, w1=-2.899636977410427\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.3708840118297374, w1=-3.147601811564634\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.638351812378129, w1=-3.3955666449579343\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.9058196125132905, w1=-3.6435314781795416\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.173287412547206, w1=-3.8914963113616734\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.440755212555857, w1=-4.13946114453457\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.708223012558095, w1=-4.387425977705272\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-4.975690812558682, w1=-4.635390810875443\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.243158612558837, w1=-4.883355644045485\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-5.510626412558878, w1=-5.131320477215495\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-5.778094212558889, w1=-5.379285310385497\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-6.045562012558892, w1=-5.627250143555497\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-6.313029812558892, w1=-5.875214976725497\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-6.580497612558893, w1=-6.123179809895496\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-6.847965412558893, w1=-6.371144643065495\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-7.1154332125588935, w1=-6.6191094762354945\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-7.382901012558894, w1=-6.867074309405494\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-7.650368812558894, w1=-7.115039142575493\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-7.917836612558895, w1=-7.363003975745492\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-8.185304412558894, w1=-7.610968808915492\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-8.452772212558894, w1=-7.858933642085491\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-407.2547945448147, w0=-0.7087576799999998, w1=-0.6809161767056127\n",
      "Logistic regression iter. 1/29: loss=-71.6905197982032, w0=-0.9853466078566169, w1=-0.9367667190842337\n",
      "Logistic regression iter. 2/29: loss=-16.06491525170581, w0=-1.2595835323343347, w1=-1.1908901912084093\n",
      "Logistic regression iter. 3/29: loss=-6.655289586026766, w0=-1.5334303868220243, w1=-1.4447446343706982\n",
      "Logistic regression iter. 4/29: loss=-5.469396534580079, w0=-1.8072077896302456, w1=-1.6985538222284937\n",
      "Logistic regression iter. 5/29: loss=-5.82517240839353, w0=-2.0809721631592493, w1=-1.952354972639154\n",
      "Logistic regression iter. 6/29: loss=-6.482694587759712, w0=-2.354733983096706, w1=-2.206154631595744\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.6284952831741832, w1=-2.459954003346506\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.9022564737994725, w1=-2.71375331798711\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.1760176406762364, w1=-2.967552620945586\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.44977880225821, w1=-3.2213519214531945\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.7235399626302343, w1=-3.475151221434718\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.997301122719491, w1=-3.7289505213009333\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.2710622827412985, w1=-3.9827498211413865\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.544823442746715, w1=-4.236549120975981\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.818584602748079, w1=-4.490348420809222\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-5.092345762748427, w1=-4.744147720642145\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.366106922748516, w1=-4.997947020474992\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-5.639868082748539, w1=-5.251746320307821\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-5.913629242748544, w1=-5.505545620140645\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-6.187390402748545, w1=-5.7593449199734685\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-6.461151562748546, w1=-6.013144219806292\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-6.734912722748546, w1=-6.266943519639115\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-7.008673882748546, w1=-6.520742819471938\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-7.2824350427485465, w1=-6.77454211930476\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-7.556196202748547, w1=-7.028341419137583\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-7.829957362748547, w1=-7.282140718970405\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-8.103718522748547, w1=-7.5359400188032275\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-8.377479682748547, w1=-7.78973931863605\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-8.651240842748546, w1=-8.043538618468872\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-407.2547945448147, w0=-0.7087576799999998, w1=-0.6809161767056127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 1/29: loss=-71.6905197982032, w0=-0.9853466078566169, w1=-0.9367667190842337\n",
      "Logistic regression iter. 2/29: loss=-16.06491525170581, w0=-1.2595835323343347, w1=-1.1908901912084093\n",
      "Logistic regression iter. 3/29: loss=-6.655289586026766, w0=-1.5334303868220243, w1=-1.4447446343706982\n",
      "Logistic regression iter. 4/29: loss=-5.469396534580079, w0=-1.8072077896302456, w1=-1.6985538222284937\n",
      "Logistic regression iter. 5/29: loss=-5.82517240839353, w0=-2.0809721631592493, w1=-1.952354972639154\n",
      "Logistic regression iter. 6/29: loss=-6.482694587759712, w0=-2.354733983096706, w1=-2.206154631595744\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.6284952831741832, w1=-2.459954003346506\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.9022564737994725, w1=-2.71375331798711\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.1760176406762364, w1=-2.967552620945586\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.44977880225821, w1=-3.2213519214531945\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.7235399626302343, w1=-3.475151221434718\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.997301122719491, w1=-3.7289505213009333\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.2710622827412985, w1=-3.9827498211413865\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.544823442746715, w1=-4.236549120975981\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.818584602748079, w1=-4.490348420809222\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-5.092345762748427, w1=-4.744147720642145\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.366106922748516, w1=-4.997947020474992\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-5.639868082748539, w1=-5.251746320307821\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-5.913629242748544, w1=-5.505545620140645\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-6.187390402748545, w1=-5.7593449199734685\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-6.461151562748546, w1=-6.013144219806292\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-6.734912722748546, w1=-6.266943519639115\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-7.008673882748546, w1=-6.520742819471938\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-7.2824350427485465, w1=-6.77454211930476\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-7.556196202748547, w1=-7.028341419137583\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-7.829957362748547, w1=-7.282140718970405\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-8.103718522748547, w1=-7.5359400188032275\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-8.377479682748547, w1=-7.78973931863605\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-8.651240842748546, w1=-8.043538618468872\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-407.2547945448147, w0=-0.7087576799999998, w1=-0.6809161767056127\n",
      "Logistic regression iter. 1/29: loss=-71.6905197982032, w0=-0.9853466078566169, w1=-0.9367667190842337\n",
      "Logistic regression iter. 2/29: loss=-16.06491525170581, w0=-1.2595835323343347, w1=-1.1908901912084093\n",
      "Logistic regression iter. 3/29: loss=-6.655289586026766, w0=-1.5334303868220243, w1=-1.4447446343706982\n",
      "Logistic regression iter. 4/29: loss=-5.469396534580079, w0=-1.8072077896302456, w1=-1.6985538222284937\n",
      "Logistic regression iter. 5/29: loss=-5.82517240839353, w0=-2.0809721631592493, w1=-1.952354972639154\n",
      "Logistic regression iter. 6/29: loss=-6.482694587759712, w0=-2.354733983096706, w1=-2.206154631595744\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.6284952831741832, w1=-2.459954003346506\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.9022564737994725, w1=-2.71375331798711\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.1760176406762364, w1=-2.967552620945586\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.44977880225821, w1=-3.2213519214531945\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.7235399626302343, w1=-3.475151221434718\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.997301122719491, w1=-3.7289505213009333\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.2710622827412985, w1=-3.9827498211413865\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.544823442746715, w1=-4.236549120975981\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.818584602748079, w1=-4.490348420809222\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-5.092345762748427, w1=-4.744147720642145\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.366106922748516, w1=-4.997947020474992\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-5.639868082748539, w1=-5.251746320307821\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-5.913629242748544, w1=-5.505545620140645\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-6.187390402748545, w1=-5.7593449199734685\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-6.461151562748546, w1=-6.013144219806292\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-6.734912722748546, w1=-6.266943519639115\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-7.008673882748546, w1=-6.520742819471938\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-7.2824350427485465, w1=-6.77454211930476\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-7.556196202748547, w1=-7.028341419137583\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-7.829957362748547, w1=-7.282140718970405\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-8.103718522748547, w1=-7.5359400188032275\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-8.377479682748547, w1=-7.78973931863605\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-8.651240842748546, w1=-8.043538618468872\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-407.2547945448147, w0=-0.7087576799999998, w1=-0.6809161767056127\n",
      "Logistic regression iter. 1/29: loss=-71.6905197982032, w0=-0.9853466078566169, w1=-0.9367667190842337\n",
      "Logistic regression iter. 2/29: loss=-16.06491525170581, w0=-1.2595835323343347, w1=-1.1908901912084093\n",
      "Logistic regression iter. 3/29: loss=-6.655289586026766, w0=-1.5334303868220243, w1=-1.4447446343706982\n",
      "Logistic regression iter. 4/29: loss=-5.469396534580079, w0=-1.8072077896302456, w1=-1.6985538222284937\n",
      "Logistic regression iter. 5/29: loss=-5.82517240839353, w0=-2.0809721631592493, w1=-1.952354972639154\n",
      "Logistic regression iter. 6/29: loss=-6.482694587759712, w0=-2.354733983096706, w1=-2.206154631595744\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.6284952831741832, w1=-2.459954003346506\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.9022564737994725, w1=-2.71375331798711\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.1760176406762364, w1=-2.967552620945586\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.44977880225821, w1=-3.2213519214531945\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.7235399626302343, w1=-3.475151221434718\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-3.997301122719491, w1=-3.7289505213009333\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.2710622827412985, w1=-3.9827498211413865\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.544823442746715, w1=-4.236549120975981\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.818584602748079, w1=-4.490348420809222\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-5.092345762748427, w1=-4.744147720642145\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.366106922748516, w1=-4.997947020474992\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-5.639868082748539, w1=-5.251746320307821\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-5.913629242748544, w1=-5.505545620140645\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-6.187390402748545, w1=-5.7593449199734685\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-6.461151562748546, w1=-6.013144219806292\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-6.734912722748546, w1=-6.266943519639115\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-7.008673882748546, w1=-6.520742819471938\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-7.2824350427485465, w1=-6.77454211930476\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-7.556196202748547, w1=-7.028341419137583\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-7.829957362748547, w1=-7.282140718970405\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-8.103718522748547, w1=-7.5359400188032275\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-8.377479682748547, w1=-7.78973931863605\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-8.651240842748546, w1=-8.043538618468872\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-366.1500102920148, w0=-0.7250509599999999, w1=-0.696569422147121\n",
      "Logistic regression iter. 1/29: loss=-62.705735753432506, w0=-1.0077006486159252, w1=-0.9580782247806193\n",
      "Logistic regression iter. 2/29: loss=-14.142771014050052, w0=-1.288177260629046, w1=-1.2179982373599558\n",
      "Logistic regression iter. 3/29: loss=-6.305600487624526, w0=-1.5683053316628661, w1=-1.4776790649225031\n",
      "Logistic regression iter. 4/29: loss=-5.482335442267671, w0=-1.8483733533088689, w1=-1.7373209947397277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 5/29: loss=-5.932804387304596, w0=-2.128430462915691, w1=-1.9969562403725756\n",
      "Logistic regression iter. 6/29: loss=-6.625007998119795, w0=-2.4084854989336417, w1=-2.2565902849675337\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.688540125293522, w1=-2.5162241054543304\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.968594667889547, w1=-2.7758578827332863\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.2486491928228083, w1=-3.0354916514379764\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.528703713926801, w1=-3.295125418396681\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.808758234179462, w1=-3.5547591849914504\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-4.088812754238476, w1=-3.8143929515087276\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.3688672742525165, w1=-4.074026718009179\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.648921794255912, w1=-4.33366048450591\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.9289763142567455, w1=-4.593294251001805\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-5.209030834256952, w1=-4.85292801749751\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.4890853542570035, w1=-5.112561783993169\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-5.769139874257016, w1=-5.372195550488819\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-6.049194394257019, w1=-5.631829316984466\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-6.32924891425702, w1=-5.8914630834801125\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-6.60930343425702, w1=-6.151096849975759\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-6.8893579542570205, w1=-6.4107306164714055\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-7.169412474257021, w1=-6.670364382967052\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-7.449466994257021, w1=-6.9299981494626985\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-7.729521514257021, w1=-7.189631915958345\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-8.009576034257021, w1=-7.449265682453992\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-8.289630554257021, w1=-7.708899448949638\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-8.569685074257022, w1=-7.968533215445285\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-8.849739594257022, w1=-8.22816698194093\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-366.1500102920148, w0=-0.7250509599999999, w1=-0.696569422147121\n",
      "Logistic regression iter. 1/29: loss=-62.705735753432506, w0=-1.0077006486159252, w1=-0.9580782247806193\n",
      "Logistic regression iter. 2/29: loss=-14.142771014050052, w0=-1.288177260629046, w1=-1.2179982373599558\n",
      "Logistic regression iter. 3/29: loss=-6.305600487624526, w0=-1.5683053316628661, w1=-1.4776790649225031\n",
      "Logistic regression iter. 4/29: loss=-5.482335442267671, w0=-1.8483733533088689, w1=-1.7373209947397277\n",
      "Logistic regression iter. 5/29: loss=-5.932804387304596, w0=-2.128430462915691, w1=-1.9969562403725756\n",
      "Logistic regression iter. 6/29: loss=-6.625007998119795, w0=-2.4084854989336417, w1=-2.2565902849675337\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.688540125293522, w1=-2.5162241054543304\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.968594667889547, w1=-2.7758578827332863\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.2486491928228083, w1=-3.0354916514379764\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.528703713926801, w1=-3.295125418396681\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.808758234179462, w1=-3.5547591849914504\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-4.088812754238476, w1=-3.8143929515087276\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.3688672742525165, w1=-4.074026718009179\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.648921794255912, w1=-4.33366048450591\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.9289763142567455, w1=-4.593294251001805\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-5.209030834256952, w1=-4.85292801749751\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.4890853542570035, w1=-5.112561783993169\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-5.769139874257016, w1=-5.372195550488819\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-6.049194394257019, w1=-5.631829316984466\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-6.32924891425702, w1=-5.8914630834801125\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-6.60930343425702, w1=-6.151096849975759\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-6.8893579542570205, w1=-6.4107306164714055\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-7.169412474257021, w1=-6.670364382967052\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-7.449466994257021, w1=-6.9299981494626985\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-7.729521514257021, w1=-7.189631915958345\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-8.009576034257021, w1=-7.449265682453992\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-8.289630554257021, w1=-7.708899448949638\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-8.569685074257022, w1=-7.968533215445285\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-8.849739594257022, w1=-8.22816698194093\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-366.1500102920148, w0=-0.7250509599999999, w1=-0.696569422147121\n",
      "Logistic regression iter. 1/29: loss=-62.705735753432506, w0=-1.0077006486159252, w1=-0.9580782247806193\n",
      "Logistic regression iter. 2/29: loss=-14.142771014050052, w0=-1.288177260629046, w1=-1.2179982373599558\n",
      "Logistic regression iter. 3/29: loss=-6.305600487624526, w0=-1.5683053316628661, w1=-1.4776790649225031\n",
      "Logistic regression iter. 4/29: loss=-5.482335442267671, w0=-1.8483733533088689, w1=-1.7373209947397277\n",
      "Logistic regression iter. 5/29: loss=-5.932804387304596, w0=-2.128430462915691, w1=-1.9969562403725756\n",
      "Logistic regression iter. 6/29: loss=-6.625007998119795, w0=-2.4084854989336417, w1=-2.2565902849675337\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.688540125293522, w1=-2.5162241054543304\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.968594667889547, w1=-2.7758578827332863\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.2486491928228083, w1=-3.0354916514379764\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.528703713926801, w1=-3.295125418396681\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.808758234179462, w1=-3.5547591849914504\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-4.088812754238476, w1=-3.8143929515087276\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.3688672742525165, w1=-4.074026718009179\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.648921794255912, w1=-4.33366048450591\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.9289763142567455, w1=-4.593294251001805\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-5.209030834256952, w1=-4.85292801749751\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.4890853542570035, w1=-5.112561783993169\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-5.769139874257016, w1=-5.372195550488819\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-6.049194394257019, w1=-5.631829316984466\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-6.32924891425702, w1=-5.8914630834801125\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-6.60930343425702, w1=-6.151096849975759\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-6.8893579542570205, w1=-6.4107306164714055\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-7.169412474257021, w1=-6.670364382967052\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-7.449466994257021, w1=-6.9299981494626985\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-7.729521514257021, w1=-7.189631915958345\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-8.009576034257021, w1=-7.449265682453992\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-8.289630554257021, w1=-7.708899448949638\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-8.569685074257022, w1=-7.968533215445285\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-8.849739594257022, w1=-8.22816698194093\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-366.1500102920148, w0=-0.7250509599999999, w1=-0.696569422147121\n",
      "Logistic regression iter. 1/29: loss=-62.705735753432506, w0=-1.0077006486159252, w1=-0.9580782247806193\n",
      "Logistic regression iter. 2/29: loss=-14.142771014050052, w0=-1.288177260629046, w1=-1.2179982373599558\n",
      "Logistic regression iter. 3/29: loss=-6.305600487624526, w0=-1.5683053316628661, w1=-1.4776790649225031\n",
      "Logistic regression iter. 4/29: loss=-5.482335442267671, w0=-1.8483733533088689, w1=-1.7373209947397277\n",
      "Logistic regression iter. 5/29: loss=-5.932804387304596, w0=-2.128430462915691, w1=-1.9969562403725756\n",
      "Logistic regression iter. 6/29: loss=-6.625007998119795, w0=-2.4084854989336417, w1=-2.2565902849675337\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.688540125293522, w1=-2.5162241054543304\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-2.968594667889547, w1=-2.7758578827332863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 9/29: loss=nan, w0=-3.2486491928228083, w1=-3.0354916514379764\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.528703713926801, w1=-3.295125418396681\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.808758234179462, w1=-3.5547591849914504\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-4.088812754238476, w1=-3.8143929515087276\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.3688672742525165, w1=-4.074026718009179\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.648921794255912, w1=-4.33366048450591\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-4.9289763142567455, w1=-4.593294251001805\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-5.209030834256952, w1=-4.85292801749751\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.4890853542570035, w1=-5.112561783993169\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-5.769139874257016, w1=-5.372195550488819\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-6.049194394257019, w1=-5.631829316984466\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-6.32924891425702, w1=-5.8914630834801125\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-6.60930343425702, w1=-6.151096849975759\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-6.8893579542570205, w1=-6.4107306164714055\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-7.169412474257021, w1=-6.670364382967052\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-7.449466994257021, w1=-6.9299981494626985\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-7.729521514257021, w1=-7.189631915958345\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-8.009576034257021, w1=-7.449265682453992\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-8.289630554257021, w1=-7.708899448949638\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-8.569685074257022, w1=-7.968533215445285\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-8.849739594257022, w1=-8.22816698194093\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-329.3592917461965, w0=-0.7413442399999999, w1=-0.7122226675886294\n",
      "Logistic regression iter. 1/29: loss=-54.93217565653244, w0=-1.0300736283642893, w1=-0.9794048279550285\n",
      "Logistic regression iter. 2/29: loss=-12.545142873834523, w0=-1.3167959720143172, w1=-1.2451258219132182\n",
      "Logistic regression iter. 3/29: loss=-6.040211080590127, w0=-1.603206985817267, w1=-1.5106342229711551\n",
      "Logistic regression iter. 4/29: loss=-5.516894150702635, w0=-1.889566091350544, w1=-1.776109197071096\n",
      "Logistic regression iter. 5/29: loss=-6.045993342679117, w0=-2.1759160589129336, w1=-2.041578612874325\n",
      "Logistic regression iter. 6/29: loss=-6.768786501916986, w0=-2.462264342596055, w1=-2.3070470614840066\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.748612303405152, w1=-2.5725153351979957\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-3.0349602000896954, w1=-2.837983576214345\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.321308083631864, w1=-3.103451810935274\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.607655964403067, w1=-3.368920044411794\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.8940038445748524, w1=-3.634388277636406\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-4.180351724613917, w1=-3.8998565108089034\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.466699604622968, w1=-4.165324743970403\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.7530474846251, w1=-4.430792977129538\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-5.039395364625609, w1=-4.696261210288156\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-5.325743244625731, w1=-4.961729443446659\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.612091124625761, w1=-5.227197676605137\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-5.898439004625768, w1=-5.492665909763608\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-6.18478688462577, w1=-5.758134142922079\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-6.47113476462577, w1=-6.023602376080548\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-6.75748264462577, w1=-6.289070609239018\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-7.04383052462577, w1=-6.554538842397488\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-7.33017840462577, w1=-6.820007075555957\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-7.61652628462577, w1=-7.085475308714427\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-7.9028741646257705, w1=-7.350943541872897\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-8.18922204462577, w1=-7.6164117750313665\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-8.47556992462577, w1=-7.881880008189836\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-8.761917804625769, w1=-8.147348241348306\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-9.048265684625768, w1=-8.412816474506776\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-329.3592917461965, w0=-0.7413442399999999, w1=-0.7122226675886294\n",
      "Logistic regression iter. 1/29: loss=-54.93217565653244, w0=-1.0300736283642893, w1=-0.9794048279550285\n",
      "Logistic regression iter. 2/29: loss=-12.545142873834523, w0=-1.3167959720143172, w1=-1.2451258219132182\n",
      "Logistic regression iter. 3/29: loss=-6.040211080590127, w0=-1.603206985817267, w1=-1.5106342229711551\n",
      "Logistic regression iter. 4/29: loss=-5.516894150702635, w0=-1.889566091350544, w1=-1.776109197071096\n",
      "Logistic regression iter. 5/29: loss=-6.045993342679117, w0=-2.1759160589129336, w1=-2.041578612874325\n",
      "Logistic regression iter. 6/29: loss=-6.768786501916986, w0=-2.462264342596055, w1=-2.3070470614840066\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.748612303405152, w1=-2.5725153351979957\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-3.0349602000896954, w1=-2.837983576214345\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.321308083631864, w1=-3.103451810935274\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.607655964403067, w1=-3.368920044411794\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.8940038445748524, w1=-3.634388277636406\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-4.180351724613917, w1=-3.8998565108089034\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.466699604622968, w1=-4.165324743970403\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.7530474846251, w1=-4.430792977129538\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-5.039395364625609, w1=-4.696261210288156\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-5.325743244625731, w1=-4.961729443446659\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.612091124625761, w1=-5.227197676605137\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-5.898439004625768, w1=-5.492665909763608\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-6.18478688462577, w1=-5.758134142922079\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-6.47113476462577, w1=-6.023602376080548\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-6.75748264462577, w1=-6.289070609239018\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-7.04383052462577, w1=-6.554538842397488\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-7.33017840462577, w1=-6.820007075555957\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-7.61652628462577, w1=-7.085475308714427\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-7.9028741646257705, w1=-7.350943541872897\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-8.18922204462577, w1=-7.6164117750313665\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-8.47556992462577, w1=-7.881880008189836\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-8.761917804625769, w1=-8.147348241348306\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-9.048265684625768, w1=-8.412816474506776\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-329.3592917461965, w0=-0.7413442399999999, w1=-0.7122226675886294\n",
      "Logistic regression iter. 1/29: loss=-54.93217565653244, w0=-1.0300736283642893, w1=-0.9794048279550285\n",
      "Logistic regression iter. 2/29: loss=-12.545142873834523, w0=-1.3167959720143172, w1=-1.2451258219132182\n",
      "Logistic regression iter. 3/29: loss=-6.040211080590127, w0=-1.603206985817267, w1=-1.5106342229711551\n",
      "Logistic regression iter. 4/29: loss=-5.516894150702635, w0=-1.889566091350544, w1=-1.776109197071096\n",
      "Logistic regression iter. 5/29: loss=-6.045993342679117, w0=-2.1759160589129336, w1=-2.041578612874325\n",
      "Logistic regression iter. 6/29: loss=-6.768786501916986, w0=-2.462264342596055, w1=-2.3070470614840066\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.748612303405152, w1=-2.5725153351979957\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-3.0349602000896954, w1=-2.837983576214345\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.321308083631864, w1=-3.103451810935274\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.607655964403067, w1=-3.368920044411794\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.8940038445748524, w1=-3.634388277636406\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-4.180351724613917, w1=-3.8998565108089034\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.466699604622968, w1=-4.165324743970403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 14/29: loss=nan, w0=-4.7530474846251, w1=-4.430792977129538\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-5.039395364625609, w1=-4.696261210288156\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-5.325743244625731, w1=-4.961729443446659\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.612091124625761, w1=-5.227197676605137\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-5.898439004625768, w1=-5.492665909763608\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-6.18478688462577, w1=-5.758134142922079\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-6.47113476462577, w1=-6.023602376080548\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-6.75748264462577, w1=-6.289070609239018\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-7.04383052462577, w1=-6.554538842397488\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-7.33017840462577, w1=-6.820007075555957\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-7.61652628462577, w1=-7.085475308714427\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-7.9028741646257705, w1=-7.350943541872897\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-8.18922204462577, w1=-7.6164117750313665\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-8.47556992462577, w1=-7.881880008189836\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-8.761917804625769, w1=-8.147348241348306\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-9.048265684625768, w1=-8.412816474506776\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-329.3592917461965, w0=-0.7413442399999999, w1=-0.7122226675886294\n",
      "Logistic regression iter. 1/29: loss=-54.93217565653244, w0=-1.0300736283642893, w1=-0.9794048279550285\n",
      "Logistic regression iter. 2/29: loss=-12.545142873834523, w0=-1.3167959720143172, w1=-1.2451258219132182\n",
      "Logistic regression iter. 3/29: loss=-6.040211080590127, w0=-1.603206985817267, w1=-1.5106342229711551\n",
      "Logistic regression iter. 4/29: loss=-5.516894150702635, w0=-1.889566091350544, w1=-1.776109197071096\n",
      "Logistic regression iter. 5/29: loss=-6.045993342679117, w0=-2.1759160589129336, w1=-2.041578612874325\n",
      "Logistic regression iter. 6/29: loss=-6.768786501916986, w0=-2.462264342596055, w1=-2.3070470614840066\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.748612303405152, w1=-2.5725153351979957\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-3.0349602000896954, w1=-2.837983576214345\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.321308083631864, w1=-3.103451810935274\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.607655964403067, w1=-3.368920044411794\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.8940038445748524, w1=-3.634388277636406\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-4.180351724613917, w1=-3.8998565108089034\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.466699604622968, w1=-4.165324743970403\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.7530474846251, w1=-4.430792977129538\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-5.039395364625609, w1=-4.696261210288156\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-5.325743244625731, w1=-4.961729443446659\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.612091124625761, w1=-5.227197676605137\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-5.898439004625768, w1=-5.492665909763608\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-6.18478688462577, w1=-5.758134142922079\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-6.47113476462577, w1=-6.023602376080548\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-6.75748264462577, w1=-6.289070609239018\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-7.04383052462577, w1=-6.554538842397488\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-7.33017840462577, w1=-6.820007075555957\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-7.61652628462577, w1=-7.085475308714427\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-7.9028741646257705, w1=-7.350943541872897\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-8.18922204462577, w1=-7.6164117750313665\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-8.47556992462577, w1=-7.881880008189836\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-8.761917804625769, w1=-8.147348241348306\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-9.048265684625768, w1=-8.412816474506776\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-296.41503219243015, w0=-0.7576375199999998, w1=-0.7278759130301377\n",
      "Logistic regression iter. 1/29: loss=-48.20466615413278, w0=-1.0524640548899926, w1=-1.0007452581565213\n",
      "Logistic regression iter. 2/29: loss=-11.218675782057108, w0=-1.3454375013431148, w1=-1.272271157690645\n",
      "Logistic regression iter. 3/29: loss=-5.843028202899241, w0=-1.6381329390052437, w1=-1.5436081462843037\n",
      "Logistic regression iter. 4/29: loss=-5.568412302886496, w0=-1.930783514320261, w1=-1.814916413811797\n",
      "Logistic regression iter. 5/29: loss=-6.163417958474385, w0=-2.22342643778821, w1=-2.086220059542369\n",
      "Logistic regression iter. 6/29: loss=-6.913657045245348, w0=-2.5160679937498753, w1=-2.3575229263552617\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.80870929518521, w1=-2.6288256566570256\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-3.1013505475142096, w1=-2.9001283622082723\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.3939917900597982, w1=-3.1714310631355613\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.686633030599065, w1=-3.4427337631755024\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.9792742707159907, w1=-3.7140364630409746\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-4.271915510741881, w1=-3.985339162871375\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.564556750747722, w1=-4.25664186269458\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.857197990749062, w1=-4.527944562516281\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-5.1498392307493726, w1=-4.799247262337663\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-5.442480470749445, w1=-5.070549962158976\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.735121710749462, w1=-5.341852661980274\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-6.027762950749466, w1=-5.613155361801569\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-6.320404190749467, w1=-5.884458061622863\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-6.613045430749467, w1=-6.1557607614441565\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-6.905686670749467, w1=-6.42706346126545\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-7.198327910749467, w1=-6.698366161086744\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-7.490969150749467, w1=-6.969668860908038\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-7.783610390749467, w1=-7.2409715607293315\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-8.076251630749466, w1=-7.512274260550625\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-8.368892870749466, w1=-7.783576960371919\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-8.661534110749466, w1=-8.054879660193212\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-8.954175350749466, w1=-8.326182360014505\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-9.246816590749466, w1=-8.597485059835797\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-296.41503219243015, w0=-0.7576375199999998, w1=-0.7278759130301377\n",
      "Logistic regression iter. 1/29: loss=-48.20466615413278, w0=-1.0524640548899926, w1=-1.0007452581565213\n",
      "Logistic regression iter. 2/29: loss=-11.218675782057108, w0=-1.3454375013431148, w1=-1.272271157690645\n",
      "Logistic regression iter. 3/29: loss=-5.843028202899241, w0=-1.6381329390052437, w1=-1.5436081462843037\n",
      "Logistic regression iter. 4/29: loss=-5.568412302886496, w0=-1.930783514320261, w1=-1.814916413811797\n",
      "Logistic regression iter. 5/29: loss=-6.163417958474385, w0=-2.22342643778821, w1=-2.086220059542369\n",
      "Logistic regression iter. 6/29: loss=-6.913657045245348, w0=-2.5160679937498753, w1=-2.3575229263552617\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.80870929518521, w1=-2.6288256566570256\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-3.1013505475142096, w1=-2.9001283622082723\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.3939917900597982, w1=-3.1714310631355613\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.686633030599065, w1=-3.4427337631755024\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.9792742707159907, w1=-3.7140364630409746\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-4.271915510741881, w1=-3.985339162871375\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.564556750747722, w1=-4.25664186269458\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.857197990749062, w1=-4.527944562516281\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-5.1498392307493726, w1=-4.799247262337663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 16/29: loss=nan, w0=-5.442480470749445, w1=-5.070549962158976\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.735121710749462, w1=-5.341852661980274\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-6.027762950749466, w1=-5.613155361801569\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-6.320404190749467, w1=-5.884458061622863\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-6.613045430749467, w1=-6.1557607614441565\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-6.905686670749467, w1=-6.42706346126545\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-7.198327910749467, w1=-6.698366161086744\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-7.490969150749467, w1=-6.969668860908038\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-7.783610390749467, w1=-7.2409715607293315\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-8.076251630749466, w1=-7.512274260550625\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-8.368892870749466, w1=-7.783576960371919\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-8.661534110749466, w1=-8.054879660193212\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-8.954175350749466, w1=-8.326182360014505\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-9.246816590749466, w1=-8.597485059835797\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-296.41503219243015, w0=-0.7576375199999998, w1=-0.7278759130301377\n",
      "Logistic regression iter. 1/29: loss=-48.20466615413278, w0=-1.0524640548899926, w1=-1.0007452581565213\n",
      "Logistic regression iter. 2/29: loss=-11.218675782057108, w0=-1.3454375013431148, w1=-1.272271157690645\n",
      "Logistic regression iter. 3/29: loss=-5.843028202899241, w0=-1.6381329390052437, w1=-1.5436081462843037\n",
      "Logistic regression iter. 4/29: loss=-5.568412302886496, w0=-1.930783514320261, w1=-1.814916413811797\n",
      "Logistic regression iter. 5/29: loss=-6.163417958474385, w0=-2.22342643778821, w1=-2.086220059542369\n",
      "Logistic regression iter. 6/29: loss=-6.913657045245348, w0=-2.5160679937498753, w1=-2.3575229263552617\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.80870929518521, w1=-2.6288256566570256\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-3.1013505475142096, w1=-2.9001283622082723\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.3939917900597982, w1=-3.1714310631355613\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.686633030599065, w1=-3.4427337631755024\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.9792742707159907, w1=-3.7140364630409746\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-4.271915510741881, w1=-3.985339162871375\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.564556750747722, w1=-4.25664186269458\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.857197990749062, w1=-4.527944562516281\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-5.1498392307493726, w1=-4.799247262337663\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-5.442480470749445, w1=-5.070549962158976\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.735121710749462, w1=-5.341852661980274\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-6.027762950749466, w1=-5.613155361801569\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-6.320404190749467, w1=-5.884458061622863\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-6.613045430749467, w1=-6.1557607614441565\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-6.905686670749467, w1=-6.42706346126545\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-7.198327910749467, w1=-6.698366161086744\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-7.490969150749467, w1=-6.969668860908038\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-7.783610390749467, w1=-7.2409715607293315\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-8.076251630749466, w1=-7.512274260550625\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-8.368892870749466, w1=-7.783576960371919\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-8.661534110749466, w1=-8.054879660193212\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-8.954175350749466, w1=-8.326182360014505\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-9.246816590749466, w1=-8.597485059835797\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-296.41503219243015, w0=-0.7576375199999998, w1=-0.7278759130301377\n",
      "Logistic regression iter. 1/29: loss=-48.20466615413278, w0=-1.0524640548899926, w1=-1.0007452581565213\n",
      "Logistic regression iter. 2/29: loss=-11.218675782057108, w0=-1.3454375013431148, w1=-1.272271157690645\n",
      "Logistic regression iter. 3/29: loss=-5.843028202899241, w0=-1.6381329390052437, w1=-1.5436081462843037\n",
      "Logistic regression iter. 4/29: loss=-5.568412302886496, w0=-1.930783514320261, w1=-1.814916413811797\n",
      "Logistic regression iter. 5/29: loss=-6.163417958474385, w0=-2.22342643778821, w1=-2.086220059542369\n",
      "Logistic regression iter. 6/29: loss=-6.913657045245348, w0=-2.5160679937498753, w1=-2.3575229263552617\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.80870929518521, w1=-2.6288256566570256\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-3.1013505475142096, w1=-2.9001283622082723\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.3939917900597982, w1=-3.1714310631355613\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.686633030599065, w1=-3.4427337631755024\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-3.9792742707159907, w1=-3.7140364630409746\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-4.271915510741881, w1=-3.985339162871375\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.564556750747722, w1=-4.25664186269458\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.857197990749062, w1=-4.527944562516281\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-5.1498392307493726, w1=-4.799247262337663\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-5.442480470749445, w1=-5.070549962158976\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.735121710749462, w1=-5.341852661980274\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-6.027762950749466, w1=-5.613155361801569\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-6.320404190749467, w1=-5.884458061622863\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-6.613045430749467, w1=-6.1557607614441565\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-6.905686670749467, w1=-6.42706346126545\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-7.198327910749467, w1=-6.698366161086744\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-7.490969150749467, w1=-6.969668860908038\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-7.783610390749467, w1=-7.2409715607293315\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-8.076251630749466, w1=-7.512274260550625\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-8.368892870749466, w1=-7.783576960371919\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-8.661534110749466, w1=-8.054879660193212\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-8.954175350749466, w1=-8.326182360014505\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-9.246816590749466, w1=-8.597485059835797\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-266.90210092791295, w0=-0.7739307999999998, w1=-0.743529158471646\n",
      "Logistic regression iter. 1/29: loss=-42.38095249196304, w0=-1.0748705437273882, w1=-1.0220983463637645\n",
      "Logistic regression iter. 2/29: loss=-10.118908081750572, w0=-1.374099863984712, w1=-1.2994326179139533\n",
      "Logistic regression iter. 3/29: loss=-5.701068981439278, w0=-1.673080996002267, w1=-1.5765990586837926\n",
      "Logistic regression iter. 4/29: loss=-5.633246087342501, w0=-1.9720233613145373, w1=-1.8537408249705423\n",
      "Logistic regression iter. 5/29: loss=-6.284075492041517, w0=-2.2709593194244726, w1=-2.1308787482838083\n",
      "Logistic regression iter. 6/29: loss=nan, w0=-2.5698941668529836, w1=-2.408016044258964\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.868828813589219, w1=-2.6851532336643875\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-3.1677634227066567, w1=-2.9622904043295977\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.466698024537661, w1=-3.239427571597214\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.765632624915117, w1=-3.516564738231777\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-4.0645672249947875, w1=-3.793701904745432\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-4.363501825011966, w1=-4.070839071235468\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.66243642501574, w1=-4.347976237720793\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.961371025016583, w1=-4.625113404205162\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-5.260305625016773, w1=-4.902250570689332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 16/29: loss=nan, w0=-5.559240225016817, w1=-5.17938773717346\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.858174825016826, w1=-5.45652490365758\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-6.157109425016828, w1=-5.733662070141698\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-6.456044025016828, w1=-6.0107992366258145\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-6.754978625016828, w1=-6.287936403109931\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-7.053913225016828, w1=-6.565073569594048\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-7.352847825016828, w1=-6.842210736078165\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-7.651782425016828, w1=-7.119347902562282\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-7.950717025016828, w1=-7.396485069046399\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-8.249651625016828, w1=-7.673622235530516\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-8.548586225016827, w1=-7.950759402014633\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-8.847520825016826, w1=-8.22789656849875\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-9.146455425016825, w1=-8.505033734982867\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-9.445390025016824, w1=-8.782170901466984\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-266.90210092791295, w0=-0.7739307999999998, w1=-0.743529158471646\n",
      "Logistic regression iter. 1/29: loss=-42.38095249196304, w0=-1.0748705437273882, w1=-1.0220983463637645\n",
      "Logistic regression iter. 2/29: loss=-10.118908081750572, w0=-1.374099863984712, w1=-1.2994326179139533\n",
      "Logistic regression iter. 3/29: loss=-5.701068981439278, w0=-1.673080996002267, w1=-1.5765990586837926\n",
      "Logistic regression iter. 4/29: loss=-5.633246087342501, w0=-1.9720233613145373, w1=-1.8537408249705423\n",
      "Logistic regression iter. 5/29: loss=-6.284075492041517, w0=-2.2709593194244726, w1=-2.1308787482838083\n",
      "Logistic regression iter. 6/29: loss=nan, w0=-2.5698941668529836, w1=-2.408016044258964\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.868828813589219, w1=-2.6851532336643875\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-3.1677634227066567, w1=-2.9622904043295977\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.466698024537661, w1=-3.239427571597214\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.765632624915117, w1=-3.516564738231777\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-4.0645672249947875, w1=-3.793701904745432\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-4.363501825011966, w1=-4.070839071235468\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.66243642501574, w1=-4.347976237720793\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.961371025016583, w1=-4.625113404205162\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-5.260305625016773, w1=-4.902250570689332\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-5.559240225016817, w1=-5.17938773717346\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.858174825016826, w1=-5.45652490365758\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-6.157109425016828, w1=-5.733662070141698\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-6.456044025016828, w1=-6.0107992366258145\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-6.754978625016828, w1=-6.287936403109931\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-7.053913225016828, w1=-6.565073569594048\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-7.352847825016828, w1=-6.842210736078165\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-7.651782425016828, w1=-7.119347902562282\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-7.950717025016828, w1=-7.396485069046399\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-8.249651625016828, w1=-7.673622235530516\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-8.548586225016827, w1=-7.950759402014633\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-8.847520825016826, w1=-8.22789656849875\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-9.146455425016825, w1=-8.505033734982867\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-9.445390025016824, w1=-8.782170901466984\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-266.90210092791295, w0=-0.7739307999999998, w1=-0.743529158471646\n",
      "Logistic regression iter. 1/29: loss=-42.38095249196304, w0=-1.0748705437273882, w1=-1.0220983463637645\n",
      "Logistic regression iter. 2/29: loss=-10.118908081750572, w0=-1.374099863984712, w1=-1.2994326179139533\n",
      "Logistic regression iter. 3/29: loss=-5.701068981439278, w0=-1.673080996002267, w1=-1.5765990586837926\n",
      "Logistic regression iter. 4/29: loss=-5.633246087342501, w0=-1.9720233613145373, w1=-1.8537408249705423\n",
      "Logistic regression iter. 5/29: loss=-6.284075492041517, w0=-2.2709593194244726, w1=-2.1308787482838083\n",
      "Logistic regression iter. 6/29: loss=nan, w0=-2.5698941668529836, w1=-2.408016044258964\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.868828813589219, w1=-2.6851532336643875\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-3.1677634227066567, w1=-2.9622904043295977\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.466698024537661, w1=-3.239427571597214\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.765632624915117, w1=-3.516564738231777\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-4.0645672249947875, w1=-3.793701904745432\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-4.363501825011966, w1=-4.070839071235468\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.66243642501574, w1=-4.347976237720793\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.961371025016583, w1=-4.625113404205162\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-5.260305625016773, w1=-4.902250570689332\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-5.559240225016817, w1=-5.17938773717346\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.858174825016826, w1=-5.45652490365758\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-6.157109425016828, w1=-5.733662070141698\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-6.456044025016828, w1=-6.0107992366258145\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-6.754978625016828, w1=-6.287936403109931\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-7.053913225016828, w1=-6.565073569594048\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-7.352847825016828, w1=-6.842210736078165\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-7.651782425016828, w1=-7.119347902562282\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-7.950717025016828, w1=-7.396485069046399\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-8.249651625016828, w1=-7.673622235530516\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-8.548586225016827, w1=-7.950759402014633\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-8.847520825016826, w1=-8.22789656849875\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-9.146455425016825, w1=-8.505033734982867\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-9.445390025016824, w1=-8.782170901466984\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-266.90210092791295, w0=-0.7739307999999998, w1=-0.743529158471646\n",
      "Logistic regression iter. 1/29: loss=-42.38095249196304, w0=-1.0748705437273882, w1=-1.0220983463637645\n",
      "Logistic regression iter. 2/29: loss=-10.118908081750572, w0=-1.374099863984712, w1=-1.2994326179139533\n",
      "Logistic regression iter. 3/29: loss=-5.701068981439278, w0=-1.673080996002267, w1=-1.5765990586837926\n",
      "Logistic regression iter. 4/29: loss=-5.633246087342501, w0=-1.9720233613145373, w1=-1.8537408249705423\n",
      "Logistic regression iter. 5/29: loss=-6.284075492041517, w0=-2.2709593194244726, w1=-2.1308787482838083\n",
      "Logistic regression iter. 6/29: loss=nan, w0=-2.5698941668529836, w1=-2.408016044258964\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.868828813589219, w1=-2.6851532336643875\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-3.1677634227066567, w1=-2.9622904043295977\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.466698024537661, w1=-3.239427571597214\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.765632624915117, w1=-3.516564738231777\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-4.0645672249947875, w1=-3.793701904745432\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-4.363501825011966, w1=-4.070839071235468\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.66243642501574, w1=-4.347976237720793\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-4.961371025016583, w1=-4.625113404205162\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-5.260305625016773, w1=-4.902250570689332\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-5.559240225016817, w1=-5.17938773717346\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.858174825016826, w1=-5.45652490365758\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-6.157109425016828, w1=-5.733662070141698\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-6.456044025016828, w1=-6.0107992366258145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 20/29: loss=nan, w0=-6.754978625016828, w1=-6.287936403109931\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-7.053913225016828, w1=-6.565073569594048\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-7.352847825016828, w1=-6.842210736078165\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-7.651782425016828, w1=-7.119347902562282\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-7.950717025016828, w1=-7.396485069046399\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-8.249651625016828, w1=-7.673622235530516\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-8.548586225016827, w1=-7.950759402014633\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-8.847520825016826, w1=-8.22789656849875\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-9.146455425016825, w1=-8.505033734982867\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-9.445390025016824, w1=-8.782170901466984\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-240.45174031708098, w0=-0.7902240799999999, w1=-0.7591824039131543\n",
      "Logistic regression iter. 1/29: loss=-37.33846927389305, w0=-1.0972918122229611, w1=-1.043463018006293\n",
      "Logistic regression iter. 2/29: loss=-9.208770830001425, w0=-1.4027812425260584, w1=-1.3266087227926309\n",
      "Logistic regression iter. 3/29: loss=-5.603852789217657, w0=-1.7080491585672741, w1=-1.6096053529233592\n",
      "Logistic regression iter. 4/29: loss=-5.708544061392827, w0=-2.013283579605085, w1=-1.8925807872076483\n",
      "Logistic regression iter. 5/29: loss=-6.407204090237468, w0=-2.3185126356640064, w1=-2.175553026117455\n",
      "Logistic regression iter. 6/29: loss=nan, w0=-2.623740789524578, w1=-2.45852475972283\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.9289687851014756, w1=-2.7414964101164023\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-3.234196751849068, w1=-3.0244680463163083\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.539424713167234, w1=-3.3074396800187222\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.84465267343169, w1=-3.5904113132692728\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-4.149880633486034, w1=-3.873382946435982\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-4.455108593497444, w1=-4.156354579586774\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.760336553499886, w1=-4.439326212734478\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-5.065564513500417, w1=-4.722297845881573\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-5.370792473500533, w1=-5.005269479028545\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-5.676020433500558, w1=-5.288241112175492\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.981248393500564, w1=-5.571212745322434\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-6.286476353500564, w1=-5.854184378469375\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-6.591704313500564, w1=-6.137156011616316\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-6.896932273500564, w1=-6.420127644763256\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-7.202160233500564, w1=-6.703099277910196\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-7.507388193500564, w1=-6.986070911057136\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-7.812616153500564, w1=-7.269042544204076\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-8.117844113500563, w1=-7.552014177351016\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-8.423072073500563, w1=-7.834985810497956\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-8.728300033500563, w1=-8.117957443644897\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-9.033527993500563, w1=-8.400929076791838\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-9.338755953500563, w1=-8.68390070993878\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-9.643983913500563, w1=-8.96687234308572\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-240.45174031708098, w0=-0.7902240799999999, w1=-0.7591824039131543\n",
      "Logistic regression iter. 1/29: loss=-37.33846927389305, w0=-1.0972918122229611, w1=-1.043463018006293\n",
      "Logistic regression iter. 2/29: loss=-9.208770830001425, w0=-1.4027812425260584, w1=-1.3266087227926309\n",
      "Logistic regression iter. 3/29: loss=-5.603852789217657, w0=-1.7080491585672741, w1=-1.6096053529233592\n",
      "Logistic regression iter. 4/29: loss=-5.708544061392827, w0=-2.013283579605085, w1=-1.8925807872076483\n",
      "Logistic regression iter. 5/29: loss=-6.407204090237468, w0=-2.3185126356640064, w1=-2.175553026117455\n",
      "Logistic regression iter. 6/29: loss=nan, w0=-2.623740789524578, w1=-2.45852475972283\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.9289687851014756, w1=-2.7414964101164023\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-3.234196751849068, w1=-3.0244680463163083\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.539424713167234, w1=-3.3074396800187222\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.84465267343169, w1=-3.5904113132692728\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-4.149880633486034, w1=-3.873382946435982\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-4.455108593497444, w1=-4.156354579586774\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.760336553499886, w1=-4.439326212734478\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-5.065564513500417, w1=-4.722297845881573\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-5.370792473500533, w1=-5.005269479028545\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-5.676020433500558, w1=-5.288241112175492\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.981248393500564, w1=-5.571212745322434\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-6.286476353500564, w1=-5.854184378469375\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-6.591704313500564, w1=-6.137156011616316\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-6.896932273500564, w1=-6.420127644763256\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-7.202160233500564, w1=-6.703099277910196\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-7.507388193500564, w1=-6.986070911057136\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-7.812616153500564, w1=-7.269042544204076\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-8.117844113500563, w1=-7.552014177351016\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-8.423072073500563, w1=-7.834985810497956\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-8.728300033500563, w1=-8.117957443644897\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-9.033527993500563, w1=-8.400929076791838\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-9.338755953500563, w1=-8.68390070993878\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-9.643983913500563, w1=-8.96687234308572\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-240.45174031708098, w0=-0.7902240799999999, w1=-0.7591824039131543\n",
      "Logistic regression iter. 1/29: loss=-37.33846927389305, w0=-1.0972918122229611, w1=-1.043463018006293\n",
      "Logistic regression iter. 2/29: loss=-9.208770830001425, w0=-1.4027812425260584, w1=-1.3266087227926309\n",
      "Logistic regression iter. 3/29: loss=-5.603852789217657, w0=-1.7080491585672741, w1=-1.6096053529233592\n",
      "Logistic regression iter. 4/29: loss=-5.708544061392827, w0=-2.013283579605085, w1=-1.8925807872076483\n",
      "Logistic regression iter. 5/29: loss=-6.407204090237468, w0=-2.3185126356640064, w1=-2.175553026117455\n",
      "Logistic regression iter. 6/29: loss=nan, w0=-2.623740789524578, w1=-2.45852475972283\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.9289687851014756, w1=-2.7414964101164023\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-3.234196751849068, w1=-3.0244680463163083\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.539424713167234, w1=-3.3074396800187222\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.84465267343169, w1=-3.5904113132692728\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-4.149880633486034, w1=-3.873382946435982\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-4.455108593497444, w1=-4.156354579586774\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.760336553499886, w1=-4.439326212734478\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-5.065564513500417, w1=-4.722297845881573\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-5.370792473500533, w1=-5.005269479028545\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-5.676020433500558, w1=-5.288241112175492\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.981248393500564, w1=-5.571212745322434\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-6.286476353500564, w1=-5.854184378469375\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-6.591704313500564, w1=-6.137156011616316\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-6.896932273500564, w1=-6.420127644763256\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-7.202160233500564, w1=-6.703099277910196\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-7.507388193500564, w1=-6.986070911057136\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-7.812616153500564, w1=-7.269042544204076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression iter. 24/29: loss=nan, w0=-8.117844113500563, w1=-7.552014177351016\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-8.423072073500563, w1=-7.834985810497956\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-8.728300033500563, w1=-8.117957443644897\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-9.033527993500563, w1=-8.400929076791838\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-9.338755953500563, w1=-8.68390070993878\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-9.643983913500563, w1=-8.96687234308572\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-240.45174031708098, w0=-0.7902240799999999, w1=-0.7591824039131543\n",
      "Logistic regression iter. 1/29: loss=-37.33846927389305, w0=-1.0972918122229611, w1=-1.043463018006293\n",
      "Logistic regression iter. 2/29: loss=-9.208770830001425, w0=-1.4027812425260584, w1=-1.3266087227926309\n",
      "Logistic regression iter. 3/29: loss=-5.603852789217657, w0=-1.7080491585672741, w1=-1.6096053529233592\n",
      "Logistic regression iter. 4/29: loss=-5.708544061392827, w0=-2.013283579605085, w1=-1.8925807872076483\n",
      "Logistic regression iter. 5/29: loss=-6.407204090237468, w0=-2.3185126356640064, w1=-2.175553026117455\n",
      "Logistic regression iter. 6/29: loss=nan, w0=-2.623740789524578, w1=-2.45852475972283\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.9289687851014756, w1=-2.7414964101164023\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-3.234196751849068, w1=-3.0244680463163083\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.539424713167234, w1=-3.3074396800187222\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.84465267343169, w1=-3.5904113132692728\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-4.149880633486034, w1=-3.873382946435982\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-4.455108593497444, w1=-4.156354579586774\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.760336553499886, w1=-4.439326212734478\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-5.065564513500417, w1=-4.722297845881573\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-5.370792473500533, w1=-5.005269479028545\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-5.676020433500558, w1=-5.288241112175492\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-5.981248393500564, w1=-5.571212745322434\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-6.286476353500564, w1=-5.854184378469375\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-6.591704313500564, w1=-6.137156011616316\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-6.896932273500564, w1=-6.420127644763256\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-7.202160233500564, w1=-6.703099277910196\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-7.507388193500564, w1=-6.986070911057136\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-7.812616153500564, w1=-7.269042544204076\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-8.117844113500563, w1=-7.552014177351016\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-8.423072073500563, w1=-7.834985810497956\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-8.728300033500563, w1=-8.117957443644897\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-9.033527993500563, w1=-8.400929076791838\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-9.338755953500563, w1=-8.68390070993878\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-9.643983913500563, w1=-8.96687234308572\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-216.73619868651238, w0=-0.8065173599999998, w1=-0.7748356493546626\n",
      "Logistic regression iter. 1/29: loss=-32.97157192674426, w0=-1.11972667355943, w1=-1.0648382862371415\n",
      "Logistic regression iter. 2/29: loss=-8.45734294126631, w0=-1.4314799740636244, w1=-1.3537981269956791\n",
      "Logistic regression iter. 3/29: loss=-5.542913304516293, w0=-1.7430356085946155, w1=-1.642625574971309\n",
      "Logistic regression iter. 4/29: loss=-5.792072917544422, w0=-2.0545623058891875, w1=-1.9314348167590496\n",
      "Logistic regression iter. 5/29: loss=-6.532224240977741, w0=-2.3660845108119264, w1=-2.220241401596537\n",
      "Logistic regression iter. 6/29: loss=nan, w0=-2.6776059827812375, w1=-2.509047579377893\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.9891273298807866, w1=-2.7978536921719153\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-3.3006486548783123, w1=-3.0866597942124088\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.6121699758281025, w1=-3.3754658944162905\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.9236912960135655, w1=-3.664271994297472\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-4.2352126160506725, w1=-3.95307809412048\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-4.54673393605826, w1=-4.241884193932752\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.858255256059841, w1=-4.5306902937430005\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-5.169776576060175, w1=-4.81949639355286\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-5.481297896060247, w1=-5.108302493362642\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-5.792819216060262, w1=-5.39710859317241\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-6.104340536060265, w1=-5.685914692982175\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-6.415861856060266, w1=-5.974720792791939\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-6.727383176060266, w1=-6.263526892601703\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-7.038904496060265, w1=-6.5523329924114675\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-7.350425816060265, w1=-6.841139092221232\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-7.661947136060265, w1=-7.129945192030996\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-7.973468456060265, w1=-7.41875129184076\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-8.284989776060264, w1=-7.707557391650524\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-8.596511096060265, w1=-7.996363491460288\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-8.908032416060266, w1=-8.285169591270051\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-9.219553736060266, w1=-8.573975691079815\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-9.531075056060267, w1=-8.862781790889578\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-9.842596376060268, w1=-9.151587890699341\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-216.73619868651238, w0=-0.8065173599999998, w1=-0.7748356493546626\n",
      "Logistic regression iter. 1/29: loss=-32.97157192674426, w0=-1.11972667355943, w1=-1.0648382862371415\n",
      "Logistic regression iter. 2/29: loss=-8.45734294126631, w0=-1.4314799740636244, w1=-1.3537981269956791\n",
      "Logistic regression iter. 3/29: loss=-5.542913304516293, w0=-1.7430356085946155, w1=-1.642625574971309\n",
      "Logistic regression iter. 4/29: loss=-5.792072917544422, w0=-2.0545623058891875, w1=-1.9314348167590496\n",
      "Logistic regression iter. 5/29: loss=-6.532224240977741, w0=-2.3660845108119264, w1=-2.220241401596537\n",
      "Logistic regression iter. 6/29: loss=nan, w0=-2.6776059827812375, w1=-2.509047579377893\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.9891273298807866, w1=-2.7978536921719153\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-3.3006486548783123, w1=-3.0866597942124088\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.6121699758281025, w1=-3.3754658944162905\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.9236912960135655, w1=-3.664271994297472\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-4.2352126160506725, w1=-3.95307809412048\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-4.54673393605826, w1=-4.241884193932752\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.858255256059841, w1=-4.5306902937430005\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-5.169776576060175, w1=-4.81949639355286\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-5.481297896060247, w1=-5.108302493362642\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-5.792819216060262, w1=-5.39710859317241\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-6.104340536060265, w1=-5.685914692982175\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-6.415861856060266, w1=-5.974720792791939\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-6.727383176060266, w1=-6.263526892601703\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-7.038904496060265, w1=-6.5523329924114675\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-7.350425816060265, w1=-6.841139092221232\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-7.661947136060265, w1=-7.129945192030996\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-7.973468456060265, w1=-7.41875129184076\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-8.284989776060264, w1=-7.707557391650524\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-8.596511096060265, w1=-7.996363491460288\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-8.908032416060266, w1=-8.285169591270051\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-9.219553736060266, w1=-8.573975691079815\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-9.531075056060267, w1=-8.862781790889578\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-9.842596376060268, w1=-9.151587890699341\n",
      "4\n",
      "4,5\n",
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-216.73619868651238, w0=-0.8065173599999998, w1=-0.7748356493546626\n",
      "Logistic regression iter. 1/29: loss=-32.97157192674426, w0=-1.11972667355943, w1=-1.0648382862371415\n",
      "Logistic regression iter. 2/29: loss=-8.45734294126631, w0=-1.4314799740636244, w1=-1.3537981269956791\n",
      "Logistic regression iter. 3/29: loss=-5.542913304516293, w0=-1.7430356085946155, w1=-1.642625574971309\n",
      "Logistic regression iter. 4/29: loss=-5.792072917544422, w0=-2.0545623058891875, w1=-1.9314348167590496\n",
      "Logistic regression iter. 5/29: loss=-6.532224240977741, w0=-2.3660845108119264, w1=-2.220241401596537\n",
      "Logistic regression iter. 6/29: loss=nan, w0=-2.6776059827812375, w1=-2.509047579377893\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.9891273298807866, w1=-2.7978536921719153\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-3.3006486548783123, w1=-3.0866597942124088\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.6121699758281025, w1=-3.3754658944162905\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.9236912960135655, w1=-3.664271994297472\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-4.2352126160506725, w1=-3.95307809412048\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-4.54673393605826, w1=-4.241884193932752\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.858255256059841, w1=-4.5306902937430005\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-5.169776576060175, w1=-4.81949639355286\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-5.481297896060247, w1=-5.108302493362642\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-5.792819216060262, w1=-5.39710859317241\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-6.104340536060265, w1=-5.685914692982175\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-6.415861856060266, w1=-5.974720792791939\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-6.727383176060266, w1=-6.263526892601703\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-7.038904496060265, w1=-6.5523329924114675\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-7.350425816060265, w1=-6.841139092221232\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-7.661947136060265, w1=-7.129945192030996\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-7.973468456060265, w1=-7.41875129184076\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-8.284989776060264, w1=-7.707557391650524\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-8.596511096060265, w1=-7.996363491460288\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-8.908032416060266, w1=-8.285169591270051\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-9.219553736060266, w1=-8.573975691079815\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-9.531075056060267, w1=-8.862781790889578\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-9.842596376060268, w1=-9.151587890699341\n",
      "4\n",
      "4,5\n",
      "1\n",
      "2\n",
      "3\n",
      "3,5\n",
      "Logistic regression iter. 0/29: loss=-216.73619868651238, w0=-0.8065173599999998, w1=-0.7748356493546626\n",
      "Logistic regression iter. 1/29: loss=-32.97157192674426, w0=-1.11972667355943, w1=-1.0648382862371415\n",
      "Logistic regression iter. 2/29: loss=-8.45734294126631, w0=-1.4314799740636244, w1=-1.3537981269956791\n",
      "Logistic regression iter. 3/29: loss=-5.542913304516293, w0=-1.7430356085946155, w1=-1.642625574971309\n",
      "Logistic regression iter. 4/29: loss=-5.792072917544422, w0=-2.0545623058891875, w1=-1.9314348167590496\n",
      "Logistic regression iter. 5/29: loss=-6.532224240977741, w0=-2.3660845108119264, w1=-2.220241401596537\n",
      "Logistic regression iter. 6/29: loss=nan, w0=-2.6776059827812375, w1=-2.509047579377893\n",
      "Logistic regression iter. 7/29: loss=nan, w0=-2.9891273298807866, w1=-2.7978536921719153\n",
      "Logistic regression iter. 8/29: loss=nan, w0=-3.3006486548783123, w1=-3.0866597942124088\n",
      "Logistic regression iter. 9/29: loss=nan, w0=-3.6121699758281025, w1=-3.3754658944162905\n",
      "Logistic regression iter. 10/29: loss=nan, w0=-3.9236912960135655, w1=-3.664271994297472\n",
      "Logistic regression iter. 11/29: loss=nan, w0=-4.2352126160506725, w1=-3.95307809412048\n",
      "Logistic regression iter. 12/29: loss=nan, w0=-4.54673393605826, w1=-4.241884193932752\n",
      "Logistic regression iter. 13/29: loss=nan, w0=-4.858255256059841, w1=-4.5306902937430005\n",
      "Logistic regression iter. 14/29: loss=nan, w0=-5.169776576060175, w1=-4.81949639355286\n",
      "Logistic regression iter. 15/29: loss=nan, w0=-5.481297896060247, w1=-5.108302493362642\n",
      "Logistic regression iter. 16/29: loss=nan, w0=-5.792819216060262, w1=-5.39710859317241\n",
      "Logistic regression iter. 17/29: loss=nan, w0=-6.104340536060265, w1=-5.685914692982175\n",
      "Logistic regression iter. 18/29: loss=nan, w0=-6.415861856060266, w1=-5.974720792791939\n",
      "Logistic regression iter. 19/29: loss=nan, w0=-6.727383176060266, w1=-6.263526892601703\n",
      "Logistic regression iter. 20/29: loss=nan, w0=-7.038904496060265, w1=-6.5523329924114675\n",
      "Logistic regression iter. 21/29: loss=nan, w0=-7.350425816060265, w1=-6.841139092221232\n",
      "Logistic regression iter. 22/29: loss=nan, w0=-7.661947136060265, w1=-7.129945192030996\n",
      "Logistic regression iter. 23/29: loss=nan, w0=-7.973468456060265, w1=-7.41875129184076\n",
      "Logistic regression iter. 24/29: loss=nan, w0=-8.284989776060264, w1=-7.707557391650524\n",
      "Logistic regression iter. 25/29: loss=nan, w0=-8.596511096060265, w1=-7.996363491460288\n",
      "Logistic regression iter. 26/29: loss=nan, w0=-8.908032416060266, w1=-8.285169591270051\n",
      "Logistic regression iter. 27/29: loss=nan, w0=-9.219553736060266, w1=-8.573975691079815\n",
      "Logistic regression iter. 28/29: loss=nan, w0=-9.531075056060267, w1=-8.862781790889578\n",
      "Logistic regression iter. 29/29: loss=nan, w0=-9.842596376060268, w1=-9.151587890699341\n",
      "4\n",
      "4,5\n",
      "For polynomial expansion up to degree, the choice of gamma which leads to the best rmse is 0.00500 with a test rmse of 1.958\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.005, 1.9578139265553145)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEaCAYAAAD+E0veAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAuP0lEQVR4nO3deXxU9bnH8c+TnSUkLBIRRFABLbuCCyAErVqK2617r5VWW/DW69bWVuyt2lor99pFqa1t8Gq1RUXRilZ7RZCIQFxAkaCgoELY9wQCZJnMc/+YSTrEAAEySybf9+vFKzlzzpzznHicZ87vOb/fz9wdERERgJR4ByAiIolDSUFEROooKYiISB0lBRERqaOkICIidZQURESkjpKCSJSZWQ8zczNLCy//08zGNWbbwzjWnWb26JHEKy2bqZ+CSHSZWQ/gCyDd3QNNuG0+8Dd379YkgYqgOwVJEof7zVpE9qWkIAnNzI41sxfMbIuZbTOzh8Ovf9vM5pvZ78xsO3CPmeWY2ZPhbVeb2X+ZWUp4+xPN7E0zKzOzrWY2Lfy6hfexObxuiZn1ayCOq8xsYb3XbjOzl8K/jzWzD8xsp5mtMbN7DnBOhWb23fDvqWb263BMnwNj6237HTNbZma7zOxzM5sQfr0N8E/gGDMrD/87xszuMbO/Rbz/IjP7yMxKw8c9OWLdKjP7Uficy8xsmpllHdJ/IEk6SgqSsMwsFfgHsBroAXQFnonY5HTgc6AzcB/weyAHOB4YBVwLfCe87b3ATKA90C28LcB5wEigN5ALXAlsayCcl4A+ZtYr4rVvAk+Ff98dPl4uoQ/2/zCzSxpxmt8DLgAGA0OAy+qt3xxe3y58Lr8zs1PcfTcwBljv7m3D/9ZHvtHMegNPA7cCRwGvAi+bWUbEZlcAXwN6AgOAbzciZkliSgqSyE4DjgFud/fd7l7h7vMi1q9399+H296rCH2gT3T3Xe6+CvgN8K3wttXAccAx9fZTDWQDJxGqsS1z9w31A3H3PcAM4GqAcHI4iVCywN0L3b3Y3YPuvoTQh/GoRpzjFcCD7r7G3bcD99c77ivu/pmHvEkosZ3ViP0S/nu84u6vu3s18GugFTAsYpvJ7r4+fOyXgUGN3LckKSUFSWTHAqsPUHBdE/F7JyCD0F1FrdWE7i4AfgwY8G64OeU6AHd/A3gY+AOwycwKzKzdfo73FOGkQOgu4cVwssDMTjezOeGmqzLghnBMB3NMvfOIjB8zG2Nmb5vZdjMrBb7eyP3W7rtuf+4eDB+ra8Q2GyN+3wO0beS+JUkpKUgiWwN0P0AROfLRua38626gVndgHYC7b3T377n7McAE4I9mdmJ43WR3PxXoS6gZ6fb9HG8m0MnMBhFKDk9FrHuK0F3Dse6eA/yJUBI6mA2Ekl9kzACYWSbwPKFv+HnunkuoCah2vwd7dHA9EX8PM7PwsdY1Ii5poZQUJJG9S+hDc5KZtTGzLDMb3tCG7l4DPAvcZ2bZZnYc8APgbwBmdrmZ1T66uYPQB2qNmQ0Nf8tPJ1QXqABq9nOMADAdeADoALwesTob2O7uFWZ2GqE7icZ4FrjZzLqZWXvgjoh1GUAmsAUImNkYQjWQWpuAjmaWc4B9jzWzc8Ln90OgEljQyNikBVJSkIQV/qC/EDgRKAHWEmon35+bCH2wfw7MI/Tt/bHwuqHAO2ZWTugb/S3u/gWhAu4UQoliNaEi868PcIyngK8Cz9Vr1vo+8Asz2wXcRegDuTGmAK8BHwLvAy/UrnD3XcDN4X3tIJRoXopYv5xQ7eLz8NNFx0Tu2N0/Aa4hVFTfSuhveaG7VzUyNmmB1HlNRETq6E5BRETqKCmIiEgdJQUREamjpCAiInWUFEREpE6zHlmyU6dO3qNHj3iHEVO7d++mTZs28Q5DkpiuseS3aNGire5+VEPropYUzOwxQgN5bXb3fuHXOgDTCA1utgq4wt13hNdNBK4n1HHoZnd/7WDH6NGjBwsXLjzYZkmlsLCQ/Pz8eIchSUzXWPIzs9X7WxfN5qO/EBp9MdIdwGx37wXMDi9jZl8BriI0zMDXCA1BkBrF2EREpAFRSwruPhfYXu/li4Enwr8/AVwS8foz7l4Z7mW6ktAImSIiEkOxrink1Q5L7O4bzKxz+PWuwNsR261l35Ec65jZeGA8QF5eHoWFhdGLNgGVl5e3uHOW2NI11rIlSqG5odEkGxx/w90LgAKAIUOGeP22z+rqatauXUtFRUVTx5gQcnJyyMqK/eRYWVlZdOvWjfT09JgfW2JLNYWWLdZJYZOZdQnfJXQhNKsUhO4MIocP7kZo2N9DtnbtWrKzs+nRowehkYKTy65du8jOzo7pMd2dbdu2sXbtWnr27BnTY4tIbMW6n8JLwLjw7+MIzWRV+/pVZpZpZj2BXoSGTT5kFRUVdOzYMSkTQryYGR07dkzauy+R5qa4oIjC8++nuKCoyfcdzUdSnwbyCU1Ksha4G5gEPGtm1xMaCvlyAHf/yMyeBT4GAsCN4WGTD/fYRxi91Ke/qUhiKC4ooveE0aRSTdXMTIqZTf/xZzbZ/qP59NHV7t7F3dPdvZu7/6+7b3P3c9y9V/jn9ojt73P3E9y9j7v/M1pxRVtpaSl//OMfD+u9X//61yktLW3agEQkqWx9bg4ZVJJGkHSq2PZ8YZPuX8NcNLEDJYWamgPf/Lz66qvk5uY2aTyBQOCAy419n4gkhpRO7TGghhSqyaDjpflNuv9EefooroqKoLAQ8vPhzCO8C7vjjjv47LPPGDRoEOeeey5jx47l5z//OV26dGHx4sV8/PHHXHLJJaxZs4aKigpuueUWxo8fD/yrh3Z5eTljxoxhxIgRLFiwgK5duzJjxgxatWq1z7G2bNnCDTfcQElJCQAPPvggw4cP55577mH9+vWsWrWKTp060bt3732W77//fq677jq2bNnCUUcdxeOPP0737t359re/TYcOHfjggw845ZRT+M1vfnNkfwwRaXrLllNJOgtG3Umnb57fpE1HkORJ4dZbYfHiA29TVgZLlkAwCCkpMGAA5Oxvxltg0CB48MH9r580aRJLly5lcfjAhYWFvPvuuyxdurTuyZ3HHnuMDh06sHfvXoYOHcqll15Kx44d99nPihUrePrpp5kyZQpXXHEFzz//PNdcc80+29xyyy3cdtttjBgxgpKSEs4//3yWLVsGwKJFi5g3bx6tWrXinnvu2Wf5wgsv5Nprr2XcuHE89thj3Hzzzbz44osAfPrpp8yaNYvUVHUoF0k0wUCQvkueYk1Gr6gkBEjypNAYZWWhhAChn2VlB04Kh+O0007b51HOyZMn8/e//x2ANWvWsGLFii8lhZ49ezJo0CAATj31VFatWvWl/c6aNYuPP/64bnnnzp3s2rULgIsuumifO4vI5aKiIl54ITQV8Le+9S1+/OMf1213+eWXKyGIJKi3xk1hlG+lfdV2Kiec0+RFZkjypHCgb/S1iorgnHOgqgoyMmDq1CNvQqovcsTJwsJCZs2aRVFREa1btyY/P7/BRz0zMzPrfk9NTWXv3r1f2iYYDFJUVPSlZqX6x2xoOVLkk0UaHVMkcXV46QkcSI0sMjdxUmjxheYzz4TZs+Hee0M/jzQhZGdn131bb0hZWRnt27endevWLF++nLfffnu/2x7Meeedx8MPP1y3vPhgbWVhw4YN45lnngFg6tSpjBgx4rBjEJHYqKmq4ZjyT6khlWpSo1JkhiS/U2isM89suruDjh07Mnz4cPr168eYMWMYO3bsPuu/9rWv8ac//YkBAwbQp08fzjjjjMM+1uTJk7nxxhsZMGAAgUCAkSNH8qc//alR77vuuut44IEH6grNIpLY5n3rz4xiG2/2/i7e43g6XpoflZqCuTc4xFCzMGTIEK8/n8KyZcs4+eST4xRR9MVjmItayf63lRCNfZR4iguKOGnCSNIIsJdWfPbnI6slmNkidx/S0LoW33wkIpLotj77BmkEMIhKh7VISgoiIgnO2rbBgECUOqxFUlIQEUlwrRfMopJ05ve+7oibjg5GSUFEJIF98GAhQ7e8QjrVDP10atSPp6QgIpLA9jz0KEbowzra9QTQI6kiIgmtdel6HAhEsW9CJN0pNLEjGTobQoPa7dmzpwkjEpHmatGk1+lXOpcPcs9m/nn3Rr2eAEoKTS7eSeFwh8o+2LDeIhJbxQVF9J94AenU8JXS+VHrrFafkgKEBkC6//7QzyMUOXT27bffDsADDzzA0KFDGTBgAHfffTcAu3fvZuzYsQwcOJB+/foxbdo0Jk+ezPr16xk9ejSjR4/+0r4XLVrEmDFjOPXUUzn//PPZsGEDAPn5+dx5552MGjWKhx566EvLs2fPZvDgwfTv35/rrruOyspKIDRU9y9+8QtGjBjBc889d8TnLiJNZ9vzhaRRBUAqgajXEmold00hDmNn1x86e+bMmaxYsYJ3330Xd+eiiy5i7ty5bNmyhWOOOYZXXnklHEYZOTk5/Pa3v2XOnDl06tRpn/1WV1dz0003MXXqVHr27Mm0adP46U9/ymOPPQaE7lDefPNNAF5++eW65YqKCnr16sXs2bPp3bs31157LY888gi33norAFlZWcybN+/AfyMRibmsU7+CzYQaLCa1hFq6U2ho7OwmNHPmTGbOnMngwYM55ZRTWL58OStWrKB///7MmjWLn/zkJ7z11lvkHGS87k8++YSlS5dy8cUXM2jQIH75y1+ydu3auvVXXnnlPtvXLn/yySf07NmT3r17AzBu3Djmzp273/eJSGKofPl1DHj7uKtiUkuoldx3Cgkwdra7M3HiRCZMmPCldYsWLeLVV19l4sSJnHfeedx1110H3E/fvn2ZOXNmg2Mf7W+o7IONbaWhskUSz5I/L+CspX/EgcGrX+QzborZsXWn0MRjZ9cfOvv888/nscceo7y8HIB169axefNm1q9fT+vWrbnmmmv40Y9+xPvvv9/g+2v16dOHLVu28M477wCh5qSPPvrooPGcdNJJrFq1ipUrVwLw17/+lVGjRh3ROYpIdG3/w9Ok4DEZ66i+5L5TaKwmHDu7/tDZDzzwAMuWLePM8P7btm3L3/72N1auXMntt99OSkoK6enpPPLIIwCMHz+eMWPG0KVLF+bMmVO334yMDKZPn86NN97ID3/4QwKBALfeeit9+/Y9YDxZWVk8/vjjXH755QQCAYYOHcoNN9zQJOcqItGRvmNzTPsmRNLQ2c2Mhs6WaNPQ2fH14cNvccJNYyjJ6sXmkVdE5VHUAw2drTsFEZEEUVxQxEk3fZVMqjih4mNqYtQ3IZJqCiIiCWLb84Wkh/smpFAT01pCLSUFEZEE0Wb4IAwIxrhvQqSkTArNuU6SqPQ3FYm+3S+G+yZ0vSymfRMiJV1SyMrKYtu2bfoQa0LuzrZt28jKyop3KCJJq7igiJEfPIgDg9b9I25xJF2huVu3bqxdu5YtW7bEO5SoqKioiMuHc1ZWFt26dYv5cUVaiu0PP0UKoS+zdX0T4nCnkHRJIT09nZ49e8Y7jKgpLCxk8ODB8Q5DRJpY+vYNceubECnpkoKISHPzwYOFDFr3Ch+1HsLWEd+I2TDZDUm6moKISHNSXFDEV247n9ZUcOKe4rgmBIhTUjCz28zsIzNbamZPm1mWmXUws9fNbEX4Z/t4xCYiEkuRfRNiOW/C/sQ8KZhZV+BmYIi79wNSgauAO4DZ7t4LmB1eFhFJahl9TySF2M+bsD/xaj5KA1qZWRrQGlgPXAw8EV7/BHBJfEITEYmd4IsvUQMs6HlN3PomRIp5UnD3dcCvgRJgA1Dm7jOBPHffEN5mA9A51rGJiMTS4t/PZfgXf8OAU7+YHu9wgDg8fRSuFVwM9ARKgefM7JpDeP94YDxAXl4ehYWFUYgycZWXl7e4c5bY0jUWOzX3Tcagbt6EFVNeYFvvyrjGFI9HUr8KfOHuWwDM7AVgGLDJzLq4+wYz6wJsbujN7l4AFEBo6OyWNsSvhjWWaNM1Fjsf77qdIEYNKVSTQa/vfYP++fFtPopHUigBzjCz1sBe4BxgIbAbGAdMCv+cEYfYRERiYv4t0xi+ZyFvH30xFQNOj/ujqLXiUVN4B5gOvA8Uh2MoIJQMzjWzFcC54WURkaRTXFDEaZOvwYGBG19LmIQAcerR7O53A3fXe7mS0F2DiEhS2/rM66QRwIA0quM2zlFD1KNZRCTWqqowIBCuJcS7b0IkjX0kIhJDxQVF9Cn6CxtSjmb5OTfR6bLRCdN0BEoKIiIxU1xQRK8Jo8mikirSEy4hgJqPRERiZtvzhWQQ6odgBOM+zlFDlBRERGKk9ZkD4j4H88EoKYiIxMjeZ2ZgQNGxVyTEOEcNUVIQEYmBDx9+i7M+mUIQGLzmpXiHs19KCiIiMbD7f/5ACqEP3bo5mBOQnj4SEYkyDzqdNy3ZZ5yjRKwngJKCiEjUzb12CqOqljGv+1UEThqQUMNa1KfmIxGRKCouKGL41O/jwCklMxI6IYCSgohIVG179O+kUlM3Z0Ki1hJqKSmIiERR+obVAARITehaQi3VFEREoqRsdSkD1r7Koo7nUX5qfsI3HYGSgohI1Cy+4KeMopyKf7ua/Cnfjnc4jaLmIxGRKFj8+7mctfQRgsApj36f4oKieIfUKEoKIiJRsHfSZFLwhO+sVp+aj0REmlgwEOTYTQubRWe1+pQURESa2JsXPsDomtW82ft6vMcJzaLAXEvNRyIiTWjJnxcw8v/uxIGhnz7VrBICKCmIiDSp0t8+TirBZtNZrT41H4mINKFOaxcTBGqaSWe1+pQURESayNxrH2XknoUsOOZSqvqd2uyajkDNRyIiTaK4oIjhf52AA4PXv9osEwIoKYiINIntDz9VV0tIa4a1hFpqPhIRaQLtVi/BaT4D3+2PkoKIyBGaN+GvjNg5l3eOGsvewcObbdMRqPlIROSIFBcUcUbBd3BgwJY3mnVCACUFEZEjsu0PT5MWnkSnOdcSaqn5SETkCOSu+jApagm1lBRERA7TW9c9zlk75/J23oVUDDyz2TcdgZqPREQOS3FBEcMe/y4ODNw0KykSAigpiIgclu0PPpEU/RLqi0tSMLNcM5tuZsvNbJmZnWlmHczsdTNbEf7ZPh6xiYgcjAedzl+8gwPVSVJLqBWvO4WHgP9z95OAgcAy4A5gtrv3AmaHl0VEEs6b33iIkysWM//Yq5h/3r189ufZSdF0BHFICmbWDhgJ/C+Au1e5eylwMfBEeLMngEtiHZuIyMEseWQ+Z834AQ6csmZG0tQSasXj6aPjgS3A42Y2EFgE3ALkufsGAHffYGadG3qzmY0HxgPk5eVRWFgYk6ATRXl5eYs7Z4ktXWMHFrznIVJxIDRfwoopL7Ctd2Wco2o65u6xPaDZEOBtYLi7v2NmDwE7gZvcPTdiux3ufsC6wpAhQ3zhwoVRjTfRFBYWkp+fH+8wJInpGtu/qvIqtuX0JC+4oW7u5ebYdGRmi9x9SEPr4nGnsBZY6+7vhJenE6ofbDKzLuG7hC7A5jjEJiKyXwvO+gn5wfUUDrgZjj466ZqOIA41BXffCKwxsz7hl84BPgZeAsaFXxsHzIh1bCIi+7No0uuMWvwgQeC0JVOSMiFA/Ho03wRMNbMM4HPgO4QS1LNmdj1QAlwep9hERL4kddIvMdh37mUlhabh7ouBhtqzzolxKCIiB7Vm7hd8pWwBgXCJOZn6JdSnsY9ERA6guKCIjJsnECSF9yc+S8Wij5O26QiUFERE9qu4oIheE0aTRSXVpNGmR2fO+NXF8Q4rqjT2kYjIfmx9bg6Z1PZB8KQZ3+hADpoUzKy1mf3MzKaEl3uZ2QXRD01EJM52l2NQ1ychWesIkRrTfPQ4oV7HtQ1oa4HngH9EKygRkXhbNOl1hhZNZnXq8Xx+9vV0umx00tYRIjWm+egEd/8foBrA3fcSeipLRCQpFRcUMWDi12nDbvJq1rWYhACNSwpVZtYKQoN9mNkJQPIM9CEiUs+2R6aRRgADUgm0iFpCrcY0H90N/B9wrJlNBYYD345mUCIi8eJBJ2/5m0DyzZXQGAdNCu7+upm9D5xBqNnoFnffGvXIRETioHD0zxldsZh5x/07gT59k7pPQkMa8/TRcKDC3V8BcoE7zey4aAcmIhJriya9zqi5Pw/NlbD6hRaXEKBxNYVHgD3huQ9uB1YDT0Y1KhGROMj45c++PL5RC9OYpBDw0KQLFwOT3f0hIDu6YYmIxFZxQRF9d79DgLQWWUuo1ZhC8y4zmwhcA4w0s1QgPbphiYjEzocPv0WXWy5nmx1FyaSn2DX7vRbZdASNSwpXAt8Ernf3jWbWHXggumGJiMRGcUERJ990DhlUU0kGGbltyH9tYrzDipuDNh+5+0Z3/y3woZl1AMpRb2YRSRLb/jyd9FDfXFKoaZF1hEgHvVMwswnAL4C9hDuwhX8eH8W4RESiLhgI0uWjWQAEWnAdIVJjmo9+BPRV3wQRSSbFBUXsuet+Tq9cwpzhP8XatGmxdYRIjUkKnwF7oh2IiEisFBcUceKEs8mighpS6PitrzNgwrB4h5UQGpMUJgILzOwdIsY8cveboxaViEgUbX1uDn2pwADH2P7Cm6CkADQuKfwZeAMoBoLRDUdEJPps21ZSCM2TUKU6wj4akxQC7v6DqEciIhID82+ZxrAPfs8nGf1YP+rqFjUsdmM0JinMMbPxwMvs23y0PWpRiYhEweKH3+KMyd8khSDdq1ZSpYTwJY1JCt8M/4zszaFHUkWk2am56+ekhlvB06gO9UlQUthHY4bO7hmLQEREoqm4oIiBO+YQIBUH9UnYj8bcKYiINGuL/nsWx0+8gq0pnVl335PsmrNQfRL2Q0lBRJJacUERA+/4GqnUUOmZbOnQtkWPbXQwBxz7yEKOjVUwIiJNbde9vyONmhY53/LhOGBSCM+j8GJsQhERaVqfv7qcQWtfpoaUFj1HwqFoTPPR22Y21N3fi3o0IiJN5IPfFXLMj66iikyK75zG3oUfqY7QCI1JCqOBCWa2GthNaKY6d/cBUY1MROQwFRcU0f8HXyWNGirJpPVxR3H6faojNEZjksKYqEchItKEausIACm1dQTdITRKY/oprI5FICIiTeGTaYs5de0MakghiKmOcIji9khqeK7nhcA6d78gPKvbNKAHsAq4wt13xCs+EWl+3rvvNU742dXstHZ88bO/sOftJaojHKKDTscZRbcAyyKW7wBmu3svYHZ4WUSkUZY8Mp9T/uvrtPcdZPsuWnXtQP5rE5UQDlFckoKZdQPGAo9GvHwx8ET49yeAS2Iclog0YzV3/oxUguqPcITi1Xz0IPBjIDvitTx33wDg7hvMrHNDbwyP2DoeIC8vj8LCwuhGmmDKy8tb3DlLbDXHa2zz7xdyRem+4xpt7del2Z1HIoh5UjCzC4DN7r7IzPIP9f3uXgAUAAwZMsTz8w95F81aYWEhLe2cJbaa0zVWXFDE9t9P5aKlU/iw3Qj8l/dR+o/5dLw0n8vUbHRY4nGnMBy4yMy+DmQB7czsb8AmM+sSvkvoAmyOQ2wi0kxEzrPsGJW33cFpN42Em0bGO7RmLeY1BXef6O7d3L0HcBXwhrtfA7wEjAtvNg6YEevYRKT52Pr0TLLC8ywHSWFP0ZJ4h5QU4vn0UX2TgHPNbAVwbnhZRORLaqpqaP/+GwAENM9yk4rr0NnuXggUhn/fBpwTz3hEJPEVFxQR/NHtDNo1n8JBt0LnzuqL0IQ0n4KINBvFBUX0mTCKDKqpJo2O/3GFkkETS6TmIxGRA9r5q4dJpzq85OqLEAW6UxCRZuH9B2YzdPWzBDWmUVQpKYhIwvv4yYX0+vElrM48idKf/Zrdc99XHSFKlBREJKHNv2UaAydfR3lKO7IXvEavU46Bn54f77CSlmoKIpKw3v7xC5w5+WrasIecYClbFmok/2hTUhCRhLT+nTX0+fX1GI4BaVSrsBwDSgoiknA2vr+eqrPOJt2rqCSTalJVWI4R1RREJKG8e/cr9Lz3Oxzl5Xwx5Q08GHr0VIXl2FBSEJGE8fZPX+K0X12C4VSSiQc9lAiUDGJGzUcikhDWFZXQ6/7v1NUQNFFOfCgpiEjclRR+jp81kkyvUA0hztR8JCJxNf+WafSbPB7DWfPXuQT2VKmGEEdKCiISN3Ove4yzHr8egEoyCeypUg0hztR8JCJxsWjS65z++A0AqiEkECUFEYm5BTc9Tf+JY9mQdiwVZKmGkEDUfCQiMfXm4FsZtfghlmcNosvyOax8bZlqCAlESUFEYiJQEeDdE69m1LrpBIHuFZ/w2WvLVENIMGo+EpGoKysp44NjL2TYuukEMVKAdKpUQ0hASgoiEjXFBUXMGXo7ZccPYtDWWcw5/SeqISQ4NR+JSFQUFxTRa8Jo+lEJwNxLJzN6+k0UF1ysGkICU1IQkSbnQWfPXfeTSSUGBEjFd5UDqIaQ4NR8JCJNas/WPcw/8VpO3/QyQVKoJpUqNRU1G7pTEJEmUVxQxLaC5+nx4YsMC3xO4dm/oMNlZ7P9xblqKmpGlBRE5IgVFxTRe0I+GVQB8OZFvyV/xm2hlf8xPI6RyaFS85GIHJGK0gr8hz8kkyoMqCEVKiriHZYcJiUFETlsK2d8REmX0xhQXkQ1aaofJAE1H4nIISkuKGLr9DmwYwdnLHyYcsvmvXteIatLez1qmgSUFESk0YoLijhxwtn0pYIUoLjN6eQteJGhA44ObaBk0Oyp+UhEGsWDzs77JpMVTgg1pLD1zIvoXJsQJCkoKYjIQW1avIF3ul3K8JJnCGIESKWSTDpdPjreoUkTU/ORiDSornawt4JB8x9moO+l8Ov/Q+6YMyh9eZ5qB0kq5knBzI4FngSOBoJAgbs/ZGYdgGlAD2AVcIW774h1fCLy5drBJ5kDyJjxHPnn9w5t8J9nxTU+iZ54NB8FgB+6+8nAGcCNZvYV4A5gtrv3AmaHl0UkxgLl1VROvHuf2sH6s66kZ21CkKQW86Tg7hvc/f3w77uAZUBX4GLgifBmTwCXxDo2kZbuvXte4YRLbmDI9tepIVW1gxYorjUFM+sBDAbeAfLcfQOEEoeZdY5nbCItRXFBEdumPE+HT95m6K75rEzrzaL7Z5LRoa36HbRAcUsKZtYWeB641d13mllj3zceGA+Ql5dHYWFh1GJMROXl5S3unCV6tkz7lEv+dCNpBAB4pcc4qv/7EnI7pwOVMPFMtlGpa64FiUtSMLN0Qglhqru/EH55k5l1Cd8ldAE2N/Redy8ACgCGDBni+fn5sQg5YRQWFtLSzlmaXlV5FUXfKWDM9DtIDyeEAKm06d0HOufqGmvBYl5TsNAtwf8Cy9z9txGrXgLGhX8fB8yIdWwiyS4YCDL/+1PZ2P4kRk2/iTVZvaggU2MWSZ143CkMB74FFJvZ4vBrdwKTgGfN7HqgBLg8DrGJJJ3a/gaWlUWX159keMWHfJI1kIV3/5NT7zyfpY++vU/tQE1FLVvMk4K7zwP2V0A4J5axiCS7yHmSDdiQcgwL/vMpzvjdlaSkhRoKND2mRNIwFyJJyIPOu3f9g/bfv5qscEKoIYXl+f/BsN9fXZcQROrTlSGSRAIVARbc9DQr2gzktHsvJCNYQRXpVNf2N7hSN+NyYBr7SKSZKy4oYuszs7Dd5fR8fzrDAp/zWcbJzPveE5z+4NUs/9tC9TeQRlNSEGnG3vvFPxl498X0oxoDVmaczDt3/J2h917ECaoZyGFQUhBpZjzoLJ1SRNl/P8IZXzxNGjUABEhhzahrGH3/JfENUJo11RREmonyjeXMvaaAT9sMpv8NwxnwxQze7XoJFWSF+xlk0ukyjVEkR0Z3CiIJqrigiG3PF5J24nHUvFXEoOInGclOPskayNx//zOn/PqbDDu6bd12qhlIU1BSEElAiya9Tv+JF5BOFTYTqkjj3Z5XkXPH9+n33TPok/Kvrj6qGUhTUlIQSRAVpRUs/tWr+FNPMWTdi6SHawU1GPOH/YTR838Z5wilJVBSEImjmqoaPnyokN1TnmLAiuc5gzK2WGfe6/oNTln3EqkEqCaDTuPGxjtUaSGUFERiqHYcotRuR+OLiznpw2c4JbiRnWSz5IRv0Or6bzLwtrMZlpWmWoHEhZKCSAxU76lmwXemMOzZW0gjgBGqE7zf5QI+v+qbDPqvCxjRodU+71GtQOJBSUEkSrYu28Ly3/2TlP97hb5rXmMUZTjUjUM0/6yJjJ77i3iHKbIPJQWRJhIMBPnkmQ/Y9NgrdHr3Vb6y+11G4GxKOZolvS4l0ONEznj9XtKoCtUJrhkT75BFvkRJQeQwFRcUsfWp1yAjk7RVK+n92aucHNxIH4yP2wxl7uh7yLtuLH2uGkxeeMiJ4oJ81QkkoSkpiByC0i92sOLJIiqefIZhn08lhSAGlNOa4m5jWfG1sfS5dQz9+nZu8P2qE0iiU1IQ2Q8POuuKSlg9dR41b86jy2fzOKHyI4bi1JBSlxACpPDeORMZPeu/4h2yyBFTUhAJq6mqYeXfi9k4fR7p78yj57p5dAuuoxuwk2w+7TiMucOvIGfsCDzonHR7qMdxNRl0ukLzFEhyUFKQFqm4oIitT88kpWMHfEcpbT+cR+9tRfRhF32ADSld+aLbWXx62nCOvmwEJ/5bf4ZkpO67j3azVR+QpKOkIEkvGAiyZu4XbHy9mL3vFpP9QSGDd8zBcAwIAiuz+rO47zWkjhzOcf8+gq5ndqdLyv6mEg9RfUCSkZKCJJUtSzex5p9L2bWgmJSPiumwvpgeuz/iOPZwXHibHdYewgkhQApvnfVTRs/9Bb3jGLdIolBSkGapfGM5q175iB1ziwl+WEx2yVKOLS3mKN/CUeFttthRrMntz8ITv0vKwP60H9mf48Z8hbX/WErmhHP+VQ9QfwGROkoKktA+eOhNyv7yd7xDR6yyklafFdNlazHdA1/QL7zNblqzqk1flp94IR/37U/2sP4cO6YfR/XLq0sQkfqPP5NiVA8QaYiSgsSNB53tK7axeWEJZUtWU7miBC8pIXNjCdmlJXTZvZLBbK/bPkAKqzP6sLbLUD7vfR2thvbj6HP7c+zInvRNO7RJBFUPEGmYkoJETeXOSjYuXMv2xSXs/ng1gc9LSF1XQuutJbQvL+HoqhI6speOEe/ZQys2ZnRnR9vubPTjyd2zg1ScAKm8dfbdjJ79M06I2xmJJD8lBTkskd/ydy4toeKT1ft8yz9qbwl5wY0cB3UFXoBNKUezpVV3Nh3Vn5Kjx2LHHUdW7+6069edzkO606FXR44PP/VTXFBEZWTb/5Vfjcu5irQkSgryJcFAkPcfmEX58zOxnj1Izc0m8NnqQ/qW/2nXsSzr2p20E46jzcnd6TCoO0cP6UZeu0zyGhmH2v5FYk9JIQlV76lm55oyyteWsntdKRWbyqjcXEpgSyk120qhrAwrKyW1vJT0PWVk7i2lVWUpbapLya4pI5syhtTubNG/9vulb/ndu5PZqzs5A4770rf8pqK2f5HYUlJIQBWlFewsKaV8bSl71oc+1Ks2lxLYWsr2lZ9RmDqTlJ2hD/WMPaVkVpTRuir0od4uWEob9tAR9vkWHymIsdNyKE/JYXd6LnszcynN7cmW1jnUZOeSvXopA0vnhNvyU5h36m2c+cZ9h/QtX0SaJyWFJuZBp3xjOeXrQt/U924opWJjKdVbywhsLcV3lEJZGSm7Skmv/VCvLKNNVSltA6W081KyqCILaHicTagmjTLLpTwtlz3pOVRk5rKlXRc2tMmlJjsHcnKx9rmkdsolvWMOWUfn0qpLLm265tK2aw7Zx2STm5ZC7n72X78tv+P4S8lslxmVv5eIJJYWmxT2N/9tTVUNu9btpHxdGbvXhT7UKzeFPtRrtoU+1K2slJTyMtLLS8ncW0pWZSmtq8vIrimlnZeRTQ3ZBzj2HlqxKyUn9KGekUtFVnt2duhJoG0uwXa5kJNDSodc0jrlkn5ULll5ObTqkkvbbrksWV3MuRefR6cUo1OU/jZqyxdpuVpkUljwg+c47XdXk0INPjOFVTf2IM0DZNeUksNOcmG/36IhNGLmrtRc9qTlsCczl13ZXdne+isEsnPxdrmQm0tKhxzSO+WS0Tn0Lb3V0Tm07ZZLu2NzaN0uk9ZwWE0xy3d+ijVxu31D1JYv0jK1yKRQ9cZ8UqkJD4YWJJCSxZrupxFsm4PXNr10zCWtYw6ZefWaXrq2o11WGu3ifRIiIlGQcEnBzL4GPASkAo+6+6SmPkbH71/J3gkFdW3me3//KGfpW7GISGIlBTNLBf4AnAusBd4zs5fc/eOmPI7azEVEGpZQSQE4DVjp7p8DmNkzwMVAkyYFUJu5iEhDEi0pdAXWRCyvBU6P3MDMxgPjAfLy8igsLIxZcImgvLy8xZ2zxJausZYt0ZJCQ4/V+D4L7gVAAcCQIUM8Pz8/BmEljsLCQlraOUts6Rpr2Q5tvOHoWwscG7HcDVgfp1hERFqcREsK7wG9zKynmWUAVwEvxTkmEZEWI6Gaj9w9YGb/CbxG6JHUx9z9oziHJSLSYiRUUgBw91eBV+Mdh4hIS2TufvCtEpSZbQFKgbLD3EXOIb63sds3ZrsDbXOgdZ2ArY2IIdEc6t86UY51JPtK1OvrYOt1jcX2WPG4xo5z94amMAd3b9b/gIJYvbex2zdmuwNtc5B1C+P9N4/1f6d4HisZr6+Drdc1FttjJdo1lmiF5sPxcgzf29jtG7PdgbY5knNKVLE8p6Y8VjJeX4dyrOZE11gTbN+sm49aIjNb6O5DDr6lyOHRNdayJcOdQktTEO8AJOnpGmvBdKcgIiJ1dKcgIiJ1lBRERKSOkoKIiNRRUkgiZnaJmU0xsxlmdl6845HkY2bHm9n/mtn0eMci0aGkkCDM7DEz22xmS+u9/jUz+8TMVprZHQfah7u/6O7fA74NXBnFcKUZaqJr7HN3vz66kUo86emjBGFmI4Fy4El37xd+LRX4lIjpSYGrCQ0WeH+9XVzn7pvD7/sNMNXd349R+NIMNPE1Nt3dL4tV7BI7CTcgXkvl7nPNrEe9lxucntTd7wcuqL8PMzNgEvBPJQSprymuMUl+aj5KbA1NT9r1ANvfBHwVuMzMbohmYJI0DukaM7OOZvYnYLCZTYx2cBJ7ulNIbAednnSfFe6TgcnRC0eS0KFeY9sAfeFIYrpTSGyanlSiTdeY7ENJIbFpelKJNl1jsg8lhQRhZk8DRUAfM1trZte7ewConZ50GfCsa3pSOUy6xqQx9EiqiIjU0Z2CiIjUUVIQEZE6SgoiIlJHSUFEROooKYiISB0lBRERqaOkIFKPmZU30X7uMbMfNWK7v5iZRhyVhKCkICIidZQURPbDzNqa2Wwze9/Mis3s4vDrPcxsuZk9amZLzWyqmX3VzOab2QozOy1iNwPN7I3w698Lv9/M7GEz+9jMXgE6RxzzLjN7L7zfgvBw6CIxo6Qgsn8VwL+5+ynAaOA3ER/SJwIPAQOAk4BvAiOAHwF3RuxjADAWOBO4y8yOAf4N6AP0B74HDIvY/mF3HxqeBKcVmtNAYkxDZ4vsnwG/Cs9YFiQ0z0BeeN0X7l4MYGYfAbPd3c2sGOgRsY8Z7r4X2GtmcwhNajMSeNrda4D1ZvZGxPajzezHQGugA/AR8HLUzlCkHiUFkf37d+Ao4FR3rzazVUBWeF1lxHbBiOUg+/5/VX9wMd/P65hZFvBHYIi7rzGzeyKOJxITaj4S2b8cYHM4IYwGjjuMfVxsZllm1hHIJzRU9VzgKjNLNbMuhJqm4F8JYKuZtQX0RJLEnO4URPZvKvCymS0EFgPLD2Mf7wKvAN2Be919vZn9HTgbKAY+Bd4EcPdSM5sSfn0VoQQiElMaOltEROqo+UhEROooKYiISB0lBRERqaOkICIidZQURESkjpKCiIjUUVIQEZE6SgoiIlLn/wG7YGtCPtFjOwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cross_validation_logistic_regression(y, tx, k_fold, initial_w, max_iters, gammas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68989dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "3,5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (250000,21) (250000,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcross_validation_reg_logistic_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlambdas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgammas\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\ML\\The_MadLads\\cross_validation.py:374\u001b[0m, in \u001b[0;36mcross_validation_reg_logistic_regression\u001b[1;34m(y, tx, k_fold, initial_w, max_iters, lambdas, gammas)\u001b[0m\n\u001b[0;32m    372\u001b[0m rmse_te_tmp \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k_fold):\n\u001b[1;32m--> 374\u001b[0m     loss_tr, loss_te \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreg_logistic_regression\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    375\u001b[0m     rmse_tr_tmp\u001b[38;5;241m.\u001b[39mappend(loss_tr)\n\u001b[0;32m    376\u001b[0m     rmse_te_tmp\u001b[38;5;241m.\u001b[39mappend(loss_te)\n",
      "File \u001b[1;32m~\\ML\\The_MadLads\\cross_validation.py:86\u001b[0m, in \u001b[0;36mcross_validation\u001b[1;34m(y, tx, func, k_indices, k, lambda_, degree, initial_w, max_iters, gamma, batch_size)\u001b[0m\n\u001b[0;32m     84\u001b[0m     w \u001b[38;5;241m=\u001b[39m ridge_regression(train_y, model_tr, lambda_)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m func \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg_logistic_regression\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 86\u001b[0m     l, ws \u001b[38;5;241m=\u001b[39m \u001b[43mreg_logistic_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     w \u001b[38;5;241m=\u001b[39m ws[max_iters\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m func \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogistic_regression\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32m~\\ML\\The_MadLads\\logistic_regression.py:70\u001b[0m, in \u001b[0;36mreg_logistic_regression\u001b[1;34m(y, tx, lambda_, initial_w, max_iters, gamma)\u001b[0m\n\u001b[0;32m     66\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n_iter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# compute gradient\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     gradient \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m/\u001b[39mN \u001b[38;5;241m*\u001b[39m tx\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdot(\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m) \u001b[38;5;241m+\u001b[39m (lambda_\u001b[38;5;241m/\u001b[39mN)\u001b[38;5;241m*\u001b[39mw\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m# update w by gradient\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     w \u001b[38;5;241m=\u001b[39m w \u001b[38;5;241m-\u001b[39m (gamma \u001b[38;5;241m*\u001b[39m gradient)\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (250000,21) (250000,) "
     ]
    }
   ],
   "source": [
    "cross_validation_reg_logistic_regression(y, tx, k_fold, initial_w, max_iters,lambdas, gammas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d40a008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
