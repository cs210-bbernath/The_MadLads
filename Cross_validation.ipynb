{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fd0d97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from helper import *\n",
    "\n",
    "from costs import compute_mse\n",
    "from ridge_regression import ridge_regression\n",
    "from build_polynomial import build_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3351eeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\n",
    "    \n",
    "    Args:\n",
    "        y:      shape=(N,)\n",
    "        k_fold: K in K-fold, i.e. the fold num\n",
    "        seed:   the random seed\n",
    "\n",
    "    Returns:\n",
    "        A 2D array of shape=(k_fold, N/k_fold) that indicates the data indices for each fold\n",
    "\n",
    "    >>> build_k_indices(np.array([1., 2., 3., 4.]), 2, 1)\n",
    "    array([[3, 2],\n",
    "           [0, 1]])\n",
    "    \"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0380fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression for a fold corresponding to k_indices\n",
    "    \n",
    "    Args:\n",
    "        y:          shape=(N,)\n",
    "        x:          shape=(N,)\n",
    "        k_indices:  2D array returned by build_k_indices()\n",
    "        k:          scalar, the k-th fold (N.B.: not to confused with k_fold which is the fold nums)\n",
    "        lambda_:    scalar, cf. ridge_regression()\n",
    "        degree:     scalar, cf. build_poly()\n",
    "\n",
    "    Returns:\n",
    "        train and test root mean square errors rmse = sqrt(2 mse)\n",
    "\n",
    "    >>> cross_validation(np.array([1.,2.,3.,4.]), np.array([6.,7.,8.,9.]), np.array([[3,2], [0,1]]), 1, 2, 3)\n",
    "    (0.019866645527597114, 0.33555914361295175)\n",
    "    \"\"\"\n",
    "    # get k'th subgroup in test, others in train: TODO\n",
    "\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    \n",
    "    indices = k_indices[k]\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        if i in indices:\n",
    "            test_x.append(x[i])\n",
    "            test_y.append(y[i])\n",
    "        else:\n",
    "            train_x.append(x[i])\n",
    "            train_y.append(y[i])\n",
    "\n",
    "\n",
    "    # form data with polynomial degree:\n",
    "    poly_tr = build_poly(train_x, degree)\n",
    "    poly_te = build_poly(test_x, degree)\n",
    "\n",
    "    # ridge regression:\n",
    "    w = ridge_regression(train_y, poly_tr, lambda_)\n",
    "\n",
    "    # calculate the loss for train and test data:\n",
    "    loss_tr = np.sqrt(2*compute_mse(train_y, poly_tr, w))\n",
    "    loss_te = np.sqrt(2*compute_mse(test_y, poly_te, w))\n",
    "    \n",
    "    return loss_tr, loss_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2825cc56",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor polynomial expansion up to degree \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m.f, the choice of lambda which leads to the best test rmse is \u001b[39m\u001b[38;5;132;01m%.5f\u001b[39;00m\u001b[38;5;124m with a test rmse of \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (degree, best_lambda, best_rmse))\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_lambda, best_rmse\n\u001b[1;32m---> 45\u001b[0m best_lambda, best_rmse \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validation_demo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogspace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mcross_validation_demo\u001b[1;34m(degree, k_fold, lambdas)\u001b[0m\n\u001b[0;32m     18\u001b[0m lambdas \u001b[38;5;241m=\u001b[39m lambdas\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# split data in k fold\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m k_indices \u001b[38;5;241m=\u001b[39m build_k_indices(\u001b[43my\u001b[49m, k_fold, seed)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# define lists to store the loss of training data and test data\u001b[39;00m\n\u001b[0;32m     22\u001b[0m rmse_tr \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "from plots import cross_validation_visualization\n",
    "\n",
    "def cross_validation_demo(degree, k_fold, lambdas):\n",
    "    \"\"\"cross validation over regularisation parameter lambda.\n",
    "    \n",
    "    Args:\n",
    "        degree: integer, degree of the polynomial expansion\n",
    "        k_fold: integer, the number of folds\n",
    "        lambdas: shape = (p, ) where p is the number of values of lambda to test\n",
    "    Returns:\n",
    "        best_lambda : scalar, value of the best lambda\n",
    "        best_rmse : scalar, the associated root mean squared error for the best lambda\n",
    "    \"\"\"\n",
    "    \n",
    "    seed = 12\n",
    "    degree = degree\n",
    "    k_fold = k_fold\n",
    "    lambdas = lambdas\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    \n",
    "    # cross validation over lambdas:\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        rmse_tr_tmp = []\n",
    "        rmse_te_tmp = []\n",
    "        for k in range(k_fold):\n",
    "            loss_tr, loss_te = cross_validation(y, x, k_indices, k, lambda_, degree)\n",
    "            rmse_tr_tmp.append(loss_tr)\n",
    "            rmse_te_tmp.append(loss_te)\n",
    "        rmse_tr.append(np.mean(rmse_tr_tmp))\n",
    "        rmse_te.append(np.mean(rmse_te_tmp))\n",
    "\n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "    \n",
    "    best_rmse = min(rmse_te)\n",
    "    best_lambda = lambdas[rmse_te.index(best_rmse)]\n",
    "    \n",
    "    print(\"For polynomial expansion up to degree %.f, the choice of lambda which leads to the best test rmse is %.5f with a test rmse of %.3f\" % (degree, best_lambda, best_rmse))\n",
    "    return best_lambda, best_rmse\n",
    "\n",
    "\n",
    "best_lambda, best_rmse = cross_validation_demo(7, 4, np.logspace(-4, 0, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc60c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_degree_selection(degrees, k_fold, lambdas, seed = 1):\n",
    "    \"\"\"cross validation over regularisation parameter lambda and degree.\n",
    "    \n",
    "    Args:\n",
    "        degrees: shape = (d,), where d is the number of degrees to test \n",
    "        k_fold: integer, the number of folds\n",
    "        lambdas: shape = (p, ) where p is the number of values of lambda to test\n",
    "    Returns:\n",
    "        best_degree : integer, value of the best degree\n",
    "        best_lambda : scalar, value of the best lambda\n",
    "        best_rmse : value of the rmse for the couple (best_degree, best_lambda)\n",
    "        \n",
    "    >>> best_degree_selection(np.arange(2,11), 4, np.logspace(-4, 0, 30))\n",
    "    (7, 0.004520353656360241, 0.28957280566456634)\n",
    "    \"\"\"\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    # cross validation over degrees and lambdas:\n",
    "    best_degrees = []\n",
    "    best_lambdas = []\n",
    "    best_rmses = []\n",
    "    for deg in degrees:  \n",
    "        rmse_tr = []\n",
    "        rmse_te = []\n",
    "        for ind, lambda_ in enumerate(lambdas):\n",
    "            rmse_tr_tmp = []\n",
    "            rmse_te_tmp = []\n",
    "            for k in range(k_fold):\n",
    "                loss_tr, loss_te = cross_validation(y, x, k_indices, k, lambda_, deg)\n",
    "                rmse_tr_tmp.append(loss_tr)\n",
    "                rmse_te_tmp.append(loss_te)\n",
    "            rmse_tr.append(np.mean(rmse_tr_tmp))\n",
    "            rmse_te.append(np.mean(rmse_te_tmp))\n",
    "        \n",
    "        \n",
    "        best_rmse = min(rmse_te)\n",
    "        best_lambda = lambdas[rmse_te.index(best_rmse)]\n",
    "        \n",
    "        best_lambdas.append(best_lambda)\n",
    "        best_rmses.append(best_rmse)\n",
    "        \n",
    "    best_rmse = min(best_rmses)\n",
    "    best_lambda = lambdas[best_rmses.index(best_rmse)]\n",
    "    best_degree = degrees[best_rmses.index(best_rmse)]\n",
    "    \n",
    "    return best_degree, best_lambda, best_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4685f6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f644cc67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4917a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1310b457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fa305e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
